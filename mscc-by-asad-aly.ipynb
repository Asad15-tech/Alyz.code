{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa69a6bb",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-25T19:40:43.791933Z",
     "iopub.status.busy": "2025-11-25T19:40:43.791733Z",
     "iopub.status.idle": "2025-11-25T19:40:43.797731Z",
     "shell.execute_reply": "2025-11-25T19:40:43.797051Z"
    },
    "papermill": {
     "duration": 0.010346,
     "end_time": "2025-11-25T19:40:43.799052",
     "exception": false,
     "start_time": "2025-11-25T19:40:43.788706",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Defining the path for parent directory\n",
    "DIR = '/kaggle/input/airs-ai-in-respiratory-sounds/'\n",
    "# Defining all paths\n",
    "train_path = '/kaggle/input/airs-ai-in-respiratory-sounds/train.csv'\n",
    "test_path = '/kaggle/input/airs-ai-in-respiratory-sounds/test.csv'\n",
    "sound_files_path = '/kaggle/input/airs-ai-in-respiratory-sounds/sounds/sounds/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09616728",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T19:40:43.803625Z",
     "iopub.status.busy": "2025-11-25T19:40:43.803040Z",
     "iopub.status.idle": "2025-11-25T19:41:36.846599Z",
     "shell.execute_reply": "2025-11-25T19:41:36.845831Z"
    },
    "papermill": {
     "duration": 53.049327,
     "end_time": "2025-11-25T19:41:36.850192",
     "exception": false,
     "start_time": "2025-11-25T19:40:43.800865",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/torchaudio/models/wav2vec2_fairseq_base_ls960.pth\" to /root/.cache/torch/hub/checkpoints/wav2vec2_fairseq_base_ls960.pth\n",
      "100%|██████████| 360M/360M [00:01<00:00, 320MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total mapped audio files: 882\n",
      "Audio embeddings shape: (546, 768)\n"
     ]
    }
   ],
   "source": [
    "# ======================================\n",
    "# 1️⃣ IMPORTS + DATA + WAV2VEC EMBEDDINGS\n",
    "# ======================================\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchaudio\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# ✅ Device selection\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ✅ Paths\n",
    "train_csv = \"/kaggle/input/airs-ai-in-respiratory-sounds/train.csv\"\n",
    "audio_path = \"/kaggle/input/airs-ai-in-respiratory-sounds/sounds/sounds\"\n",
    "\n",
    "# ✅ Load dataset\n",
    "train_df = pd.read_csv(train_csv)\n",
    "\n",
    "# ✅ Tabular features\n",
    "tab_features = [\n",
    "    'age','gender','tbContactHistory','wheezingHistory','phlegmCough',\n",
    "    'familyAsthmaHistory','feverHistory','coldPresent','packYears'\n",
    "]\n",
    "\n",
    "# ✅ WAV2VEC2 model\n",
    "bundle = torchaudio.pipelines.WAV2VEC2_BASE\n",
    "wav2vec_model = bundle.get_model().to(device).eval()\n",
    "SR = 16000\n",
    "FIXED_LENGTH = 5 * SR  # 5 seconds audio\n",
    "\n",
    "# ✅ Map audio files\n",
    "file_map = {}\n",
    "for folder in os.listdir(audio_path):\n",
    "    fpath = os.path.join(audio_path, folder)\n",
    "    if os.path.isdir(fpath):\n",
    "        wavs = [f for f in os.listdir(fpath) if f.endswith(\".wav\")]\n",
    "        if wavs:\n",
    "            file_map[folder] = os.path.join(fpath, wavs[0])\n",
    "\n",
    "print(\"Total mapped audio files:\", len(file_map))\n",
    "\n",
    "\n",
    "# ✅ Audio loader + augmentation\n",
    "def load_audio(file_id):\n",
    "    y, sr = librosa.load(file_map[file_id], sr=SR)\n",
    "    if len(y) < FIXED_LENGTH:\n",
    "        y = np.pad(y, (0, FIXED_LENGTH - len(y)))\n",
    "    else:\n",
    "        y = y[:FIXED_LENGTH]\n",
    "    y = y + 0.001 * np.random.randn(len(y))  # light noise\n",
    "    return y\n",
    "\n",
    "# ✅ Extract WAV2VEC embeddings\n",
    "def get_embedding(audio):\n",
    "    t = torch.tensor(audio, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        feat, _ = wav2vec_model.extract_features(t)\n",
    "    return torch.mean(feat[-1], dim=1).cpu().numpy().squeeze()\n",
    "\n",
    "\n",
    "# ✅ Build embedding dataset\n",
    "audio_embs = []\n",
    "valid_ids = []\n",
    "\n",
    "for cid in train_df[\"candidateID\"]:\n",
    "    if cid not in file_map:\n",
    "        continue\n",
    "    emb = get_embedding(load_audio(cid))\n",
    "    audio_embs.append(emb)\n",
    "    valid_ids.append(cid)\n",
    "\n",
    "audio_embs = np.array(audio_embs)\n",
    "\n",
    "# ✅ Filter rows with audio\n",
    "df = train_df[train_df[\"candidateID\"].isin(valid_ids)]\n",
    "\n",
    "# ✅ Extract tabular + target\n",
    "X_tab = df[tab_features].values\n",
    "y = df[\"disease\"].values\n",
    "\n",
    "print(\"Audio embeddings shape:\", audio_embs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6de48b75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T19:41:36.856749Z",
     "iopub.status.busy": "2025-11-25T19:41:36.856372Z",
     "iopub.status.idle": "2025-11-25T19:41:36.873961Z",
     "shell.execute_reply": "2025-11-25T19:41:36.873165Z"
    },
    "papermill": {
     "duration": 0.022205,
     "end_time": "2025-11-25T19:41:36.875198",
     "exception": false,
     "start_time": "2025-11-25T19:41:36.852993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final feature shape: (546, 777)\n"
     ]
    }
   ],
   "source": [
    "# ======================================\n",
    "# 2️⃣ PREPROCESS — IMPUTE + SCALE + COMBINE\n",
    "# ======================================\n",
    "\n",
    "# ✅ Impute missing tabular values\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "X_tab = imputer.fit_transform(X_tab)\n",
    "\n",
    "# ✅ Scale tabular features (audio already normalized)\n",
    "scaler = StandardScaler()\n",
    "X_tab = scaler.fit_transform(X_tab)\n",
    "\n",
    "# ✅ Combine tabular + audio embeddings\n",
    "X = np.hstack([X_tab, audio_embs])\n",
    "print(\"Final feature shape:\", X.shape)\n",
    "\n",
    "# ✅ Train-val split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.15, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# ✅ Convert to tensors\n",
    "import torch\n",
    "X_train_t = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "X_val_t = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "y_val_t = torch.tensor(y_val, dtype=torch.long).to(device)\n",
    "\n",
    "# ✅ DataLoaders\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_loader = DataLoader(TensorDataset(X_train_t, y_train_t), batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val_t, y_val_t), batch_size=32, shuffle=False)\n",
    "\n",
    "num_classes = len(np.unique(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b229298c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T19:41:36.881687Z",
     "iopub.status.busy": "2025-11-25T19:41:36.881011Z",
     "iopub.status.idle": "2025-11-25T19:41:48.059850Z",
     "shell.execute_reply": "2025-11-25T19:41:48.058960Z"
    },
    "papermill": {
     "duration": 11.183337,
     "end_time": "2025-11-25T19:41:48.061100",
     "exception": false,
     "start_time": "2025-11-25T19:41:36.877763",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300 | Loss: 17.1951 | Val Acc: 0.4390\n",
      "Epoch 2/300 | Loss: 16.3533 | Val Acc: 0.4634\n",
      "Epoch 3/300 | Loss: 15.3853 | Val Acc: 0.5122\n",
      "Epoch 4/300 | Loss: 15.3712 | Val Acc: 0.6098\n",
      "Epoch 5/300 | Loss: 15.0652 | Val Acc: 0.6585\n",
      "Epoch 6/300 | Loss: 14.7121 | Val Acc: 0.6707\n",
      "Epoch 7/300 | Loss: 14.3255 | Val Acc: 0.7073\n",
      "Epoch 8/300 | Loss: 13.4680 | Val Acc: 0.8049\n",
      "Epoch 9/300 | Loss: 12.8879 | Val Acc: 0.8171\n",
      "Epoch 10/300 | Loss: 12.7930 | Val Acc: 0.8415\n",
      "Epoch 11/300 | Loss: 11.5749 | Val Acc: 0.8415\n",
      "Epoch 12/300 | Loss: 11.5616 | Val Acc: 0.8659\n",
      "Epoch 13/300 | Loss: 11.2595 | Val Acc: 0.8415\n",
      "Epoch 14/300 | Loss: 10.6639 | Val Acc: 0.8415\n",
      "Epoch 15/300 | Loss: 10.5287 | Val Acc: 0.8415\n",
      "Epoch 16/300 | Loss: 10.2679 | Val Acc: 0.8537\n",
      "Epoch 17/300 | Loss: 10.1708 | Val Acc: 0.8659\n",
      "Epoch 18/300 | Loss: 9.5893 | Val Acc: 0.8659\n",
      "Epoch 19/300 | Loss: 9.7361 | Val Acc: 0.8659\n",
      "Epoch 20/300 | Loss: 9.5750 | Val Acc: 0.8659\n",
      "Epoch 21/300 | Loss: 9.2983 | Val Acc: 0.8659\n",
      "Epoch 22/300 | Loss: 9.4646 | Val Acc: 0.8537\n",
      "Epoch 23/300 | Loss: 9.4505 | Val Acc: 0.8537\n",
      "Epoch 24/300 | Loss: 9.5330 | Val Acc: 0.8537\n",
      "Epoch 25/300 | Loss: 9.4948 | Val Acc: 0.8537\n",
      "Epoch 26/300 | Loss: 9.1212 | Val Acc: 0.8537\n",
      "Epoch 27/300 | Loss: 9.0595 | Val Acc: 0.8415\n",
      "Epoch 28/300 | Loss: 9.1796 | Val Acc: 0.8415\n",
      "Epoch 29/300 | Loss: 9.2610 | Val Acc: 0.8415\n",
      "Epoch 30/300 | Loss: 9.4483 | Val Acc: 0.8537\n",
      "Epoch 31/300 | Loss: 8.7318 | Val Acc: 0.8415\n",
      "Epoch 32/300 | Loss: 8.9971 | Val Acc: 0.8415\n",
      "Epoch 33/300 | Loss: 8.9340 | Val Acc: 0.8415\n",
      "Epoch 34/300 | Loss: 9.2043 | Val Acc: 0.8415\n",
      "Epoch 35/300 | Loss: 9.1688 | Val Acc: 0.8415\n",
      "Epoch 36/300 | Loss: 8.9337 | Val Acc: 0.8415\n",
      "Epoch 37/300 | Loss: 9.4830 | Val Acc: 0.8415\n",
      "Epoch 38/300 | Loss: 9.1908 | Val Acc: 0.8415\n",
      "Epoch 39/300 | Loss: 9.1239 | Val Acc: 0.8415\n",
      "Epoch 40/300 | Loss: 8.8744 | Val Acc: 0.8415\n",
      "Epoch 41/300 | Loss: 8.9541 | Val Acc: 0.8415\n",
      "Epoch 42/300 | Loss: 8.9296 | Val Acc: 0.8415\n",
      "Epoch 43/300 | Loss: 8.9631 | Val Acc: 0.8415\n",
      "Epoch 44/300 | Loss: 8.6135 | Val Acc: 0.8415\n",
      "Epoch 45/300 | Loss: 9.2296 | Val Acc: 0.8415\n",
      "Epoch 46/300 | Loss: 9.1031 | Val Acc: 0.8415\n",
      "Epoch 47/300 | Loss: 8.9024 | Val Acc: 0.8415\n",
      "Epoch 48/300 | Loss: 9.3684 | Val Acc: 0.8415\n",
      "Epoch 49/300 | Loss: 9.1224 | Val Acc: 0.8415\n",
      "Epoch 50/300 | Loss: 9.1523 | Val Acc: 0.8415\n",
      "Epoch 51/300 | Loss: 9.0422 | Val Acc: 0.8415\n",
      "Epoch 52/300 | Loss: 9.1363 | Val Acc: 0.8415\n",
      "Epoch 53/300 | Loss: 9.1883 | Val Acc: 0.8415\n",
      "Epoch 54/300 | Loss: 8.6818 | Val Acc: 0.8415\n",
      "Epoch 55/300 | Loss: 8.7787 | Val Acc: 0.8415\n",
      "Epoch 56/300 | Loss: 9.0716 | Val Acc: 0.8415\n",
      "Epoch 57/300 | Loss: 8.8981 | Val Acc: 0.8415\n",
      "Epoch 58/300 | Loss: 9.0564 | Val Acc: 0.8415\n",
      "Epoch 59/300 | Loss: 9.1943 | Val Acc: 0.8415\n",
      "Epoch 60/300 | Loss: 9.0000 | Val Acc: 0.8415\n",
      "Epoch 61/300 | Loss: 8.7164 | Val Acc: 0.8415\n",
      "Epoch 62/300 | Loss: 9.2025 | Val Acc: 0.8415\n",
      "Epoch 63/300 | Loss: 9.1387 | Val Acc: 0.8415\n",
      "Epoch 64/300 | Loss: 9.3548 | Val Acc: 0.8415\n",
      "Epoch 65/300 | Loss: 9.2721 | Val Acc: 0.8415\n",
      "Epoch 66/300 | Loss: 8.9541 | Val Acc: 0.8415\n",
      "Epoch 67/300 | Loss: 9.2384 | Val Acc: 0.8415\n",
      "Epoch 68/300 | Loss: 9.1090 | Val Acc: 0.8415\n",
      "Epoch 69/300 | Loss: 9.0721 | Val Acc: 0.8415\n",
      "Epoch 70/300 | Loss: 9.1456 | Val Acc: 0.8415\n",
      "Epoch 71/300 | Loss: 9.2151 | Val Acc: 0.8415\n",
      "Epoch 72/300 | Loss: 8.7259 | Val Acc: 0.8415\n",
      "Epoch 73/300 | Loss: 9.0234 | Val Acc: 0.8415\n",
      "Epoch 74/300 | Loss: 8.8332 | Val Acc: 0.8415\n",
      "Epoch 75/300 | Loss: 9.0770 | Val Acc: 0.8415\n",
      "Epoch 76/300 | Loss: 8.9126 | Val Acc: 0.8415\n",
      "Epoch 77/300 | Loss: 8.7298 | Val Acc: 0.8415\n",
      "Epoch 78/300 | Loss: 9.1642 | Val Acc: 0.8415\n",
      "Epoch 79/300 | Loss: 9.1261 | Val Acc: 0.8415\n",
      "Epoch 80/300 | Loss: 8.9522 | Val Acc: 0.8415\n",
      "Epoch 81/300 | Loss: 8.9170 | Val Acc: 0.8415\n",
      "Epoch 82/300 | Loss: 9.0235 | Val Acc: 0.8415\n",
      "Epoch 83/300 | Loss: 8.8048 | Val Acc: 0.8415\n",
      "Epoch 84/300 | Loss: 8.9401 | Val Acc: 0.8415\n",
      "Epoch 85/300 | Loss: 8.8269 | Val Acc: 0.8415\n",
      "Epoch 86/300 | Loss: 9.0966 | Val Acc: 0.8415\n",
      "Epoch 87/300 | Loss: 8.6075 | Val Acc: 0.8415\n",
      "Epoch 88/300 | Loss: 9.1096 | Val Acc: 0.8415\n",
      "Epoch 89/300 | Loss: 8.9143 | Val Acc: 0.8415\n",
      "Epoch 90/300 | Loss: 8.6106 | Val Acc: 0.8415\n",
      "Epoch 91/300 | Loss: 9.1073 | Val Acc: 0.8415\n",
      "Epoch 92/300 | Loss: 9.0574 | Val Acc: 0.8415\n",
      "Epoch 93/300 | Loss: 8.6548 | Val Acc: 0.8415\n",
      "Epoch 94/300 | Loss: 9.1094 | Val Acc: 0.8415\n",
      "Epoch 95/300 | Loss: 9.0899 | Val Acc: 0.8415\n",
      "Epoch 96/300 | Loss: 8.8319 | Val Acc: 0.8415\n",
      "Epoch 97/300 | Loss: 9.1010 | Val Acc: 0.8415\n",
      "Epoch 98/300 | Loss: 9.0845 | Val Acc: 0.8415\n",
      "Epoch 99/300 | Loss: 9.0765 | Val Acc: 0.8415\n",
      "Epoch 100/300 | Loss: 9.1025 | Val Acc: 0.8415\n",
      "Epoch 101/300 | Loss: 8.8827 | Val Acc: 0.8415\n",
      "Epoch 102/300 | Loss: 9.5688 | Val Acc: 0.8415\n",
      "Epoch 103/300 | Loss: 9.1080 | Val Acc: 0.8415\n",
      "Epoch 104/300 | Loss: 9.6302 | Val Acc: 0.8415\n",
      "Epoch 105/300 | Loss: 9.1610 | Val Acc: 0.8415\n",
      "Epoch 106/300 | Loss: 9.0810 | Val Acc: 0.8415\n",
      "Epoch 107/300 | Loss: 8.8911 | Val Acc: 0.8415\n",
      "Epoch 108/300 | Loss: 9.0531 | Val Acc: 0.8415\n",
      "Epoch 109/300 | Loss: 8.9316 | Val Acc: 0.8415\n",
      "Epoch 110/300 | Loss: 9.0125 | Val Acc: 0.8415\n",
      "Epoch 111/300 | Loss: 8.8549 | Val Acc: 0.8415\n",
      "Epoch 112/300 | Loss: 9.2582 | Val Acc: 0.8415\n",
      "⛔ Early stopping triggered!\n",
      "\n",
      "✅ Validation Accuracy: 0.8414634146341463\n",
      "\n",
      "✅ Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.90      0.84        21\n",
      "           1       0.86      0.89      0.88        36\n",
      "           2       0.86      0.72      0.78        25\n",
      "\n",
      "    accuracy                           0.84        82\n",
      "   macro avg       0.84      0.84      0.83        82\n",
      "weighted avg       0.84      0.84      0.84        82\n",
      "\n",
      "\n",
      "✅ Confusion Matrix:\n",
      " [[19  1  1]\n",
      " [ 2 32  2]\n",
      " [ 3  4 18]]\n"
     ]
    }
   ],
   "source": [
    "# ===========================================================\n",
    "# ✅ Improved MLP Model for Tabular + Audio Embeddings (Reproducible)\n",
    "# ===========================================================\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# -----------------------------\n",
    "# ✅ Set Random Seed for Reproducibility\n",
    "# -----------------------------\n",
    "RANDOM_STATE = 42\n",
    "random.seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "torch.cuda.manual_seed_all(RANDOM_STATE)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# -----------------------------\n",
    "# ✅ Improved MLP Architecture\n",
    "# -----------------------------\n",
    "class TabAudioMLP(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 768),\n",
    "            nn.LayerNorm(768),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.65),\n",
    "\n",
    "            nn.Linear(768, 384),\n",
    "            nn.LayerNorm(384),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.55),\n",
    "\n",
    "            nn.Linear(384, 192),\n",
    "            nn.LayerNorm(192),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.45),\n",
    "\n",
    "            nn.Linear(192, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# -----------------------------\n",
    "# ✅ Initialize Model\n",
    "# -----------------------------\n",
    "model = TabAudioMLP(X_train.shape[1], num_classes).to(device)\n",
    "\n",
    "# ✅ Label smoothing improves generalization\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "# ✅ AdamW optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
    "\n",
    "# ✅ Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='max', factor=0.3, patience=5, verbose=True\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# ✅ Training Loop with Early Stopping\n",
    "# -----------------------------\n",
    "epochs = 300\n",
    "best_acc = 0\n",
    "patience = 100\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(xb)\n",
    "        loss = criterion(outputs, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # -----------------------------\n",
    "    # ✅ Validation\n",
    "    # -----------------------------\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for xb, _ in val_loader:\n",
    "            xb = xb.to(device)\n",
    "            preds.extend(torch.argmax(model(xb), dim=1).cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(y_val, preds)\n",
    "    scheduler.step(acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Loss: {total_loss:.4f} | Val Acc: {acc:.4f}\")\n",
    "\n",
    "    # ✅ Early stopping\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"⛔ Early stopping triggered!\")\n",
    "            break\n",
    "\n",
    "# -----------------------------\n",
    "# ✅ Final Evaluation\n",
    "# -----------------------------\n",
    "print(\"\\n✅ Validation Accuracy:\", accuracy_score(y_val, preds))\n",
    "print(\"\\n✅ Classification Report:\\n\", classification_report(y_val, preds))\n",
    "print(\"\\n✅ Confusion Matrix:\\n\", confusion_matrix(y_val, preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "939baee9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T19:41:48.070271Z",
     "iopub.status.busy": "2025-11-25T19:41:48.069873Z",
     "iopub.status.idle": "2025-11-25T19:42:02.402961Z",
     "shell.execute_reply": "2025-11-25T19:42:02.401016Z"
    },
    "papermill": {
     "duration": 14.339128,
     "end_time": "2025-11-25T19:42:02.404419",
     "exception": false,
     "start_time": "2025-11-25T19:41:48.065291",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Submission file created: submission.csv\n"
     ]
    }
   ],
   "source": [
    "# ===========================================================\n",
    "# ✅ Prepare Kaggle Submission (Tabular + Audio)\n",
    "# ===========================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import librosa\n",
    "\n",
    "# -----------------------------\n",
    "# 1️⃣ Load test CSV\n",
    "# -----------------------------\n",
    "test_csv = \"/kaggle/input/airs-ai-in-respiratory-sounds/test.csv\"\n",
    "test_df = pd.read_csv(test_csv)\n",
    "test_ids = test_df['candidateID'].values\n",
    "\n",
    "# -----------------------------\n",
    "# 2️⃣ Extract tabular features\n",
    "# -----------------------------\n",
    "tab_features = [\n",
    "    'age','gender','tbContactHistory','wheezingHistory','phlegmCough',\n",
    "    'familyAsthmaHistory','feverHistory','coldPresent','packYears'\n",
    "]\n",
    "X_test_tab = test_df[tab_features].values\n",
    "\n",
    "# Impute missing tabular values using training imputer\n",
    "X_test_tab = imputer.transform(X_test_tab)\n",
    "# Scale tabular features using training scaler\n",
    "X_test_tab = scaler.transform(X_test_tab)\n",
    "\n",
    "# -----------------------------\n",
    "# 3️⃣ Extract audio embeddings\n",
    "# -----------------------------\n",
    "audio_embs_test = []\n",
    "for cid in test_df['candidateID']:\n",
    "    if cid in file_map:  # use same mapping as train\n",
    "        y, _ = librosa.load(file_map[cid], sr=SR)\n",
    "        if len(y) < FIXED_LENGTH:\n",
    "            y = np.pad(y, (0, FIXED_LENGTH - len(y)))\n",
    "        else:\n",
    "            y = y[:FIXED_LENGTH]\n",
    "        y = y + 0.001 * np.random.randn(len(y))  # light noise\n",
    "        # WAV2VEC embedding\n",
    "        t = torch.tensor(y, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            feat, _ = wav2vec_model.extract_features(t)\n",
    "        emb = torch.mean(feat[-1], dim=1).cpu().numpy().squeeze()\n",
    "        audio_embs_test.append(emb)\n",
    "    else:\n",
    "        # if no audio, fill zeros\n",
    "        audio_embs_test.append(np.zeros(audio_embs.shape[1]))\n",
    "\n",
    "audio_embs_test = np.array(audio_embs_test)\n",
    "\n",
    "# -----------------------------\n",
    "# 4️⃣ Combine tabular + audio\n",
    "# -----------------------------\n",
    "X_test_combined = np.hstack([X_test_tab, audio_embs_test])\n",
    "X_test_tensor = torch.tensor(X_test_combined, dtype=torch.float32).to(device)\n",
    "\n",
    "# -----------------------------\n",
    "# 5️⃣ Make predictions\n",
    "# -----------------------------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_preds = torch.argmax(model(X_test_tensor), dim=1).cpu().numpy()\n",
    "\n",
    "# -----------------------------\n",
    "# 6️⃣ Prepare submission CSV\n",
    "# -----------------------------\n",
    "submission = pd.DataFrame({\n",
    "    \"candidateID\": test_ids,\n",
    "    \"target\": test_preds\n",
    "})\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"✅ Submission file created: submission.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 10191418,
     "sourceId": 87331,
     "sourceType": "competition"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 85.019859,
   "end_time": "2025-11-25T19:42:05.091922",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-25T19:40:40.072063",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
