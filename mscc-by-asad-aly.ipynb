{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f664184",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-26T05:08:31.015618Z",
     "iopub.status.busy": "2025-11-26T05:08:31.014999Z",
     "iopub.status.idle": "2025-11-26T05:08:31.021456Z",
     "shell.execute_reply": "2025-11-26T05:08:31.020740Z"
    },
    "papermill": {
     "duration": 0.010844,
     "end_time": "2025-11-26T05:08:31.022527",
     "exception": false,
     "start_time": "2025-11-26T05:08:31.011683",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Defining the path for parent directory\n",
    "DIR = '/kaggle/input/airs-ai-in-respiratory-sounds/'\n",
    "# Defining all paths\n",
    "train_path = '/kaggle/input/airs-ai-in-respiratory-sounds/train.csv'\n",
    "test_path = '/kaggle/input/airs-ai-in-respiratory-sounds/test.csv'\n",
    "sound_files_path = '/kaggle/input/airs-ai-in-respiratory-sounds/sounds/sounds/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "649e61b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T05:08:31.026829Z",
     "iopub.status.busy": "2025-11-26T05:08:31.026613Z",
     "iopub.status.idle": "2025-11-26T05:09:21.493340Z",
     "shell.execute_reply": "2025-11-26T05:09:21.492518Z"
    },
    "papermill": {
     "duration": 50.472028,
     "end_time": "2025-11-26T05:09:21.496391",
     "exception": false,
     "start_time": "2025-11-26T05:08:31.024363",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/torchaudio/models/wav2vec2_fairseq_base_ls960.pth\" to /root/.cache/torch/hub/checkpoints/wav2vec2_fairseq_base_ls960.pth\n",
      "100%|██████████| 360M/360M [00:01<00:00, 213MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total mapped audio files: 882\n",
      "Audio embeddings shape: (546, 768)\n"
     ]
    }
   ],
   "source": [
    "# ======================================\n",
    "# 1️⃣ IMPORTS + DATA + WAV2VEC EMBEDDINGS\n",
    "# ======================================\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchaudio\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# ✅ Device selection\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ✅ Paths\n",
    "train_csv = \"/kaggle/input/airs-ai-in-respiratory-sounds/train.csv\"\n",
    "audio_path = \"/kaggle/input/airs-ai-in-respiratory-sounds/sounds/sounds\"\n",
    "\n",
    "# ✅ Load dataset\n",
    "train_df = pd.read_csv(train_csv)\n",
    "\n",
    "# ✅ Tabular features\n",
    "tab_features = [\n",
    "    'age','gender','tbContactHistory','wheezingHistory','phlegmCough',\n",
    "    'familyAsthmaHistory','feverHistory','coldPresent','packYears'\n",
    "]\n",
    "\n",
    "# ✅ WAV2VEC2 model\n",
    "bundle = torchaudio.pipelines.WAV2VEC2_BASE\n",
    "wav2vec_model = bundle.get_model().to(device).eval()\n",
    "SR = 16000\n",
    "FIXED_LENGTH = 5 * SR  # 5 seconds audio\n",
    "\n",
    "# ✅ Map audio files\n",
    "file_map = {}\n",
    "for folder in os.listdir(audio_path):\n",
    "    fpath = os.path.join(audio_path, folder)\n",
    "    if os.path.isdir(fpath):\n",
    "        wavs = [f for f in os.listdir(fpath) if f.endswith(\".wav\")]\n",
    "        if wavs:\n",
    "            file_map[folder] = os.path.join(fpath, wavs[0])\n",
    "\n",
    "print(\"Total mapped audio files:\", len(file_map))\n",
    "\n",
    "\n",
    "# ✅ Audio loader + augmentation\n",
    "def load_audio(file_id):\n",
    "    y, sr = librosa.load(file_map[file_id], sr=SR)\n",
    "    if len(y) < FIXED_LENGTH:\n",
    "        y = np.pad(y, (0, FIXED_LENGTH - len(y)))\n",
    "    else:\n",
    "        y = y[:FIXED_LENGTH]\n",
    "    y = y + 0.001 * np.random.randn(len(y))  # light noise\n",
    "    return y\n",
    "\n",
    "# ✅ Extract WAV2VEC embeddings\n",
    "def get_embedding(audio):\n",
    "    t = torch.tensor(audio, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        feat, _ = wav2vec_model.extract_features(t)\n",
    "    return torch.mean(feat[-1], dim=1).cpu().numpy().squeeze()\n",
    "\n",
    "\n",
    "# ✅ Build embedding dataset\n",
    "audio_embs = []\n",
    "valid_ids = []\n",
    "\n",
    "for cid in train_df[\"candidateID\"]:\n",
    "    if cid not in file_map:\n",
    "        continue\n",
    "    emb = get_embedding(load_audio(cid))\n",
    "    audio_embs.append(emb)\n",
    "    valid_ids.append(cid)\n",
    "\n",
    "audio_embs = np.array(audio_embs)\n",
    "\n",
    "# ✅ Filter rows with audio\n",
    "df = train_df[train_df[\"candidateID\"].isin(valid_ids)]\n",
    "\n",
    "# ✅ Extract tabular + target\n",
    "X_tab = df[tab_features].values\n",
    "y = df[\"disease\"].values\n",
    "\n",
    "print(\"Audio embeddings shape:\", audio_embs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd5d21f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T05:09:21.502854Z",
     "iopub.status.busy": "2025-11-26T05:09:21.502148Z",
     "iopub.status.idle": "2025-11-26T05:09:21.521219Z",
     "shell.execute_reply": "2025-11-26T05:09:21.520446Z"
    },
    "papermill": {
     "duration": 0.023558,
     "end_time": "2025-11-26T05:09:21.522444",
     "exception": false,
     "start_time": "2025-11-26T05:09:21.498886",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final feature shape: (546, 777)\n"
     ]
    }
   ],
   "source": [
    "# ======================================\n",
    "# 2️⃣ PREPROCESS — IMPUTE + SCALE + COMBINE\n",
    "# ======================================\n",
    "\n",
    "# ✅ Impute missing tabular values\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "X_tab = imputer.fit_transform(X_tab)\n",
    "\n",
    "# ✅ Scale tabular features (audio already normalized)\n",
    "scaler = StandardScaler()\n",
    "X_tab = scaler.fit_transform(X_tab)\n",
    "\n",
    "# ✅ Combine tabular + audio embeddings\n",
    "X = np.hstack([X_tab, audio_embs])\n",
    "print(\"Final feature shape:\", X.shape)\n",
    "\n",
    "# ✅ Train-val split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.15, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# ✅ Convert to tensors\n",
    "import torch\n",
    "X_train_t = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "X_val_t = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "y_val_t = torch.tensor(y_val, dtype=torch.long).to(device)\n",
    "\n",
    "# ✅ DataLoaders\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_loader = DataLoader(TensorDataset(X_train_t, y_train_t), batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val_t, y_val_t), batch_size=32, shuffle=False)\n",
    "\n",
    "num_classes = len(np.unique(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ee14610",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T05:09:21.529481Z",
     "iopub.status.busy": "2025-11-26T05:09:21.528957Z",
     "iopub.status.idle": "2025-11-26T05:09:29.425766Z",
     "shell.execute_reply": "2025-11-26T05:09:29.424934Z"
    },
    "papermill": {
     "duration": 7.901754,
     "end_time": "2025-11-26T05:09:29.427053",
     "exception": false,
     "start_time": "2025-11-26T05:09:21.525299",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300 | Loss: 17.1867 | Val Acc: 0.4512\n",
      "Epoch 2/300 | Loss: 16.3267 | Val Acc: 0.4512\n",
      "Epoch 3/300 | Loss: 15.3577 | Val Acc: 0.5244\n",
      "Epoch 4/300 | Loss: 15.2713 | Val Acc: 0.6220\n",
      "Epoch 5/300 | Loss: 15.0969 | Val Acc: 0.6829\n",
      "Epoch 6/300 | Loss: 14.7380 | Val Acc: 0.6585\n",
      "Epoch 7/300 | Loss: 14.4319 | Val Acc: 0.6951\n",
      "Epoch 8/300 | Loss: 13.4776 | Val Acc: 0.8171\n",
      "Epoch 9/300 | Loss: 12.8820 | Val Acc: 0.8171\n",
      "Epoch 10/300 | Loss: 12.8099 | Val Acc: 0.8293\n",
      "Epoch 11/300 | Loss: 11.4526 | Val Acc: 0.8415\n",
      "Epoch 12/300 | Loss: 11.4599 | Val Acc: 0.8415\n",
      "Epoch 13/300 | Loss: 11.3312 | Val Acc: 0.8537\n",
      "Epoch 14/300 | Loss: 10.7556 | Val Acc: 0.8659\n",
      "Epoch 15/300 | Loss: 10.5145 | Val Acc: 0.8537\n",
      "Epoch 16/300 | Loss: 10.1659 | Val Acc: 0.8537\n",
      "Epoch 17/300 | Loss: 10.2356 | Val Acc: 0.8537\n",
      "Epoch 18/300 | Loss: 9.7034 | Val Acc: 0.8659\n",
      "Epoch 19/300 | Loss: 9.6946 | Val Acc: 0.8659\n",
      "Epoch 20/300 | Loss: 9.5236 | Val Acc: 0.8659\n",
      "Epoch 21/300 | Loss: 9.4512 | Val Acc: 0.8659\n",
      "Epoch 22/300 | Loss: 9.5962 | Val Acc: 0.8659\n",
      "Epoch 23/300 | Loss: 9.5862 | Val Acc: 0.8659\n",
      "Epoch 24/300 | Loss: 9.6247 | Val Acc: 0.8659\n",
      "Epoch 25/300 | Loss: 9.8926 | Val Acc: 0.8659\n",
      "Epoch 26/300 | Loss: 9.3751 | Val Acc: 0.8659\n",
      "Epoch 27/300 | Loss: 9.5679 | Val Acc: 0.8659\n",
      "Epoch 28/300 | Loss: 9.4282 | Val Acc: 0.8659\n",
      "Epoch 29/300 | Loss: 9.5957 | Val Acc: 0.8659\n",
      "Epoch 30/300 | Loss: 9.7456 | Val Acc: 0.8659\n",
      "Epoch 31/300 | Loss: 8.9696 | Val Acc: 0.8659\n",
      "Epoch 32/300 | Loss: 9.4530 | Val Acc: 0.8659\n",
      "Epoch 33/300 | Loss: 9.4295 | Val Acc: 0.8659\n",
      "Epoch 34/300 | Loss: 9.4361 | Val Acc: 0.8659\n",
      "Epoch 35/300 | Loss: 9.4918 | Val Acc: 0.8659\n",
      "Epoch 36/300 | Loss: 9.2551 | Val Acc: 0.8659\n",
      "Epoch 37/300 | Loss: 9.6722 | Val Acc: 0.8659\n",
      "Epoch 38/300 | Loss: 9.4511 | Val Acc: 0.8659\n",
      "Epoch 39/300 | Loss: 9.5127 | Val Acc: 0.8659\n",
      "Epoch 40/300 | Loss: 9.3278 | Val Acc: 0.8659\n",
      "Epoch 41/300 | Loss: 9.2795 | Val Acc: 0.8659\n",
      "Epoch 42/300 | Loss: 9.4698 | Val Acc: 0.8659\n",
      "Epoch 43/300 | Loss: 9.3178 | Val Acc: 0.8659\n",
      "Epoch 44/300 | Loss: 9.0127 | Val Acc: 0.8659\n",
      "⛔ Early stopping triggered!\n",
      "\n",
      "✅ Validation Accuracy: 0.8658536585365854\n",
      "\n",
      "✅ Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.95      0.89        21\n",
      "           1       0.89      0.92      0.90        36\n",
      "           2       0.86      0.72      0.78        25\n",
      "\n",
      "    accuracy                           0.87        82\n",
      "   macro avg       0.86      0.86      0.86        82\n",
      "weighted avg       0.87      0.87      0.86        82\n",
      "\n",
      "\n",
      "✅ Confusion Matrix:\n",
      " [[20  0  1]\n",
      " [ 1 33  2]\n",
      " [ 3  4 18]]\n"
     ]
    }
   ],
   "source": [
    "# ===========================================================\n",
    "# ✅ Improved MLP Model for Tabular + Audio Embeddings (Reproducible)\n",
    "# ===========================================================\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# -----------------------------\n",
    "# ✅ Set Random Seed for Reproducibility\n",
    "# -----------------------------\n",
    "RANDOM_STATE = 42\n",
    "random.seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "torch.cuda.manual_seed_all(RANDOM_STATE)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# -----------------------------\n",
    "# ✅ Improved MLP Architecture\n",
    "# -----------------------------\n",
    "class TabAudioMLP(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 768),\n",
    "            nn.LayerNorm(768),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.65),\n",
    "\n",
    "            nn.Linear(768, 384),\n",
    "            nn.LayerNorm(384),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.55),\n",
    "\n",
    "            nn.Linear(384, 192),\n",
    "            nn.LayerNorm(192),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.45),\n",
    "\n",
    "            nn.Linear(192, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# -----------------------------\n",
    "# ✅ Initialize Model\n",
    "# -----------------------------\n",
    "model = TabAudioMLP(X_train.shape[1], num_classes).to(device)\n",
    "\n",
    "# ✅ Label smoothing improves generalization\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "# ✅ AdamW optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
    "\n",
    "# ✅ Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='max', factor=0.2, patience=3, verbose=True\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# ✅ Training Loop with Early Stopping\n",
    "# -----------------------------\n",
    "epochs = 300\n",
    "best_acc = 0\n",
    "patience = 30\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(xb)\n",
    "        loss = criterion(outputs, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # -----------------------------\n",
    "    # ✅ Validation\n",
    "    # -----------------------------\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for xb, _ in val_loader:\n",
    "            xb = xb.to(device)\n",
    "            preds.extend(torch.argmax(model(xb), dim=1).cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(y_val, preds)\n",
    "    scheduler.step(acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Loss: {total_loss:.4f} | Val Acc: {acc:.4f}\")\n",
    "\n",
    "    # ✅ Early stopping\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"⛔ Early stopping triggered!\")\n",
    "            break\n",
    "\n",
    "# -----------------------------\n",
    "# ✅ Final Evaluation\n",
    "# -----------------------------\n",
    "print(\"\\n✅ Validation Accuracy:\", accuracy_score(y_val, preds))\n",
    "print(\"\\n✅ Classification Report:\\n\", classification_report(y_val, preds))\n",
    "print(\"\\n✅ Confusion Matrix:\\n\", confusion_matrix(y_val, preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcacf452",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T05:09:29.434894Z",
     "iopub.status.busy": "2025-11-26T05:09:29.434337Z",
     "iopub.status.idle": "2025-11-26T05:09:29.438799Z",
     "shell.execute_reply": "2025-11-26T05:09:29.438064Z"
    },
    "papermill": {
     "duration": 0.009632,
     "end_time": "2025-11-26T05:09:29.440163",
     "exception": false,
     "start_time": "2025-11-26T05:09:29.430531",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # ===========================================================\n",
    "# # ✅ Prepare Kaggle Submission (Tabular + Audio)\n",
    "# # ===========================================================\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import librosa\n",
    "\n",
    "# # -----------------------------\n",
    "# # 1️⃣ Load test CSV\n",
    "# # -----------------------------\n",
    "# test_csv = \"/kaggle/input/airs-ai-in-respiratory-sounds/test.csv\"\n",
    "# test_df = pd.read_csv(test_csv)\n",
    "# test_ids = test_df['candidateID'].values\n",
    "\n",
    "# # -----------------------------\n",
    "# # 2️⃣ Extract tabular features\n",
    "# # -----------------------------\n",
    "# tab_features = [\n",
    "#     'age','gender','tbContactHistory','wheezingHistory','phlegmCough',\n",
    "#     'familyAsthmaHistory','feverHistory','coldPresent','packYears'\n",
    "# ]\n",
    "# X_test_tab = test_df[tab_features].values\n",
    "\n",
    "# # Impute missing tabular values using training imputer\n",
    "# X_test_tab = imputer.transform(X_test_tab)\n",
    "# # Scale tabular features using training scaler\n",
    "# X_test_tab = scaler.transform(X_test_tab)\n",
    "\n",
    "# # -----------------------------\n",
    "# # 3️⃣ Extract audio embeddings\n",
    "# # -----------------------------\n",
    "# audio_embs_test = []\n",
    "# for cid in test_df['candidateID']:\n",
    "#     if cid in file_map:  # use same mapping as train\n",
    "#         y, _ = librosa.load(file_map[cid], sr=SR)\n",
    "#         if len(y) < FIXED_LENGTH:\n",
    "#             y = np.pad(y, (0, FIXED_LENGTH - len(y)))\n",
    "#         else:\n",
    "#             y = y[:FIXED_LENGTH]\n",
    "#         y = y + 0.001 * np.random.randn(len(y))  # light noise\n",
    "#         # WAV2VEC embedding\n",
    "#         t = torch.tensor(y, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "#         with torch.no_grad():\n",
    "#             feat, _ = wav2vec_model.extract_features(t)\n",
    "#         emb = torch.mean(feat[-1], dim=1).cpu().numpy().squeeze()\n",
    "#         audio_embs_test.append(emb)\n",
    "#     else:\n",
    "#         # if no audio, fill zeros\n",
    "#         audio_embs_test.append(np.zeros(audio_embs.shape[1]))\n",
    "\n",
    "# audio_embs_test = np.array(audio_embs_test)\n",
    "\n",
    "# # -----------------------------\n",
    "# # 4️⃣ Combine tabular + audio\n",
    "# # -----------------------------\n",
    "# X_test_combined = np.hstack([X_test_tab, audio_embs_test])\n",
    "# X_test_tensor = torch.tensor(X_test_combined, dtype=torch.float32).to(device)\n",
    "\n",
    "# # -----------------------------\n",
    "# # 5️⃣ Make predictions\n",
    "# # -----------------------------\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     test_preds = torch.argmax(model(X_test_tensor), dim=1).cpu().numpy()\n",
    "\n",
    "# # -----------------------------\n",
    "# # 6️⃣ Prepare submission CSV\n",
    "# # -----------------------------\n",
    "# submission = pd.DataFrame({\n",
    "#     \"candidateID\": test_ids,\n",
    "#     \"disease\": test_preds\n",
    "# })\n",
    "# submission.to_csv(\"submission.csv\", index=False)\n",
    "# print(\"✅ Submission file created: submission.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 10191418,
     "sourceId": 87331,
     "sourceType": "competition"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 65.037077,
   "end_time": "2025-11-26T05:09:32.212341",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-26T05:08:27.175264",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
