{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":87331,"databundleVersionId":10191418,"sourceType":"competition"}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ======================================\n# Complete Preprocessing + Train/Test Handling (Combined Snippet)\n# ======================================\n\nimport os\nimport librosa\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import accuracy_score\nimport tensorflow as tf\nfrom tensorflow.keras import layers, Model\n\n\n# -----------------------------\n# 1ï¸âƒ£ Load Data\n# -----------------------------\ntrain_csv = \"/kaggle/input/airs-ai-in-respiratory-sounds/train.csv\"\ntest_csv  = \"/kaggle/input/airs-ai-in-respiratory-sounds/test.csv\"\naudio_path = \"/kaggle/input/airs-ai-in-respiratory-sounds/sounds/sounds\"\n\ntrain_df = pd.read_csv(train_csv)\ntest_df  = pd.read_csv(test_csv)\n\ntab_features = ['age', 'gender', 'tbContactHistory', 'wheezingHistory', 'phlegmCough',\n                'familyAsthmaHistory', 'feverHistory', 'coldPresent', 'packYears']\n\n# -----------------------------\n# 2ï¸âƒ£ Map audio files\n# -----------------------------\nfile_map = {}\nfor folder in os.listdir(audio_path):\n    fpath = os.path.join(audio_path, folder)\n    if os.path.isdir(fpath):\n        wavs = [f for f in os.listdir(fpath) if f.endswith(\".wav\")]\n        if wavs:\n            file_map[folder] = os.path.join(fpath, wavs[0])\n\n# -----------------------------\n# 3ï¸âƒ£ MFCC Extraction Function\n# -----------------------------\ndef extract_mfcc_features(file_path, n_mfcc=40, duration=5, sr=22050):\n    try:\n        y, sr = librosa.load(file_path, sr=sr, duration=duration)\n        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n        mfcc_scaled = np.mean(mfcc.T, axis=0)\n        chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n        chroma_scaled = np.mean(chroma.T, axis=0)\n        mel = librosa.feature.melspectrogram(y=y, sr=sr)\n        mel_scaled = np.mean(mel.T, axis=0)\n        features = np.hstack([mfcc_scaled, chroma_scaled, mel_scaled])\n        return features\n    except:\n        return np.zeros(n_mfcc + 12 + 128)\n\n# -----------------------------\n# 4ï¸âƒ£ Extract features for train\n# -----------------------------\nX_audio = []\nvalid_ids = []\nfor i, cid in enumerate(train_df['candidateID']):\n    if cid in file_map:\n        X_audio.append(extract_mfcc_features(file_map[cid]))\n        valid_ids.append(cid)\nX_audio = np.array(X_audio)\n\ndf = train_df[train_df['candidateID'].isin(valid_ids)]\nX_tab = df[tab_features].values\ny = df['disease'].values\n\n# -----------------------------\n# 5ï¸âƒ£ Extract features for test\n# -----------------------------\nX_audio_test = []\nfor i, cid in enumerate(test_df['candidateID']):\n    if cid in file_map:\n        X_audio_test.append(extract_mfcc_features(file_map[cid]))\nX_audio_test = np.array(X_audio_test)\nX_tab_test = test_df[tab_features].values\n\n# -----------------------------\n# 6ï¸âƒ£ Iterative Imputation + Scaling (Train + Test)\n# -----------------------------\niterative_imputer = IterativeImputer(\n    max_iter=10,\n    random_state=42,\n    estimator=RandomForestClassifier(n_estimators=50, random_state=42)\n)\nX_tab_iterative = iterative_imputer.fit_transform(X_tab)\nX_tab_test_iterative = iterative_imputer.transform(X_tab_test)  # apply same imputer\n\nscaler_tab = StandardScaler()\nX_tab_scaled = scaler_tab.fit_transform(X_tab_iterative)\nX_tab_scaled_test = scaler_tab.transform(X_tab_test_iterative)\n\n# -----------------------------\n# 7ï¸âƒ£ Train-validation split\n# -----------------------------\nX_tab_train, X_tab_val, X_audio_train, X_audio_val, y_train, y_val = train_test_split(\n    X_tab_scaled, X_audio, y,\n    test_size=0.15,\n    stratify=y,\n    random_state=42\n)\n\n# Reshape audio for model\nX_audio_train = X_audio_train.reshape(X_audio_train.shape[0], X_audio_train.shape[1], 1)\nX_audio_val = X_audio_val.reshape(X_audio_val.shape[0], X_audio_val.shape[1], 1)\nX_audio_test_fixed = X_audio_test.reshape(X_audio_test.shape[0], X_audio_test.shape[1], 1)\n\n# -----------------------------\n# 8ï¸âƒ£ Class Weights\n# -----------------------------\nclass_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\nclass_weight_dict = dict(enumerate(class_weights))\n\n# -----------------------------\n# 9ï¸âƒ£ Flatten MFCC for augmentation (Train only)\n# -----------------------------\nX_mel_2d = X_audio_train.reshape(X_audio_train.shape[0], -1)\nX_tab_mel = X_tab_train\ny_mel = y_train\n\ndef advanced_flat_augmentation(feature_vector):\n    augmented = feature_vector.copy()\n    if np.random.random() > 0.5:\n        augmented += np.random.normal(0, 0.01, augmented.shape)\n    if np.random.random() > 0.5:\n        augmented = np.roll(augmented, np.random.randint(-5,5))\n    return augmented\n\nX_mel_augmented = []\nX_tab_augmented = []\ny_augmented = []\n\n# Original train data\nX_mel_augmented.extend(X_mel_2d)\nX_tab_augmented.extend(X_tab_mel)\ny_augmented.extend(y_mel)\n\n# Augmented data (3x)\nfor i in range(len(X_mel_2d)):\n    for _ in range(3):\n        aug_mel = advanced_flat_augmentation(X_mel_2d[i])\n        X_mel_augmented.append(aug_mel)\n        X_tab_augmented.append(X_tab_mel[i])\n        y_augmented.append(y_mel[i])\n\nX_mel_augmented = np.array(X_mel_augmented)\nX_tab_augmented = np.array(X_tab_augmented)\ny_augmented = np.array(y_augmented)\n\nprint(\"âœ… Preprocessing complete! Ready for training + testing.\")\nprint(f\"Train samples (augmented): {len(X_mel_augmented)}, Test samples: {X_audio_test_fixed.shape[0]}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T17:49:30.517668Z","iopub.execute_input":"2025-12-07T17:49:30.517879Z","iopub.status.idle":"2025-12-07T17:50:52.011197Z","shell.execute_reply.started":"2025-12-07T17:49:30.517860Z","shell.execute_reply":"2025-12-07T17:50:52.010348Z"}},"outputs":[{"name":"stderr","text":"2025-12-07 17:49:35.135455: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1765129775.343883      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1765129775.403112      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/librosa/core/pitch.py:103: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/usr/local/lib/python3.11/dist-packages/librosa/core/pitch.py:103: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n","output_type":"stream"},{"name":"stdout","text":"âœ… Preprocessing complete! Ready for training + testing.\nTrain samples (augmented): 1856, Test samples: 338\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# -----------------------------\n# 7ï¸âƒ£ Train-validation split (85% train, 15% validation)\n# -----------------------------\nX_tab_train, X_tab_val, X_audio_train, X_audio_val, y_train, y_val = train_test_split(\n    X_tab_scaled, X_audio, y,\n    test_size=0.15,       # 15% for validation\n    stratify=y,\n    random_state=42\n)\n\n# Reshape audio for model\nX_audio_train = X_audio_train.reshape(X_audio_train.shape[0], X_audio_train.shape[1], 1)\nX_audio_val = X_audio_val.reshape(X_audio_val.shape[0], X_audio_val.shape[1], 1)\nX_audio_test_fixed = X_audio_test.reshape(X_audio_test.shape[0], X_audio_test.shape[1], 1)\n\nprint(f\"âœ… Training samples: {X_audio_train.shape[0]}, Validation samples: {X_audio_val.shape[0]}, Test samples: {X_audio_test_fixed.shape[0]}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T17:50:52.012607Z","iopub.execute_input":"2025-12-07T17:50:52.013524Z","iopub.status.idle":"2025-12-07T17:50:52.020727Z","shell.execute_reply.started":"2025-12-07T17:50:52.013501Z","shell.execute_reply":"2025-12-07T17:50:52.019985Z"}},"outputs":[{"name":"stdout","text":"âœ… Training samples: 464, Validation samples: 82, Test samples: 338\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# ======================================\n# FIXED SUPER ADVANCED MODEL TRAINING (No PCA)\n# ======================================\n\n# -----------------------------\n# 1ï¸âƒ£ Convert augmented lists to arrays\n# -----------------------------\nX_mel_augmented = np.array(X_mel_augmented)\nX_tab_augmented = np.array(X_tab_augmented)\ny_augmented = np.array(y_augmented)\n\n# -----------------------------\n# 2ï¸âƒ£ Train-validation split (85% train, 15% validation)\n# -----------------------------\nX_mel_aug_train, X_mel_aug_val, X_tab_aug_train, X_tab_aug_val, y_aug_train, y_aug_val = train_test_split(\n    X_mel_augmented, X_tab_augmented, y_augmented,\n    test_size=0.15,\n    stratify=y_augmented,\n    random_state=42\n)\n\n# -----------------------------\n# 3ï¸âƒ£ Flatten MFCC features\n# -----------------------------\nX_mel_aug_train_flat = X_mel_aug_train.reshape(X_mel_aug_train.shape[0], -1)\nX_mel_aug_val_flat   = X_mel_aug_val.reshape(X_mel_aug_val.shape[0], -1)\n\nprint(f\"âœ… Flattened MFCC features: Train {X_mel_aug_train_flat.shape}, Val {X_mel_aug_val_flat.shape}\")\n\n# -----------------------------\n# 4ï¸âƒ£ Create Fixed Super Advanced Model\n# -----------------------------\nnum_classes = 3\n\ndef create_fixed_super_advanced_model(tabular_dim, pretrained_dim, num_classes):\n    \"\"\"Fixed super advanced model\"\"\"\n    \n    pretrained_input = tf.keras.Input(shape=(pretrained_dim,), name='pretrained_features')\n    x_pre = layers.Dense(1024, activation='relu')(pretrained_input)\n    x_pre = layers.BatchNormalization()(x_pre)\n    x_pre = layers.Dropout(0.4)(x_pre)\n    x_pre = layers.Dense(512, activation='relu')(x_pre)\n    x_pre = layers.BatchNormalization()(x_pre)\n    x_pre = layers.Dropout(0.3)(x_pre)\n    x_pre = layers.Dense(256, activation='relu')(x_pre)\n    x_pre = layers.BatchNormalization()(x_pre)\n    x_pre = layers.Dropout(0.2)(x_pre)\n    \n    tabular_input = tf.keras.Input(shape=(tabular_dim,), name='tabular_input')\n    x_tab = layers.Dense(256, activation='relu')(tabular_input)\n    x_tab = layers.BatchNormalization()(x_tab)\n    x_tab = layers.Dropout(0.3)(x_tab)\n    x_tab = layers.Dense(128, activation='relu')(x_tab)\n    x_tab = layers.BatchNormalization()(x_tab)\n    x_tab = layers.Dropout(0.2)(x_tab)\n    x_tab = layers.Dense(64, activation='relu')(x_tab)\n    \n    combined = layers.concatenate([x_pre, x_tab])\n    x = layers.Dense(512, activation='relu')(combined)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.5)(x)\n    \n    residual1 = layers.Dense(512, activation='relu')(x)\n    residual1 = layers.BatchNormalization()(residual1)\n    x = layers.add([x, residual1])\n    x = layers.Dropout(0.4)(x)\n    \n    x = layers.Dense(256, activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    residual2 = layers.Dense(256, activation='relu')(x)\n    residual2 = layers.BatchNormalization()(residual2)\n    x = layers.add([x, residual2])\n    x = layers.Dropout(0.3)(x)\n    \n    x = layers.Dense(128, activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.2)(x)\n    x = layers.Dense(64, activation='relu')(x)\n    \n    output = layers.Dense(num_classes, activation='softmax')(x)\n    \n    model = Model(inputs=[tabular_input, pretrained_input], outputs=output, name='fixed_super_advanced_model')\n    return model\n\nsuper_model = create_fixed_super_advanced_model(\n    tabular_dim=X_tab_aug_train.shape[1],\n    pretrained_dim=X_mel_aug_train_flat.shape[1],\n    num_classes=num_classes\n)\n\nsuper_model.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nprint(\"âœ… Model created and compiled!\")\nsuper_model.summary()\n\n# -----------------------------\n# 5ï¸âƒ£ Compute class weights\n# -----------------------------\nclass_weights = compute_class_weight('balanced', classes=np.unique(y_aug_train), y=y_aug_train)\nclass_weight_dict = dict(enumerate(class_weights))\nprint(f\"âœ… Class weights: {class_weight_dict}\")\n\n# -----------------------------\n# 6ï¸âƒ£ Callbacks\n# -----------------------------\ncallbacks = [\n    tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=40, restore_best_weights=True, verbose=1),\n    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=20, min_lr=1e-8, verbose=1),\n    tf.keras.callbacks.ModelCheckpoint('fixed_super_advanced_model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n]\n\n# -----------------------------\n# 7ï¸âƒ£ Train the model\n# -----------------------------\nsuper_history = super_model.fit(\n    [X_tab_aug_train, X_mel_aug_train_flat],\n    y_aug_train,\n    batch_size=16,\n    epochs=300,\n    validation_data=([X_tab_aug_val, X_mel_aug_val_flat], y_aug_val),\n    class_weight=class_weight_dict,\n    callbacks=callbacks,\n    verbose=1\n)\n\n# -----------------------------\n# 8ï¸âƒ£ Evaluate on validation\n# -----------------------------\n\n\nsuper_model.load_weights('fixed_super_advanced_model.h5')\nsuper_pred = super_model.predict([X_tab_aug_val, X_mel_aug_val_flat])\nsuper_accuracy = accuracy_score(y_aug_val, np.argmax(super_pred, axis=1))\nprint(f\"\\nğŸ¯ Validation Accuracy: {super_accuracy:.4f}\")\n\n# -----------------------------\n# 9ï¸âƒ£ Plot train / validation metrics\n# -----------------------------\nplt.figure(figsize=(12,5))\nplt.subplot(1,2,1)\nplt.plot(super_history.history['loss'], label='Train Loss')\nplt.plot(super_history.history['val_loss'], label='Val Loss')\nplt.title('Loss')\nplt.legend()\n\nplt.subplot(1,2,2)\nplt.plot(super_history.history['accuracy'], label='Train Accuracy')\nplt.plot(super_history.history['val_accuracy'], label='Val Accuracy')\nplt.title('Accuracy')\nplt.legend()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T17:50:52.021629Z","iopub.execute_input":"2025-12-07T17:50:52.021854Z","iopub.status.idle":"2025-12-07T17:53:30.033095Z","shell.execute_reply.started":"2025-12-07T17:50:52.021837Z","shell.execute_reply":"2025-12-07T17:53:30.032441Z"}},"outputs":[{"name":"stdout","text":"âœ… Flattened MFCC features: Train (1577, 180), Val (279, 180)\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1765129852.706262      47 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"},{"name":"stdout","text":"âœ… Model created and compiled!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"fixed_super_advanced_model\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"fixed_super_advanced_model\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ pretrained_features â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m180\u001b[0m)       â”‚          \u001b[38;5;34m0\u001b[0m â”‚ -                 â”‚\nâ”‚ (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense (\u001b[38;5;33mDense\u001b[0m)       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)      â”‚    \u001b[38;5;34m185,344\u001b[0m â”‚ pretrained_featuâ€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalization â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)      â”‚      \u001b[38;5;34m4,096\u001b[0m â”‚ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ tabular_input       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m)         â”‚          \u001b[38;5;34m0\u001b[0m â”‚ -                 â”‚\nâ”‚ (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)      â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_3 (\u001b[38;5;33mDense\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       â”‚      \u001b[38;5;34m2,560\u001b[0m â”‚ tabular_input[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       â”‚    \u001b[38;5;34m524,800\u001b[0m â”‚ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       â”‚      \u001b[38;5;34m1,024\u001b[0m â”‚ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       â”‚      \u001b[38;5;34m2,048\u001b[0m â”‚ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_3 (\u001b[38;5;33mDropout\u001b[0m) â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_4 (\u001b[38;5;33mDense\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚     \u001b[38;5;34m32,896\u001b[0m â”‚ dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_2 (\u001b[38;5;33mDense\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       â”‚    \u001b[38;5;34m131,328\u001b[0m â”‚ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚        \u001b[38;5;34m512\u001b[0m â”‚ dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       â”‚      \u001b[38;5;34m1,024\u001b[0m â”‚ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_4 (\u001b[38;5;33mDropout\u001b[0m) â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_2 (\u001b[38;5;33mDropout\u001b[0m) â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_5 (\u001b[38;5;33mDense\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        â”‚      \u001b[38;5;34m8,256\u001b[0m â”‚ dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ concatenate         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m320\u001b[0m)       â”‚          \u001b[38;5;34m0\u001b[0m â”‚ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  â”‚\nâ”‚ (\u001b[38;5;33mConcatenate\u001b[0m)       â”‚                   â”‚            â”‚ dense_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_6 (\u001b[38;5;33mDense\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       â”‚    \u001b[38;5;34m164,352\u001b[0m â”‚ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       â”‚      \u001b[38;5;34m2,048\u001b[0m â”‚ dense_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_5 (\u001b[38;5;33mDropout\u001b[0m) â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_7 (\u001b[38;5;33mDense\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       â”‚    \u001b[38;5;34m262,656\u001b[0m â”‚ dropout_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       â”‚      \u001b[38;5;34m2,048\u001b[0m â”‚ dense_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ add (\u001b[38;5;33mAdd\u001b[0m)           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       â”‚          \u001b[38;5;34m0\u001b[0m â”‚ dropout_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  â”‚\nâ”‚                     â”‚                   â”‚            â”‚ batch_normalizatâ€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_6 (\u001b[38;5;33mDropout\u001b[0m) â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       â”‚          \u001b[38;5;34m0\u001b[0m â”‚ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_8 (\u001b[38;5;33mDense\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       â”‚    \u001b[38;5;34m131,328\u001b[0m â”‚ dropout_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       â”‚      \u001b[38;5;34m1,024\u001b[0m â”‚ dense_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_9 (\u001b[38;5;33mDense\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       â”‚     \u001b[38;5;34m65,792\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       â”‚      \u001b[38;5;34m1,024\u001b[0m â”‚ dense_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ add_1 (\u001b[38;5;33mAdd\u001b[0m)         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\nâ”‚                     â”‚                   â”‚            â”‚ batch_normalizatâ€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_7 (\u001b[38;5;33mDropout\u001b[0m) â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       â”‚          \u001b[38;5;34m0\u001b[0m â”‚ add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_10 (\u001b[38;5;33mDense\u001b[0m)    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚     \u001b[38;5;34m32,896\u001b[0m â”‚ dropout_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚        \u001b[38;5;34m512\u001b[0m â”‚ dense_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_8 (\u001b[38;5;33mDropout\u001b[0m) â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_11 (\u001b[38;5;33mDense\u001b[0m)    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        â”‚      \u001b[38;5;34m8,256\u001b[0m â”‚ dropout_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_12 (\u001b[38;5;33mDense\u001b[0m)    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)         â”‚        \u001b[38;5;34m195\u001b[0m â”‚ dense_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ<span style=\"font-weight: bold\"> Layer (type)        </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape      </span>â”ƒ<span style=\"font-weight: bold\">    Param # </span>â”ƒ<span style=\"font-weight: bold\"> Connected to      </span>â”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ pretrained_features â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">180</span>)       â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                 â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)      â”‚    <span style=\"color: #00af00; text-decoration-color: #00af00\">185,344</span> â”‚ pretrained_featuâ€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalization â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)      â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span> â”‚ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ tabular_input       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)         â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                 â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)      â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560</span> â”‚ tabular_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       â”‚    <span style=\"color: #00af00; text-decoration-color: #00af00\">524,800</span> â”‚ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> â”‚ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> â”‚ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> â”‚ dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       â”‚    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> â”‚ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> â”‚ dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> â”‚ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> â”‚ dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ concatenate         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>)       â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       â”‚                   â”‚            â”‚ dense_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       â”‚    <span style=\"color: #00af00; text-decoration-color: #00af00\">164,352</span> â”‚ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> â”‚ dense_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       â”‚    <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span> â”‚ dropout_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> â”‚ dense_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ dropout_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  â”‚\nâ”‚                     â”‚                   â”‚            â”‚ batch_normalizatâ€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       â”‚    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> â”‚ dropout_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> â”‚ dense_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> â”‚ batch_normalizatâ€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> â”‚ dense_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ add_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\nâ”‚                     â”‚                   â”‚            â”‚ batch_normalizatâ€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> â”‚ dropout_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> â”‚ dense_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> â”‚ dropout_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">195</span> â”‚ dense_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,566,019\u001b[0m (5.97 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,566,019</span> (5.97 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,558,339\u001b[0m (5.94 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,558,339</span> (5.94 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m7,680\u001b[0m (30.00 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,680</span> (30.00 KB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"âœ… Class weights: {0: 1.301155115511551, 1: 0.7651625424551188, 2: 1.0816186556927299}\nEpoch 1/300\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1765129860.530235     114 service.cc:148] XLA service 0x7e08cc002aa0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1765129860.530666     114 service.cc:156]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\nI0000 00:00:1765129861.549182     114 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m39/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3084 - loss: 1.4424","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1765129867.367557     114 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.3155 - loss: 1.4108\nEpoch 1: val_accuracy improved from -inf to 0.28674, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 85ms/step - accuracy: 0.3155 - loss: 1.4105 - val_accuracy: 0.2867 - val_loss: 1.1573 - learning_rate: 1.0000e-04\nEpoch 2/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3378 - loss: 1.3088\nEpoch 2: val_accuracy improved from 0.28674 to 0.34767, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.3386 - loss: 1.3082 - val_accuracy: 0.3477 - val_loss: 1.1192 - learning_rate: 1.0000e-04\nEpoch 3/300\n\u001b[1m98/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3367 - loss: 1.3227\nEpoch 3: val_accuracy improved from 0.34767 to 0.41935, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.3371 - loss: 1.3217 - val_accuracy: 0.4194 - val_loss: 1.0777 - learning_rate: 1.0000e-04\nEpoch 4/300\n\u001b[1m95/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4022 - loss: 1.1964\nEpoch 4: val_accuracy did not improve from 0.41935\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4015 - loss: 1.1973 - val_accuracy: 0.4050 - val_loss: 1.0677 - learning_rate: 1.0000e-04\nEpoch 5/300\n\u001b[1m94/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3820 - loss: 1.2144\nEpoch 5: val_accuracy improved from 0.41935 to 0.44086, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.3822 - loss: 1.2136 - val_accuracy: 0.4409 - val_loss: 1.0426 - learning_rate: 1.0000e-04\nEpoch 6/300\n\u001b[1m93/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3828 - loss: 1.1806\nEpoch 6: val_accuracy improved from 0.44086 to 0.50538, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.3834 - loss: 1.1799 - val_accuracy: 0.5054 - val_loss: 1.0010 - learning_rate: 1.0000e-04\nEpoch 7/300\n\u001b[1m94/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4137 - loss: 1.1636\nEpoch 7: val_accuracy improved from 0.50538 to 0.59140, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4146 - loss: 1.1614 - val_accuracy: 0.5914 - val_loss: 0.9451 - learning_rate: 1.0000e-04\nEpoch 8/300\n\u001b[1m92/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4142 - loss: 1.1085\nEpoch 8: val_accuracy did not improve from 0.59140\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4147 - loss: 1.1089 - val_accuracy: 0.5878 - val_loss: 0.9216 - learning_rate: 1.0000e-04\nEpoch 9/300\n\u001b[1m92/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4710 - loss: 1.0690\nEpoch 9: val_accuracy improved from 0.59140 to 0.64875, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.4711 - loss: 1.0671 - val_accuracy: 0.6487 - val_loss: 0.8604 - learning_rate: 1.0000e-04\nEpoch 10/300\n\u001b[1m93/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4993 - loss: 1.0290\nEpoch 10: val_accuracy improved from 0.64875 to 0.66667, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.4990 - loss: 1.0290 - val_accuracy: 0.6667 - val_loss: 0.8021 - learning_rate: 1.0000e-04\nEpoch 11/300\n\u001b[1m97/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5471 - loss: 0.9605\nEpoch 11: val_accuracy did not improve from 0.66667\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5472 - loss: 0.9611 - val_accuracy: 0.6452 - val_loss: 0.7936 - learning_rate: 1.0000e-04\nEpoch 12/300\n\u001b[1m98/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5287 - loss: 0.9891\nEpoch 12: val_accuracy did not improve from 0.66667\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5288 - loss: 0.9888 - val_accuracy: 0.6667 - val_loss: 0.7548 - learning_rate: 1.0000e-04\nEpoch 13/300\n\u001b[1m97/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5623 - loss: 0.9012\nEpoch 13: val_accuracy improved from 0.66667 to 0.69892, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5628 - loss: 0.9004 - val_accuracy: 0.6989 - val_loss: 0.7071 - learning_rate: 1.0000e-04\nEpoch 14/300\n\u001b[1m97/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6320 - loss: 0.8399\nEpoch 14: val_accuracy improved from 0.69892 to 0.71326, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6317 - loss: 0.8404 - val_accuracy: 0.7133 - val_loss: 0.6854 - learning_rate: 1.0000e-04\nEpoch 15/300\n\u001b[1m93/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6479 - loss: 0.8390\nEpoch 15: val_accuracy improved from 0.71326 to 0.72043, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6476 - loss: 0.8381 - val_accuracy: 0.7204 - val_loss: 0.6619 - learning_rate: 1.0000e-04\nEpoch 16/300\n\u001b[1m96/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6420 - loss: 0.7815\nEpoch 16: val_accuracy improved from 0.72043 to 0.72760, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6426 - loss: 0.7806 - val_accuracy: 0.7276 - val_loss: 0.6458 - learning_rate: 1.0000e-04\nEpoch 17/300\n\u001b[1m96/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6737 - loss: 0.7722\nEpoch 17: val_accuracy did not improve from 0.72760\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6738 - loss: 0.7717 - val_accuracy: 0.7276 - val_loss: 0.6470 - learning_rate: 1.0000e-04\nEpoch 18/300\n\u001b[1m93/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6545 - loss: 0.7483\nEpoch 18: val_accuracy improved from 0.72760 to 0.73477, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6568 - loss: 0.7473 - val_accuracy: 0.7348 - val_loss: 0.6371 - learning_rate: 1.0000e-04\nEpoch 19/300\n\u001b[1m92/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7155 - loss: 0.6818\nEpoch 19: val_accuracy improved from 0.73477 to 0.74194, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7153 - loss: 0.6828 - val_accuracy: 0.7419 - val_loss: 0.6180 - learning_rate: 1.0000e-04\nEpoch 20/300\n\u001b[1m92/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7213 - loss: 0.7310\nEpoch 20: val_accuracy improved from 0.74194 to 0.75269, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7221 - loss: 0.7271 - val_accuracy: 0.7527 - val_loss: 0.6120 - learning_rate: 1.0000e-04\nEpoch 21/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7390 - loss: 0.6665\nEpoch 21: val_accuracy improved from 0.75269 to 0.76703, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7375 - loss: 0.6674 - val_accuracy: 0.7670 - val_loss: 0.5999 - learning_rate: 1.0000e-04\nEpoch 22/300\n\u001b[1m92/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7342 - loss: 0.6936\nEpoch 22: val_accuracy improved from 0.76703 to 0.77061, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7339 - loss: 0.6920 - val_accuracy: 0.7706 - val_loss: 0.5786 - learning_rate: 1.0000e-04\nEpoch 23/300\n\u001b[1m93/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7467 - loss: 0.6446\nEpoch 23: val_accuracy did not improve from 0.77061\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7461 - loss: 0.6469 - val_accuracy: 0.7634 - val_loss: 0.5867 - learning_rate: 1.0000e-04\nEpoch 24/300\n\u001b[1m92/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7287 - loss: 0.6891\nEpoch 24: val_accuracy did not improve from 0.77061\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7299 - loss: 0.6876 - val_accuracy: 0.7670 - val_loss: 0.5811 - learning_rate: 1.0000e-04\nEpoch 25/300\n\u001b[1m92/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7503 - loss: 0.6361\nEpoch 25: val_accuracy improved from 0.77061 to 0.77778, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7502 - loss: 0.6369 - val_accuracy: 0.7778 - val_loss: 0.5668 - learning_rate: 1.0000e-04\nEpoch 26/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7559 - loss: 0.6617\nEpoch 26: val_accuracy did not improve from 0.77778\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7563 - loss: 0.6584 - val_accuracy: 0.7778 - val_loss: 0.5678 - learning_rate: 1.0000e-04\nEpoch 27/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7556 - loss: 0.6295\nEpoch 27: val_accuracy did not improve from 0.77778\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7563 - loss: 0.6271 - val_accuracy: 0.7778 - val_loss: 0.5639 - learning_rate: 1.0000e-04\nEpoch 28/300\n\u001b[1m88/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7560 - loss: 0.6420\nEpoch 28: val_accuracy improved from 0.77778 to 0.78495, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7557 - loss: 0.6409 - val_accuracy: 0.7849 - val_loss: 0.5488 - learning_rate: 1.0000e-04\nEpoch 29/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7440 - loss: 0.6408\nEpoch 29: val_accuracy did not improve from 0.78495\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7457 - loss: 0.6383 - val_accuracy: 0.7849 - val_loss: 0.5442 - learning_rate: 1.0000e-04\nEpoch 30/300\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7688 - loss: 0.6240\nEpoch 30: val_accuracy improved from 0.78495 to 0.79570, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7688 - loss: 0.6240 - val_accuracy: 0.7957 - val_loss: 0.5361 - learning_rate: 1.0000e-04\nEpoch 31/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7660 - loss: 0.6197\nEpoch 31: val_accuracy did not improve from 0.79570\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7663 - loss: 0.6192 - val_accuracy: 0.7957 - val_loss: 0.5444 - learning_rate: 1.0000e-04\nEpoch 32/300\n\u001b[1m92/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7400 - loss: 0.6788\nEpoch 32: val_accuracy did not improve from 0.79570\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7417 - loss: 0.6754 - val_accuracy: 0.7921 - val_loss: 0.5405 - learning_rate: 1.0000e-04\nEpoch 33/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7506 - loss: 0.6191\nEpoch 33: val_accuracy improved from 0.79570 to 0.80645, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7528 - loss: 0.6152 - val_accuracy: 0.8065 - val_loss: 0.5396 - learning_rate: 1.0000e-04\nEpoch 34/300\n\u001b[1m92/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7799 - loss: 0.5354\nEpoch 34: val_accuracy did not improve from 0.80645\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7798 - loss: 0.5376 - val_accuracy: 0.8065 - val_loss: 0.5369 - learning_rate: 1.0000e-04\nEpoch 35/300\n\u001b[1m92/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7767 - loss: 0.5845\nEpoch 35: val_accuracy did not improve from 0.80645\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7770 - loss: 0.5838 - val_accuracy: 0.8029 - val_loss: 0.5341 - learning_rate: 1.0000e-04\nEpoch 36/300\n\u001b[1m92/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7771 - loss: 0.5680\nEpoch 36: val_accuracy did not improve from 0.80645\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7773 - loss: 0.5681 - val_accuracy: 0.8065 - val_loss: 0.5338 - learning_rate: 1.0000e-04\nEpoch 37/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7706 - loss: 0.6080\nEpoch 37: val_accuracy improved from 0.80645 to 0.81004, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7719 - loss: 0.6048 - val_accuracy: 0.8100 - val_loss: 0.5311 - learning_rate: 1.0000e-04\nEpoch 38/300\n\u001b[1m92/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7682 - loss: 0.5858\nEpoch 38: val_accuracy did not improve from 0.81004\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7694 - loss: 0.5839 - val_accuracy: 0.8100 - val_loss: 0.5198 - learning_rate: 1.0000e-04\nEpoch 39/300\n\u001b[1m97/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7679 - loss: 0.5598\nEpoch 39: val_accuracy did not improve from 0.81004\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7682 - loss: 0.5598 - val_accuracy: 0.8029 - val_loss: 0.5213 - learning_rate: 1.0000e-04\nEpoch 40/300\n\u001b[1m95/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7947 - loss: 0.5303\nEpoch 40: val_accuracy did not improve from 0.81004\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7941 - loss: 0.5315 - val_accuracy: 0.8100 - val_loss: 0.5177 - learning_rate: 1.0000e-04\nEpoch 41/300\n\u001b[1m92/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7841 - loss: 0.5251\nEpoch 41: val_accuracy did not improve from 0.81004\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7836 - loss: 0.5271 - val_accuracy: 0.8100 - val_loss: 0.5121 - learning_rate: 1.0000e-04\nEpoch 42/300\n\u001b[1m92/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7818 - loss: 0.5156\nEpoch 42: val_accuracy improved from 0.81004 to 0.82079, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7818 - loss: 0.5169 - val_accuracy: 0.8208 - val_loss: 0.5069 - learning_rate: 1.0000e-04\nEpoch 43/300\n\u001b[1m92/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7851 - loss: 0.5667\nEpoch 43: val_accuracy did not improve from 0.82079\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7855 - loss: 0.5646 - val_accuracy: 0.8208 - val_loss: 0.5061 - learning_rate: 1.0000e-04\nEpoch 44/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7953 - loss: 0.5574\nEpoch 44: val_accuracy improved from 0.82079 to 0.82437, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7945 - loss: 0.5558 - val_accuracy: 0.8244 - val_loss: 0.5053 - learning_rate: 1.0000e-04\nEpoch 45/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8066 - loss: 0.5155\nEpoch 45: val_accuracy improved from 0.82437 to 0.83871, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8063 - loss: 0.5156 - val_accuracy: 0.8387 - val_loss: 0.4999 - learning_rate: 1.0000e-04\nEpoch 46/300\n\u001b[1m92/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7967 - loss: 0.5362\nEpoch 46: val_accuracy did not improve from 0.83871\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7972 - loss: 0.5354 - val_accuracy: 0.8280 - val_loss: 0.4992 - learning_rate: 1.0000e-04\nEpoch 47/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8018 - loss: 0.5101\nEpoch 47: val_accuracy did not improve from 0.83871\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8007 - loss: 0.5117 - val_accuracy: 0.8172 - val_loss: 0.5014 - learning_rate: 1.0000e-04\nEpoch 48/300\n\u001b[1m89/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8008 - loss: 0.5215\nEpoch 48: val_accuracy did not improve from 0.83871\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8014 - loss: 0.5206 - val_accuracy: 0.8244 - val_loss: 0.5024 - learning_rate: 1.0000e-04\nEpoch 49/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8002 - loss: 0.4900\nEpoch 49: val_accuracy did not improve from 0.83871\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7997 - loss: 0.4931 - val_accuracy: 0.8100 - val_loss: 0.4996 - learning_rate: 1.0000e-04\nEpoch 50/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8115 - loss: 0.5001\nEpoch 50: val_accuracy did not improve from 0.83871\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8102 - loss: 0.5025 - val_accuracy: 0.8280 - val_loss: 0.4892 - learning_rate: 1.0000e-04\nEpoch 51/300\n\u001b[1m97/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8106 - loss: 0.4775\nEpoch 51: val_accuracy did not improve from 0.83871\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8104 - loss: 0.4778 - val_accuracy: 0.8208 - val_loss: 0.4921 - learning_rate: 1.0000e-04\nEpoch 52/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8049 - loss: 0.5152\nEpoch 52: val_accuracy did not improve from 0.83871\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8046 - loss: 0.5158 - val_accuracy: 0.8172 - val_loss: 0.4929 - learning_rate: 1.0000e-04\nEpoch 53/300\n\u001b[1m92/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7811 - loss: 0.5378\nEpoch 53: val_accuracy did not improve from 0.83871\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7834 - loss: 0.5357 - val_accuracy: 0.8244 - val_loss: 0.4839 - learning_rate: 1.0000e-04\nEpoch 54/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8252 - loss: 0.4504\nEpoch 54: val_accuracy did not improve from 0.83871\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8240 - loss: 0.4525 - val_accuracy: 0.8208 - val_loss: 0.4852 - learning_rate: 1.0000e-04\nEpoch 55/300\n\u001b[1m89/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8007 - loss: 0.4720\nEpoch 55: val_accuracy did not improve from 0.83871\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8007 - loss: 0.4753 - val_accuracy: 0.8280 - val_loss: 0.4857 - learning_rate: 1.0000e-04\nEpoch 56/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8018 - loss: 0.4894\nEpoch 56: val_accuracy did not improve from 0.83871\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8009 - loss: 0.4918 - val_accuracy: 0.8280 - val_loss: 0.4770 - learning_rate: 1.0000e-04\nEpoch 57/300\n\u001b[1m88/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8271 - loss: 0.4518\nEpoch 57: val_accuracy did not improve from 0.83871\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8252 - loss: 0.4549 - val_accuracy: 0.8208 - val_loss: 0.4706 - learning_rate: 1.0000e-04\nEpoch 58/300\n\u001b[1m92/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7925 - loss: 0.5260\nEpoch 58: val_accuracy did not improve from 0.83871\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7933 - loss: 0.5236 - val_accuracy: 0.8315 - val_loss: 0.4707 - learning_rate: 1.0000e-04\nEpoch 59/300\n\u001b[1m88/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8229 - loss: 0.4647\nEpoch 59: val_accuracy did not improve from 0.83871\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8218 - loss: 0.4654 - val_accuracy: 0.8280 - val_loss: 0.4654 - learning_rate: 1.0000e-04\nEpoch 60/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7920 - loss: 0.4987\nEpoch 60: val_accuracy did not improve from 0.83871\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7937 - loss: 0.4958 - val_accuracy: 0.8244 - val_loss: 0.4560 - learning_rate: 1.0000e-04\nEpoch 61/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8111 - loss: 0.4800\nEpoch 61: val_accuracy did not improve from 0.83871\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8110 - loss: 0.4796 - val_accuracy: 0.8208 - val_loss: 0.4563 - learning_rate: 1.0000e-04\nEpoch 62/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8158 - loss: 0.4615\nEpoch 62: val_accuracy did not improve from 0.83871\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8162 - loss: 0.4618 - val_accuracy: 0.8172 - val_loss: 0.4508 - learning_rate: 1.0000e-04\nEpoch 63/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8059 - loss: 0.4710\nEpoch 63: val_accuracy did not improve from 0.83871\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8067 - loss: 0.4706 - val_accuracy: 0.8280 - val_loss: 0.4537 - learning_rate: 1.0000e-04\nEpoch 64/300\n\u001b[1m98/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7889 - loss: 0.5252\nEpoch 64: val_accuracy did not improve from 0.83871\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7893 - loss: 0.5242 - val_accuracy: 0.8351 - val_loss: 0.4479 - learning_rate: 1.0000e-04\nEpoch 65/300\n\u001b[1m89/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8163 - loss: 0.4421\nEpoch 65: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n\nEpoch 65: val_accuracy did not improve from 0.83871\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8187 - loss: 0.4396 - val_accuracy: 0.8351 - val_loss: 0.4442 - learning_rate: 1.0000e-04\nEpoch 66/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8191 - loss: 0.4473\nEpoch 66: val_accuracy did not improve from 0.83871\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8173 - loss: 0.4501 - val_accuracy: 0.8387 - val_loss: 0.4470 - learning_rate: 5.0000e-05\nEpoch 67/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8192 - loss: 0.4534\nEpoch 67: val_accuracy did not improve from 0.83871\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8181 - loss: 0.4556 - val_accuracy: 0.8351 - val_loss: 0.4441 - learning_rate: 5.0000e-05\nEpoch 68/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8546 - loss: 0.4125\nEpoch 68: val_accuracy did not improve from 0.83871\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8529 - loss: 0.4143 - val_accuracy: 0.8387 - val_loss: 0.4333 - learning_rate: 5.0000e-05\nEpoch 69/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8384 - loss: 0.4194\nEpoch 69: val_accuracy improved from 0.83871 to 0.84229, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8380 - loss: 0.4200 - val_accuracy: 0.8423 - val_loss: 0.4343 - learning_rate: 5.0000e-05\nEpoch 70/300\n\u001b[1m96/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8259 - loss: 0.4502\nEpoch 70: val_accuracy did not improve from 0.84229\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8258 - loss: 0.4501 - val_accuracy: 0.8387 - val_loss: 0.4328 - learning_rate: 5.0000e-05\nEpoch 71/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8366 - loss: 0.4345\nEpoch 71: val_accuracy did not improve from 0.84229\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8367 - loss: 0.4343 - val_accuracy: 0.8387 - val_loss: 0.4284 - learning_rate: 5.0000e-05\nEpoch 72/300\n\u001b[1m92/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8150 - loss: 0.4561\nEpoch 72: val_accuracy did not improve from 0.84229\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8148 - loss: 0.4576 - val_accuracy: 0.8387 - val_loss: 0.4322 - learning_rate: 5.0000e-05\nEpoch 73/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8240 - loss: 0.4246\nEpoch 73: val_accuracy did not improve from 0.84229\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8243 - loss: 0.4247 - val_accuracy: 0.8351 - val_loss: 0.4337 - learning_rate: 5.0000e-05\nEpoch 74/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8150 - loss: 0.4371\nEpoch 74: val_accuracy did not improve from 0.84229\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8159 - loss: 0.4373 - val_accuracy: 0.8387 - val_loss: 0.4296 - learning_rate: 5.0000e-05\nEpoch 75/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8451 - loss: 0.4179\nEpoch 75: val_accuracy did not improve from 0.84229\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8445 - loss: 0.4196 - val_accuracy: 0.8387 - val_loss: 0.4321 - learning_rate: 5.0000e-05\nEpoch 76/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8256 - loss: 0.4072\nEpoch 76: val_accuracy did not improve from 0.84229\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8263 - loss: 0.4070 - val_accuracy: 0.8423 - val_loss: 0.4310 - learning_rate: 5.0000e-05\nEpoch 77/300\n\u001b[1m92/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8164 - loss: 0.4299\nEpoch 77: val_accuracy did not improve from 0.84229\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8175 - loss: 0.4293 - val_accuracy: 0.8387 - val_loss: 0.4264 - learning_rate: 5.0000e-05\nEpoch 78/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8367 - loss: 0.4277\nEpoch 78: val_accuracy did not improve from 0.84229\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8359 - loss: 0.4270 - val_accuracy: 0.8351 - val_loss: 0.4282 - learning_rate: 5.0000e-05\nEpoch 79/300\n\u001b[1m89/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8269 - loss: 0.4101\nEpoch 79: val_accuracy did not improve from 0.84229\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8280 - loss: 0.4095 - val_accuracy: 0.8280 - val_loss: 0.4230 - learning_rate: 5.0000e-05\nEpoch 80/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8155 - loss: 0.4640\nEpoch 80: val_accuracy did not improve from 0.84229\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8164 - loss: 0.4612 - val_accuracy: 0.8315 - val_loss: 0.4233 - learning_rate: 5.0000e-05\nEpoch 81/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8327 - loss: 0.4257\nEpoch 81: val_accuracy did not improve from 0.84229\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8325 - loss: 0.4247 - val_accuracy: 0.8315 - val_loss: 0.4250 - learning_rate: 5.0000e-05\nEpoch 82/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8181 - loss: 0.4227\nEpoch 82: val_accuracy did not improve from 0.84229\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8200 - loss: 0.4198 - val_accuracy: 0.8244 - val_loss: 0.4295 - learning_rate: 5.0000e-05\nEpoch 83/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8575 - loss: 0.3737\nEpoch 83: val_accuracy did not improve from 0.84229\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8548 - loss: 0.3806 - val_accuracy: 0.8315 - val_loss: 0.4283 - learning_rate: 5.0000e-05\nEpoch 84/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8385 - loss: 0.3829\nEpoch 84: val_accuracy did not improve from 0.84229\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8380 - loss: 0.3843 - val_accuracy: 0.8315 - val_loss: 0.4246 - learning_rate: 5.0000e-05\nEpoch 85/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8413 - loss: 0.4465\nEpoch 85: val_accuracy did not improve from 0.84229\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8410 - loss: 0.4441 - val_accuracy: 0.8280 - val_loss: 0.4235 - learning_rate: 5.0000e-05\nEpoch 86/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8431 - loss: 0.3877\nEpoch 86: val_accuracy improved from 0.84229 to 0.84588, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8429 - loss: 0.3891 - val_accuracy: 0.8459 - val_loss: 0.4186 - learning_rate: 5.0000e-05\nEpoch 87/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8312 - loss: 0.4267\nEpoch 87: val_accuracy did not improve from 0.84588\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8312 - loss: 0.4251 - val_accuracy: 0.8280 - val_loss: 0.4120 - learning_rate: 5.0000e-05\nEpoch 88/300\n\u001b[1m97/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8376 - loss: 0.4244\nEpoch 88: val_accuracy did not improve from 0.84588\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8377 - loss: 0.4236 - val_accuracy: 0.8280 - val_loss: 0.4211 - learning_rate: 5.0000e-05\nEpoch 89/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8533 - loss: 0.3814\nEpoch 89: val_accuracy did not improve from 0.84588\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8527 - loss: 0.3822 - val_accuracy: 0.8351 - val_loss: 0.4169 - learning_rate: 5.0000e-05\nEpoch 90/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8555 - loss: 0.3787\nEpoch 90: val_accuracy did not improve from 0.84588\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8550 - loss: 0.3796 - val_accuracy: 0.8387 - val_loss: 0.4111 - learning_rate: 5.0000e-05\nEpoch 91/300\n\u001b[1m89/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8431 - loss: 0.3943\nEpoch 91: val_accuracy did not improve from 0.84588\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8427 - loss: 0.3961 - val_accuracy: 0.8351 - val_loss: 0.4082 - learning_rate: 5.0000e-05\nEpoch 92/300\n\u001b[1m92/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8492 - loss: 0.3779\nEpoch 92: val_accuracy did not improve from 0.84588\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8485 - loss: 0.3801 - val_accuracy: 0.8423 - val_loss: 0.4160 - learning_rate: 5.0000e-05\nEpoch 93/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8545 - loss: 0.3703\nEpoch 93: val_accuracy did not improve from 0.84588\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8535 - loss: 0.3720 - val_accuracy: 0.8387 - val_loss: 0.4135 - learning_rate: 5.0000e-05\nEpoch 94/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8422 - loss: 0.3929\nEpoch 94: val_accuracy did not improve from 0.84588\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8418 - loss: 0.3936 - val_accuracy: 0.8423 - val_loss: 0.4068 - learning_rate: 5.0000e-05\nEpoch 95/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8550 - loss: 0.3790\nEpoch 95: val_accuracy did not improve from 0.84588\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8542 - loss: 0.3804 - val_accuracy: 0.8387 - val_loss: 0.4010 - learning_rate: 5.0000e-05\nEpoch 96/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8449 - loss: 0.3961\nEpoch 96: val_accuracy did not improve from 0.84588\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8442 - loss: 0.3956 - val_accuracy: 0.8459 - val_loss: 0.3986 - learning_rate: 5.0000e-05\nEpoch 97/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8179 - loss: 0.4064\nEpoch 97: val_accuracy did not improve from 0.84588\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8191 - loss: 0.4057 - val_accuracy: 0.8459 - val_loss: 0.4003 - learning_rate: 5.0000e-05\nEpoch 98/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8407 - loss: 0.3794\nEpoch 98: val_accuracy improved from 0.84588 to 0.84946, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8398 - loss: 0.3805 - val_accuracy: 0.8495 - val_loss: 0.3955 - learning_rate: 5.0000e-05\nEpoch 99/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8656 - loss: 0.3706\nEpoch 99: val_accuracy did not improve from 0.84946\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8637 - loss: 0.3720 - val_accuracy: 0.8423 - val_loss: 0.3957 - learning_rate: 5.0000e-05\nEpoch 100/300\n\u001b[1m92/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8589 - loss: 0.3684\nEpoch 100: val_accuracy did not improve from 0.84946\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8579 - loss: 0.3703 - val_accuracy: 0.8351 - val_loss: 0.3946 - learning_rate: 5.0000e-05\nEpoch 101/300\n\u001b[1m89/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8433 - loss: 0.4002\nEpoch 101: val_accuracy did not improve from 0.84946\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8441 - loss: 0.3970 - val_accuracy: 0.8423 - val_loss: 0.3954 - learning_rate: 5.0000e-05\nEpoch 102/300\n\u001b[1m88/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8564 - loss: 0.3385\nEpoch 102: val_accuracy did not improve from 0.84946\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8552 - loss: 0.3418 - val_accuracy: 0.8423 - val_loss: 0.3925 - learning_rate: 5.0000e-05\nEpoch 103/300\n\u001b[1m87/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8564 - loss: 0.3592\nEpoch 103: val_accuracy did not improve from 0.84946\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8556 - loss: 0.3623 - val_accuracy: 0.8495 - val_loss: 0.3905 - learning_rate: 5.0000e-05\nEpoch 104/300\n\u001b[1m89/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8423 - loss: 0.4011\nEpoch 104: val_accuracy improved from 0.84946 to 0.85305, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8432 - loss: 0.3997 - val_accuracy: 0.8530 - val_loss: 0.3861 - learning_rate: 5.0000e-05\nEpoch 105/300\n\u001b[1m89/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8513 - loss: 0.3680\nEpoch 105: val_accuracy improved from 0.85305 to 0.86022, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8518 - loss: 0.3667 - val_accuracy: 0.8602 - val_loss: 0.3873 - learning_rate: 5.0000e-05\nEpoch 106/300\n\u001b[1m94/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8609 - loss: 0.3746\nEpoch 106: val_accuracy did not improve from 0.86022\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8608 - loss: 0.3736 - val_accuracy: 0.8602 - val_loss: 0.3819 - learning_rate: 5.0000e-05\nEpoch 107/300\n\u001b[1m88/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8518 - loss: 0.3599\nEpoch 107: val_accuracy did not improve from 0.86022\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8522 - loss: 0.3602 - val_accuracy: 0.8530 - val_loss: 0.3803 - learning_rate: 5.0000e-05\nEpoch 108/300\n\u001b[1m89/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8688 - loss: 0.3273\nEpoch 108: val_accuracy did not improve from 0.86022\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8677 - loss: 0.3301 - val_accuracy: 0.8602 - val_loss: 0.3749 - learning_rate: 5.0000e-05\nEpoch 109/300\n\u001b[1m88/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8495 - loss: 0.3587\nEpoch 109: val_accuracy did not improve from 0.86022\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8492 - loss: 0.3606 - val_accuracy: 0.8530 - val_loss: 0.3731 - learning_rate: 5.0000e-05\nEpoch 110/300\n\u001b[1m88/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8369 - loss: 0.3808\nEpoch 110: val_accuracy did not improve from 0.86022\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8396 - loss: 0.3782 - val_accuracy: 0.8566 - val_loss: 0.3704 - learning_rate: 5.0000e-05\nEpoch 111/300\n\u001b[1m89/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8587 - loss: 0.3448\nEpoch 111: val_accuracy did not improve from 0.86022\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8581 - loss: 0.3443 - val_accuracy: 0.8566 - val_loss: 0.3735 - learning_rate: 5.0000e-05\nEpoch 112/300\n\u001b[1m87/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8651 - loss: 0.3330\nEpoch 112: val_accuracy improved from 0.86022 to 0.87097, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8657 - loss: 0.3333 - val_accuracy: 0.8710 - val_loss: 0.3665 - learning_rate: 5.0000e-05\nEpoch 113/300\n\u001b[1m89/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8688 - loss: 0.3260\nEpoch 113: val_accuracy improved from 0.87097 to 0.87455, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8669 - loss: 0.3292 - val_accuracy: 0.8746 - val_loss: 0.3661 - learning_rate: 5.0000e-05\nEpoch 114/300\n\u001b[1m88/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8618 - loss: 0.3380\nEpoch 114: val_accuracy did not improve from 0.87455\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8618 - loss: 0.3377 - val_accuracy: 0.8710 - val_loss: 0.3665 - learning_rate: 5.0000e-05\nEpoch 115/300\n\u001b[1m97/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8686 - loss: 0.3334\nEpoch 115: val_accuracy did not improve from 0.87455\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8684 - loss: 0.3339 - val_accuracy: 0.8746 - val_loss: 0.3643 - learning_rate: 5.0000e-05\nEpoch 116/300\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8667 - loss: 0.3226\nEpoch 116: val_accuracy did not improve from 0.87455\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8666 - loss: 0.3230 - val_accuracy: 0.8710 - val_loss: 0.3653 - learning_rate: 5.0000e-05\nEpoch 117/300\n\u001b[1m87/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8402 - loss: 0.3833\nEpoch 117: val_accuracy did not improve from 0.87455\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8421 - loss: 0.3800 - val_accuracy: 0.8638 - val_loss: 0.3669 - learning_rate: 5.0000e-05\nEpoch 118/300\n\u001b[1m88/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8586 - loss: 0.3367\nEpoch 118: val_accuracy did not improve from 0.87455\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8588 - loss: 0.3374 - val_accuracy: 0.8710 - val_loss: 0.3613 - learning_rate: 5.0000e-05\nEpoch 119/300\n\u001b[1m97/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8295 - loss: 0.4156\nEpoch 119: val_accuracy did not improve from 0.87455\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8299 - loss: 0.4144 - val_accuracy: 0.8746 - val_loss: 0.3645 - learning_rate: 5.0000e-05\nEpoch 120/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8692 - loss: 0.3140\nEpoch 120: val_accuracy improved from 0.87455 to 0.87814, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8689 - loss: 0.3161 - val_accuracy: 0.8781 - val_loss: 0.3632 - learning_rate: 5.0000e-05\nEpoch 121/300\n\u001b[1m89/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8712 - loss: 0.3495\nEpoch 121: val_accuracy improved from 0.87814 to 0.88172, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8705 - loss: 0.3473 - val_accuracy: 0.8817 - val_loss: 0.3618 - learning_rate: 5.0000e-05\nEpoch 122/300\n\u001b[1m88/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8455 - loss: 0.4064\nEpoch 122: val_accuracy did not improve from 0.88172\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8478 - loss: 0.3995 - val_accuracy: 0.8817 - val_loss: 0.3674 - learning_rate: 5.0000e-05\nEpoch 123/300\n\u001b[1m93/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8691 - loss: 0.3305\nEpoch 123: val_accuracy did not improve from 0.88172\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8691 - loss: 0.3308 - val_accuracy: 0.8817 - val_loss: 0.3642 - learning_rate: 5.0000e-05\nEpoch 124/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8802 - loss: 0.3046\nEpoch 124: val_accuracy did not improve from 0.88172\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8780 - loss: 0.3078 - val_accuracy: 0.8817 - val_loss: 0.3599 - learning_rate: 5.0000e-05\nEpoch 125/300\n\u001b[1m92/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8728 - loss: 0.3415\nEpoch 125: val_accuracy did not improve from 0.88172\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8729 - loss: 0.3406 - val_accuracy: 0.8781 - val_loss: 0.3703 - learning_rate: 5.0000e-05\nEpoch 126/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8920 - loss: 0.2851\nEpoch 126: val_accuracy did not improve from 0.88172\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8898 - loss: 0.2881 - val_accuracy: 0.8781 - val_loss: 0.3668 - learning_rate: 5.0000e-05\nEpoch 127/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8668 - loss: 0.3191\nEpoch 127: val_accuracy did not improve from 0.88172\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8669 - loss: 0.3189 - val_accuracy: 0.8817 - val_loss: 0.3539 - learning_rate: 5.0000e-05\nEpoch 128/300\n\u001b[1m87/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8677 - loss: 0.3381\nEpoch 128: val_accuracy did not improve from 0.88172\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8671 - loss: 0.3397 - val_accuracy: 0.8781 - val_loss: 0.3533 - learning_rate: 5.0000e-05\nEpoch 129/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8616 - loss: 0.3371\nEpoch 129: val_accuracy did not improve from 0.88172\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8613 - loss: 0.3369 - val_accuracy: 0.8746 - val_loss: 0.3533 - learning_rate: 5.0000e-05\nEpoch 130/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8655 - loss: 0.3278\nEpoch 130: val_accuracy did not improve from 0.88172\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8654 - loss: 0.3279 - val_accuracy: 0.8710 - val_loss: 0.3494 - learning_rate: 5.0000e-05\nEpoch 131/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8849 - loss: 0.3156\nEpoch 131: val_accuracy did not improve from 0.88172\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8844 - loss: 0.3160 - val_accuracy: 0.8781 - val_loss: 0.3478 - learning_rate: 5.0000e-05\nEpoch 132/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8614 - loss: 0.3165\nEpoch 132: val_accuracy did not improve from 0.88172\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8617 - loss: 0.3159 - val_accuracy: 0.8746 - val_loss: 0.3541 - learning_rate: 5.0000e-05\nEpoch 133/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8702 - loss: 0.3315\nEpoch 133: val_accuracy did not improve from 0.88172\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8699 - loss: 0.3324 - val_accuracy: 0.8746 - val_loss: 0.3516 - learning_rate: 5.0000e-05\nEpoch 134/300\n\u001b[1m98/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8532 - loss: 0.3546\nEpoch 134: val_accuracy did not improve from 0.88172\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8534 - loss: 0.3541 - val_accuracy: 0.8781 - val_loss: 0.3469 - learning_rate: 5.0000e-05\nEpoch 135/300\n\u001b[1m88/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8666 - loss: 0.3020\nEpoch 135: val_accuracy improved from 0.88172 to 0.88889, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8663 - loss: 0.3035 - val_accuracy: 0.8889 - val_loss: 0.3470 - learning_rate: 5.0000e-05\nEpoch 136/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8761 - loss: 0.3054\nEpoch 136: val_accuracy did not improve from 0.88889\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8766 - loss: 0.3053 - val_accuracy: 0.8817 - val_loss: 0.3497 - learning_rate: 5.0000e-05\nEpoch 137/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8797 - loss: 0.3054\nEpoch 137: val_accuracy did not improve from 0.88889\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8795 - loss: 0.3060 - val_accuracy: 0.8817 - val_loss: 0.3398 - learning_rate: 5.0000e-05\nEpoch 138/300\n\u001b[1m88/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8745 - loss: 0.3039\nEpoch 138: val_accuracy did not improve from 0.88889\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8742 - loss: 0.3045 - val_accuracy: 0.8853 - val_loss: 0.3360 - learning_rate: 5.0000e-05\nEpoch 139/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8603 - loss: 0.3303\nEpoch 139: val_accuracy did not improve from 0.88889\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8607 - loss: 0.3303 - val_accuracy: 0.8853 - val_loss: 0.3441 - learning_rate: 5.0000e-05\nEpoch 140/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8794 - loss: 0.3158\nEpoch 140: val_accuracy did not improve from 0.88889\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8794 - loss: 0.3159 - val_accuracy: 0.8817 - val_loss: 0.3444 - learning_rate: 5.0000e-05\nEpoch 141/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8793 - loss: 0.2861\nEpoch 141: val_accuracy did not improve from 0.88889\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8788 - loss: 0.2874 - val_accuracy: 0.8781 - val_loss: 0.3408 - learning_rate: 5.0000e-05\nEpoch 142/300\n\u001b[1m98/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8660 - loss: 0.3066\nEpoch 142: val_accuracy did not improve from 0.88889\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8660 - loss: 0.3071 - val_accuracy: 0.8781 - val_loss: 0.3387 - learning_rate: 5.0000e-05\nEpoch 143/300\n\u001b[1m92/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8848 - loss: 0.3004\nEpoch 143: val_accuracy did not improve from 0.88889\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8841 - loss: 0.3012 - val_accuracy: 0.8781 - val_loss: 0.3430 - learning_rate: 5.0000e-05\nEpoch 144/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8840 - loss: 0.3140\nEpoch 144: val_accuracy did not improve from 0.88889\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8834 - loss: 0.3141 - val_accuracy: 0.8746 - val_loss: 0.3434 - learning_rate: 5.0000e-05\nEpoch 145/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9043 - loss: 0.2806\nEpoch 145: val_accuracy did not improve from 0.88889\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9033 - loss: 0.2809 - val_accuracy: 0.8817 - val_loss: 0.3437 - learning_rate: 5.0000e-05\nEpoch 146/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8844 - loss: 0.2995\nEpoch 146: val_accuracy did not improve from 0.88889\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8846 - loss: 0.2980 - val_accuracy: 0.8817 - val_loss: 0.3438 - learning_rate: 5.0000e-05\nEpoch 147/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8647 - loss: 0.3222\nEpoch 147: val_accuracy did not improve from 0.88889\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8656 - loss: 0.3205 - val_accuracy: 0.8781 - val_loss: 0.3485 - learning_rate: 5.0000e-05\nEpoch 148/300\n\u001b[1m89/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8685 - loss: 0.3240\nEpoch 148: val_accuracy did not improve from 0.88889\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8699 - loss: 0.3208 - val_accuracy: 0.8817 - val_loss: 0.3445 - learning_rate: 5.0000e-05\nEpoch 149/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8656 - loss: 0.3283\nEpoch 149: val_accuracy did not improve from 0.88889\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8663 - loss: 0.3268 - val_accuracy: 0.8817 - val_loss: 0.3344 - learning_rate: 5.0000e-05\nEpoch 150/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8916 - loss: 0.2691\nEpoch 150: val_accuracy did not improve from 0.88889\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8913 - loss: 0.2699 - val_accuracy: 0.8781 - val_loss: 0.3341 - learning_rate: 5.0000e-05\nEpoch 151/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8577 - loss: 0.3391\nEpoch 151: val_accuracy did not improve from 0.88889\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8589 - loss: 0.3366 - val_accuracy: 0.8817 - val_loss: 0.3366 - learning_rate: 5.0000e-05\nEpoch 152/300\n\u001b[1m88/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8749 - loss: 0.2717\nEpoch 152: val_accuracy did not improve from 0.88889\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8748 - loss: 0.2737 - val_accuracy: 0.8781 - val_loss: 0.3417 - learning_rate: 5.0000e-05\nEpoch 153/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8957 - loss: 0.2818\nEpoch 153: val_accuracy did not improve from 0.88889\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8952 - loss: 0.2812 - val_accuracy: 0.8746 - val_loss: 0.3399 - learning_rate: 5.0000e-05\nEpoch 154/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9013 - loss: 0.2553\nEpoch 154: val_accuracy did not improve from 0.88889\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9004 - loss: 0.2582 - val_accuracy: 0.8746 - val_loss: 0.3348 - learning_rate: 5.0000e-05\nEpoch 155/300\n\u001b[1m92/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8748 - loss: 0.3150\nEpoch 155: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n\nEpoch 155: val_accuracy did not improve from 0.88889\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8758 - loss: 0.3136 - val_accuracy: 0.8817 - val_loss: 0.3315 - learning_rate: 5.0000e-05\nEpoch 156/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8990 - loss: 0.2596\nEpoch 156: val_accuracy did not improve from 0.88889\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8982 - loss: 0.2616 - val_accuracy: 0.8817 - val_loss: 0.3271 - learning_rate: 2.5000e-05\nEpoch 157/300\n\u001b[1m92/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8716 - loss: 0.2949\nEpoch 157: val_accuracy did not improve from 0.88889\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8718 - loss: 0.2962 - val_accuracy: 0.8817 - val_loss: 0.3278 - learning_rate: 2.5000e-05\nEpoch 158/300\n\u001b[1m92/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8797 - loss: 0.2815\nEpoch 158: val_accuracy did not improve from 0.88889\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8798 - loss: 0.2825 - val_accuracy: 0.8781 - val_loss: 0.3290 - learning_rate: 2.5000e-05\nEpoch 159/300\n\u001b[1m92/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8802 - loss: 0.3076\nEpoch 159: val_accuracy did not improve from 0.88889\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8807 - loss: 0.3059 - val_accuracy: 0.8781 - val_loss: 0.3294 - learning_rate: 2.5000e-05\nEpoch 160/300\n\u001b[1m92/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8801 - loss: 0.2975\nEpoch 160: val_accuracy did not improve from 0.88889\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8812 - loss: 0.2953 - val_accuracy: 0.8781 - val_loss: 0.3325 - learning_rate: 2.5000e-05\nEpoch 161/300\n\u001b[1m97/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8741 - loss: 0.3116\nEpoch 161: val_accuracy did not improve from 0.88889\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8741 - loss: 0.3118 - val_accuracy: 0.8781 - val_loss: 0.3306 - learning_rate: 2.5000e-05\nEpoch 162/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8885 - loss: 0.2806\nEpoch 162: val_accuracy did not improve from 0.88889\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8887 - loss: 0.2792 - val_accuracy: 0.8781 - val_loss: 0.3301 - learning_rate: 2.5000e-05\nEpoch 163/300\n\u001b[1m92/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8989 - loss: 0.2574\nEpoch 163: val_accuracy did not improve from 0.88889\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8991 - loss: 0.2574 - val_accuracy: 0.8781 - val_loss: 0.3322 - learning_rate: 2.5000e-05\nEpoch 164/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8667 - loss: 0.3330\nEpoch 164: val_accuracy did not improve from 0.88889\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8676 - loss: 0.3298 - val_accuracy: 0.8817 - val_loss: 0.3264 - learning_rate: 2.5000e-05\nEpoch 165/300\n\u001b[1m92/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8972 - loss: 0.2483\nEpoch 165: val_accuracy did not improve from 0.88889\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8970 - loss: 0.2495 - val_accuracy: 0.8889 - val_loss: 0.3255 - learning_rate: 2.5000e-05\nEpoch 166/300\n\u001b[1m92/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8981 - loss: 0.2620\nEpoch 166: val_accuracy did not improve from 0.88889\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8979 - loss: 0.2632 - val_accuracy: 0.8853 - val_loss: 0.3251 - learning_rate: 2.5000e-05\nEpoch 167/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8786 - loss: 0.2717\nEpoch 167: val_accuracy did not improve from 0.88889\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8798 - loss: 0.2715 - val_accuracy: 0.8817 - val_loss: 0.3196 - learning_rate: 2.5000e-05\nEpoch 168/300\n\u001b[1m92/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9009 - loss: 0.2541\nEpoch 168: val_accuracy did not improve from 0.88889\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9000 - loss: 0.2553 - val_accuracy: 0.8889 - val_loss: 0.3190 - learning_rate: 2.5000e-05\nEpoch 169/300\n\u001b[1m92/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9046 - loss: 0.2779\nEpoch 169: val_accuracy did not improve from 0.88889\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9035 - loss: 0.2793 - val_accuracy: 0.8817 - val_loss: 0.3194 - learning_rate: 2.5000e-05\nEpoch 170/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8797 - loss: 0.3075\nEpoch 170: val_accuracy did not improve from 0.88889\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8804 - loss: 0.3061 - val_accuracy: 0.8889 - val_loss: 0.3170 - learning_rate: 2.5000e-05\nEpoch 171/300\n\u001b[1m92/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8989 - loss: 0.2409\nEpoch 171: val_accuracy improved from 0.88889 to 0.89247, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8978 - loss: 0.2434 - val_accuracy: 0.8925 - val_loss: 0.3156 - learning_rate: 2.5000e-05\nEpoch 172/300\n\u001b[1m93/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8922 - loss: 0.2590\nEpoch 172: val_accuracy did not improve from 0.89247\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8914 - loss: 0.2606 - val_accuracy: 0.8889 - val_loss: 0.3162 - learning_rate: 2.5000e-05\nEpoch 173/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8865 - loss: 0.2703\nEpoch 173: val_accuracy did not improve from 0.89247\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8865 - loss: 0.2703 - val_accuracy: 0.8889 - val_loss: 0.3133 - learning_rate: 2.5000e-05\nEpoch 174/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9013 - loss: 0.2528\nEpoch 174: val_accuracy did not improve from 0.89247\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9017 - loss: 0.2528 - val_accuracy: 0.8817 - val_loss: 0.3105 - learning_rate: 2.5000e-05\nEpoch 175/300\n\u001b[1m88/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8906 - loss: 0.2778\nEpoch 175: val_accuracy improved from 0.89247 to 0.89606, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8901 - loss: 0.2781 - val_accuracy: 0.8961 - val_loss: 0.3083 - learning_rate: 2.5000e-05\nEpoch 176/300\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8798 - loss: 0.2925\nEpoch 176: val_accuracy did not improve from 0.89606\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8799 - loss: 0.2924 - val_accuracy: 0.8961 - val_loss: 0.3101 - learning_rate: 2.5000e-05\nEpoch 177/300\n\u001b[1m89/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9235 - loss: 0.2282\nEpoch 177: val_accuracy did not improve from 0.89606\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9208 - loss: 0.2320 - val_accuracy: 0.8961 - val_loss: 0.3094 - learning_rate: 2.5000e-05\nEpoch 178/300\n\u001b[1m89/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8769 - loss: 0.3155\nEpoch 178: val_accuracy did not improve from 0.89606\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8784 - loss: 0.3123 - val_accuracy: 0.8961 - val_loss: 0.3012 - learning_rate: 2.5000e-05\nEpoch 179/300\n\u001b[1m96/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8918 - loss: 0.2855\nEpoch 179: val_accuracy did not improve from 0.89606\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8918 - loss: 0.2848 - val_accuracy: 0.8961 - val_loss: 0.3078 - learning_rate: 2.5000e-05\nEpoch 180/300\n\u001b[1m89/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8930 - loss: 0.2873\nEpoch 180: val_accuracy did not improve from 0.89606\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8921 - loss: 0.2897 - val_accuracy: 0.8961 - val_loss: 0.3024 - learning_rate: 2.5000e-05\nEpoch 181/300\n\u001b[1m88/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8864 - loss: 0.2465\nEpoch 181: val_accuracy did not improve from 0.89606\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8868 - loss: 0.2478 - val_accuracy: 0.8961 - val_loss: 0.3044 - learning_rate: 2.5000e-05\nEpoch 182/300\n\u001b[1m88/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8921 - loss: 0.2689\nEpoch 182: val_accuracy did not improve from 0.89606\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8926 - loss: 0.2671 - val_accuracy: 0.8925 - val_loss: 0.3068 - learning_rate: 2.5000e-05\nEpoch 183/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8990 - loss: 0.2333\nEpoch 183: val_accuracy did not improve from 0.89606\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8991 - loss: 0.2347 - val_accuracy: 0.8961 - val_loss: 0.3125 - learning_rate: 2.5000e-05\nEpoch 184/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8631 - loss: 0.3017\nEpoch 184: val_accuracy did not improve from 0.89606\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8642 - loss: 0.2999 - val_accuracy: 0.8961 - val_loss: 0.3078 - learning_rate: 2.5000e-05\nEpoch 185/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8748 - loss: 0.3121\nEpoch 185: val_accuracy improved from 0.89606 to 0.89964, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8759 - loss: 0.3081 - val_accuracy: 0.8996 - val_loss: 0.3055 - learning_rate: 2.5000e-05\nEpoch 186/300\n\u001b[1m92/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8932 - loss: 0.3149\nEpoch 186: val_accuracy did not improve from 0.89964\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8930 - loss: 0.3143 - val_accuracy: 0.8925 - val_loss: 0.3053 - learning_rate: 2.5000e-05\nEpoch 187/300\n\u001b[1m92/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8821 - loss: 0.3010\nEpoch 187: val_accuracy did not improve from 0.89964\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8834 - loss: 0.2982 - val_accuracy: 0.8996 - val_loss: 0.3069 - learning_rate: 2.5000e-05\nEpoch 188/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8907 - loss: 0.2433\nEpoch 188: val_accuracy did not improve from 0.89964\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8906 - loss: 0.2441 - val_accuracy: 0.8996 - val_loss: 0.3055 - learning_rate: 2.5000e-05\nEpoch 189/300\n\u001b[1m89/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8948 - loss: 0.2646\nEpoch 189: val_accuracy did not improve from 0.89964\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8946 - loss: 0.2645 - val_accuracy: 0.8996 - val_loss: 0.3050 - learning_rate: 2.5000e-05\nEpoch 190/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9012 - loss: 0.2581\nEpoch 190: val_accuracy did not improve from 0.89964\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9015 - loss: 0.2572 - val_accuracy: 0.8961 - val_loss: 0.3046 - learning_rate: 2.5000e-05\nEpoch 191/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8800 - loss: 0.2986\nEpoch 191: val_accuracy did not improve from 0.89964\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8804 - loss: 0.2979 - val_accuracy: 0.8996 - val_loss: 0.3073 - learning_rate: 2.5000e-05\nEpoch 192/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8852 - loss: 0.3138\nEpoch 192: val_accuracy did not improve from 0.89964\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8858 - loss: 0.3104 - val_accuracy: 0.8961 - val_loss: 0.3070 - learning_rate: 2.5000e-05\nEpoch 193/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8867 - loss: 0.2898\nEpoch 193: val_accuracy did not improve from 0.89964\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8874 - loss: 0.2881 - val_accuracy: 0.8996 - val_loss: 0.3043 - learning_rate: 2.5000e-05\nEpoch 194/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8950 - loss: 0.2451\nEpoch 194: val_accuracy did not improve from 0.89964\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8949 - loss: 0.2459 - val_accuracy: 0.8996 - val_loss: 0.3063 - learning_rate: 2.5000e-05\nEpoch 195/300\n\u001b[1m89/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8834 - loss: 0.2509\nEpoch 195: val_accuracy did not improve from 0.89964\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8839 - loss: 0.2522 - val_accuracy: 0.8961 - val_loss: 0.3044 - learning_rate: 2.5000e-05\nEpoch 196/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9026 - loss: 0.2425\nEpoch 196: val_accuracy did not improve from 0.89964\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9013 - loss: 0.2443 - val_accuracy: 0.8961 - val_loss: 0.3047 - learning_rate: 2.5000e-05\nEpoch 197/300\n\u001b[1m89/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9077 - loss: 0.2426\nEpoch 197: val_accuracy improved from 0.89964 to 0.90323, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9073 - loss: 0.2440 - val_accuracy: 0.9032 - val_loss: 0.3042 - learning_rate: 2.5000e-05\nEpoch 198/300\n\u001b[1m87/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9083 - loss: 0.2330\nEpoch 198: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9088 - loss: 0.2317 - val_accuracy: 0.9032 - val_loss: 0.3016 - learning_rate: 2.5000e-05\nEpoch 199/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8989 - loss: 0.2239\nEpoch 199: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8989 - loss: 0.2254 - val_accuracy: 0.9032 - val_loss: 0.2953 - learning_rate: 2.5000e-05\nEpoch 200/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8957 - loss: 0.2675\nEpoch 200: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8958 - loss: 0.2660 - val_accuracy: 0.9032 - val_loss: 0.2941 - learning_rate: 2.5000e-05\nEpoch 201/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8985 - loss: 0.2663\nEpoch 201: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8983 - loss: 0.2661 - val_accuracy: 0.8996 - val_loss: 0.2971 - learning_rate: 2.5000e-05\nEpoch 202/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8711 - loss: 0.2957\nEpoch 202: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8729 - loss: 0.2920 - val_accuracy: 0.8996 - val_loss: 0.2940 - learning_rate: 2.5000e-05\nEpoch 203/300\n\u001b[1m92/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8900 - loss: 0.2748\nEpoch 203: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8905 - loss: 0.2739 - val_accuracy: 0.8996 - val_loss: 0.3005 - learning_rate: 2.5000e-05\nEpoch 204/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9032 - loss: 0.2519\nEpoch 204: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9040 - loss: 0.2497 - val_accuracy: 0.8996 - val_loss: 0.3017 - learning_rate: 2.5000e-05\nEpoch 205/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8965 - loss: 0.2451\nEpoch 205: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8968 - loss: 0.2464 - val_accuracy: 0.9032 - val_loss: 0.2997 - learning_rate: 2.5000e-05\nEpoch 206/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8957 - loss: 0.2589\nEpoch 206: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8960 - loss: 0.2578 - val_accuracy: 0.9032 - val_loss: 0.3041 - learning_rate: 2.5000e-05\nEpoch 207/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9101 - loss: 0.2346\nEpoch 207: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9098 - loss: 0.2359 - val_accuracy: 0.8996 - val_loss: 0.3069 - learning_rate: 2.5000e-05\nEpoch 208/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9009 - loss: 0.2496\nEpoch 208: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9004 - loss: 0.2500 - val_accuracy: 0.9032 - val_loss: 0.3030 - learning_rate: 2.5000e-05\nEpoch 209/300\n\u001b[1m92/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8845 - loss: 0.2799\nEpoch 209: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8851 - loss: 0.2794 - val_accuracy: 0.9032 - val_loss: 0.3044 - learning_rate: 2.5000e-05\nEpoch 210/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9116 - loss: 0.2096\nEpoch 210: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9118 - loss: 0.2100 - val_accuracy: 0.8996 - val_loss: 0.3024 - learning_rate: 2.5000e-05\nEpoch 211/300\n\u001b[1m92/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8793 - loss: 0.2627\nEpoch 211: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8798 - loss: 0.2633 - val_accuracy: 0.8925 - val_loss: 0.3021 - learning_rate: 2.5000e-05\nEpoch 212/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9001 - loss: 0.2773\nEpoch 212: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9007 - loss: 0.2746 - val_accuracy: 0.8961 - val_loss: 0.3027 - learning_rate: 2.5000e-05\nEpoch 213/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9103 - loss: 0.2387\nEpoch 213: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9098 - loss: 0.2393 - val_accuracy: 0.8996 - val_loss: 0.3003 - learning_rate: 2.5000e-05\nEpoch 214/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8913 - loss: 0.2797\nEpoch 214: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8924 - loss: 0.2760 - val_accuracy: 0.8961 - val_loss: 0.2976 - learning_rate: 2.5000e-05\nEpoch 215/300\n\u001b[1m89/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9058 - loss: 0.2291\nEpoch 215: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9062 - loss: 0.2275 - val_accuracy: 0.8961 - val_loss: 0.2985 - learning_rate: 2.5000e-05\nEpoch 216/300\n\u001b[1m97/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9091 - loss: 0.2233\nEpoch 216: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9087 - loss: 0.2241 - val_accuracy: 0.8961 - val_loss: 0.2991 - learning_rate: 2.5000e-05\nEpoch 217/300\n\u001b[1m92/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9032 - loss: 0.2339\nEpoch 217: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n\nEpoch 217: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9031 - loss: 0.2344 - val_accuracy: 0.8961 - val_loss: 0.2990 - learning_rate: 2.5000e-05\nEpoch 218/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9061 - loss: 0.2487\nEpoch 218: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9063 - loss: 0.2484 - val_accuracy: 0.8925 - val_loss: 0.3019 - learning_rate: 1.2500e-05\nEpoch 219/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9138 - loss: 0.2398\nEpoch 219: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9126 - loss: 0.2416 - val_accuracy: 0.8961 - val_loss: 0.2989 - learning_rate: 1.2500e-05\nEpoch 220/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9081 - loss: 0.2517\nEpoch 220: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9079 - loss: 0.2496 - val_accuracy: 0.8961 - val_loss: 0.2985 - learning_rate: 1.2500e-05\nEpoch 221/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8868 - loss: 0.2368\nEpoch 221: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8880 - loss: 0.2369 - val_accuracy: 0.8961 - val_loss: 0.2988 - learning_rate: 1.2500e-05\nEpoch 222/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9198 - loss: 0.2101\nEpoch 222: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9184 - loss: 0.2131 - val_accuracy: 0.8961 - val_loss: 0.2982 - learning_rate: 1.2500e-05\nEpoch 223/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8851 - loss: 0.2987\nEpoch 223: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8866 - loss: 0.2947 - val_accuracy: 0.8961 - val_loss: 0.2966 - learning_rate: 1.2500e-05\nEpoch 224/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9065 - loss: 0.2581\nEpoch 224: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9063 - loss: 0.2577 - val_accuracy: 0.8961 - val_loss: 0.2993 - learning_rate: 1.2500e-05\nEpoch 225/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9105 - loss: 0.2243\nEpoch 225: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9101 - loss: 0.2249 - val_accuracy: 0.8961 - val_loss: 0.2976 - learning_rate: 1.2500e-05\nEpoch 226/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9048 - loss: 0.2304\nEpoch 226: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9044 - loss: 0.2299 - val_accuracy: 0.8961 - val_loss: 0.2981 - learning_rate: 1.2500e-05\nEpoch 227/300\n\u001b[1m92/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9046 - loss: 0.2543\nEpoch 227: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9048 - loss: 0.2539 - val_accuracy: 0.8961 - val_loss: 0.2985 - learning_rate: 1.2500e-05\nEpoch 228/300\n\u001b[1m92/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8818 - loss: 0.2611\nEpoch 228: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8823 - loss: 0.2611 - val_accuracy: 0.8961 - val_loss: 0.2986 - learning_rate: 1.2500e-05\nEpoch 229/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9091 - loss: 0.2196\nEpoch 229: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9095 - loss: 0.2203 - val_accuracy: 0.8961 - val_loss: 0.2975 - learning_rate: 1.2500e-05\nEpoch 230/300\n\u001b[1m89/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8996 - loss: 0.2386\nEpoch 230: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8993 - loss: 0.2391 - val_accuracy: 0.8961 - val_loss: 0.2978 - learning_rate: 1.2500e-05\nEpoch 231/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8994 - loss: 0.2341\nEpoch 231: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8991 - loss: 0.2347 - val_accuracy: 0.8961 - val_loss: 0.3012 - learning_rate: 1.2500e-05\nEpoch 232/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8992 - loss: 0.2394\nEpoch 232: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8997 - loss: 0.2383 - val_accuracy: 0.8961 - val_loss: 0.3022 - learning_rate: 1.2500e-05\nEpoch 233/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9083 - loss: 0.2493\nEpoch 233: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9073 - loss: 0.2495 - val_accuracy: 0.8925 - val_loss: 0.3037 - learning_rate: 1.2500e-05\nEpoch 234/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8953 - loss: 0.2639\nEpoch 234: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8957 - loss: 0.2639 - val_accuracy: 0.8961 - val_loss: 0.3029 - learning_rate: 1.2500e-05\nEpoch 235/300\n\u001b[1m93/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8918 - loss: 0.2731\nEpoch 235: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8919 - loss: 0.2728 - val_accuracy: 0.8925 - val_loss: 0.3028 - learning_rate: 1.2500e-05\nEpoch 236/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9199 - loss: 0.2266\nEpoch 236: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9190 - loss: 0.2277 - val_accuracy: 0.8925 - val_loss: 0.3010 - learning_rate: 1.2500e-05\nEpoch 237/300\n\u001b[1m92/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8947 - loss: 0.2721\nEpoch 237: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n\nEpoch 237: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8958 - loss: 0.2695 - val_accuracy: 0.8925 - val_loss: 0.3016 - learning_rate: 1.2500e-05\nEpoch 237: early stopping\nRestoring model weights from the end of the best epoch: 197.\n\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step\n\nğŸ¯ Validation Accuracy: 0.9032\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1200x500 with 2 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# ======================================\n# Prepare test tabular features\n# ======================================\n\n# 1ï¸âƒ£ Select tabular features from test_df\ntab_features = ['age', 'gender', 'tbContactHistory', 'wheezingHistory', \n                'phlegmCough', 'familyAsthmaHistory', 'feverHistory', \n                'coldPresent', 'packYears']\n\nX_tab_test = test_df[tab_features].values\n\n# 2ï¸âƒ£ Apply the same Iterative Imputer as training\nX_tab_test_imputed = iterative_imputer.transform(X_tab_test)\n\n# 3ï¸âƒ£ Apply the same StandardScaler as training\nX_tab_test_scaled = scaler_tab.transform(X_tab_test_imputed)\n\n# 4ï¸âƒ£ Reshape test audio features\nX_audio_test_fixed = np.array(X_audio_test)  # Ensure it's np.array\nX_audio_test_flat = X_audio_test_fixed.reshape(X_audio_test_fixed.shape[0], -1)\n\n# 5ï¸âƒ£ Predict\nsuper_pred_test = super_model.predict([X_tab_test_scaled, X_audio_test_flat], verbose=1)\ny_pred_test = np.argmax(super_pred_test, axis=1)\n\n# 6ï¸âƒ£ Display results\ntest_results_df = pd.DataFrame({\n    'candidateID': test_df['candidateID'],\n    'predicted_disease': y_pred_test\n})\n\nprint(\"ğŸ¯ Sample predictions on test data:\")\nprint(test_results_df.head(10))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T17:53:30.034469Z","iopub.execute_input":"2025-12-07T17:53:30.034695Z","iopub.status.idle":"2025-12-07T17:53:30.573549Z","shell.execute_reply.started":"2025-12-07T17:53:30.034677Z","shell.execute_reply":"2025-12-07T17:53:30.572927Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\nğŸ¯ Sample predictions on test data:\n     candidateID  predicted_disease\n0  136bac9a3e081                  0\n1  b121e45942a46                  1\n2  6b6853c07e4fb                  2\n3  71de185eac888                  2\n4  25deed742f133                  0\n5  1de4591779d31                  2\n6  102efeabb10a5                  1\n7  522d1f8600a13                  0\n8  e41530046a74e                  1\n9  6337b96a160eb                  0\n","output_type":"stream"}],"execution_count":4}]}