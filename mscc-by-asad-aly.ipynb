{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":87331,"databundleVersionId":10191418,"sourceType":"competition"}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ======================================\n# Complete Preprocessing + Train/Test Handling (Combined Snippet)\n# ======================================\n\nimport os\nimport librosa\nimport numpy as np\nimport pandas as pd\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\n\n# -----------------------------\n# 1ï¸âƒ£ Load Data\n# -----------------------------\ntrain_csv = \"/kaggle/input/airs-ai-in-respiratory-sounds/train.csv\"\ntest_csv  = \"/kaggle/input/airs-ai-in-respiratory-sounds/test.csv\"\naudio_path = \"/kaggle/input/airs-ai-in-respiratory-sounds/sounds/sounds\"\n\ntrain_df = pd.read_csv(train_csv)\ntest_df  = pd.read_csv(test_csv)\n\ntab_features = ['age', 'gender', 'tbContactHistory', 'wheezingHistory', 'phlegmCough',\n                'familyAsthmaHistory', 'feverHistory', 'coldPresent', 'packYears']\n\n# -----------------------------\n# 2ï¸âƒ£ Map audio files\n# -----------------------------\nfile_map = {}\nfor folder in os.listdir(audio_path):\n    fpath = os.path.join(audio_path, folder)\n    if os.path.isdir(fpath):\n        wavs = [f for f in os.listdir(fpath) if f.endswith(\".wav\")]\n        if wavs:\n            file_map[folder] = os.path.join(fpath, wavs[0])\n\n# -----------------------------\n# 3ï¸âƒ£ MFCC Extraction Function\n# -----------------------------\ndef extract_mfcc_features(file_path, n_mfcc=40, duration=5, sr=22050):\n    try:\n        y, sr = librosa.load(file_path, sr=sr, duration=duration)\n        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n        mfcc_scaled = np.mean(mfcc.T, axis=0)\n        chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n        chroma_scaled = np.mean(chroma.T, axis=0)\n        mel = librosa.feature.melspectrogram(y=y, sr=sr)\n        mel_scaled = np.mean(mel.T, axis=0)\n        features = np.hstack([mfcc_scaled, chroma_scaled, mel_scaled])\n        return features\n    except:\n        return np.zeros(n_mfcc + 12 + 128)\n\n# -----------------------------\n# 4ï¸âƒ£ Extract features for train\n# -----------------------------\nX_audio = []\nvalid_ids = []\nfor i, cid in enumerate(train_df['candidateID']):\n    if cid in file_map:\n        X_audio.append(extract_mfcc_features(file_map[cid]))\n        valid_ids.append(cid)\nX_audio = np.array(X_audio)\n\ndf = train_df[train_df['candidateID'].isin(valid_ids)]\nX_tab = df[tab_features].values\ny = df['disease'].values\n\n# -----------------------------\n# 5ï¸âƒ£ Extract features for test\n# -----------------------------\nX_audio_test = []\nfor i, cid in enumerate(test_df['candidateID']):\n    if cid in file_map:\n        X_audio_test.append(extract_mfcc_features(file_map[cid]))\nX_audio_test = np.array(X_audio_test)\nX_tab_test = test_df[tab_features].values\n\n# -----------------------------\n# 6ï¸âƒ£ Iterative Imputation + Scaling (Train + Test)\n# -----------------------------\niterative_imputer = IterativeImputer(\n    max_iter=10,\n    random_state=42,\n    estimator=RandomForestClassifier(n_estimators=50, random_state=42)\n)\nX_tab_iterative = iterative_imputer.fit_transform(X_tab)\nX_tab_test_iterative = iterative_imputer.transform(X_tab_test)  # apply same imputer\n\nscaler_tab = StandardScaler()\nX_tab_scaled = scaler_tab.fit_transform(X_tab_iterative)\nX_tab_scaled_test = scaler_tab.transform(X_tab_test_iterative)\n\n# -----------------------------\n# 7ï¸âƒ£ Train-validation split\n# -----------------------------\nX_tab_train, X_tab_val, X_audio_train, X_audio_val, y_train, y_val = train_test_split(\n    X_tab_scaled, X_audio, y,\n    test_size=0.15,\n    stratify=y,\n    random_state=42\n)\n\n# Reshape audio for model\nX_audio_train = X_audio_train.reshape(X_audio_train.shape[0], X_audio_train.shape[1], 1)\nX_audio_val = X_audio_val.reshape(X_audio_val.shape[0], X_audio_val.shape[1], 1)\nX_audio_test_fixed = X_audio_test.reshape(X_audio_test.shape[0], X_audio_test.shape[1], 1)\n\n# -----------------------------\n# 8ï¸âƒ£ Class Weights\n# -----------------------------\nclass_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\nclass_weight_dict = dict(enumerate(class_weights))\n\n# -----------------------------\n# 9ï¸âƒ£ Flatten MFCC for augmentation (Train only)\n# -----------------------------\nX_mel_2d = X_audio_train.reshape(X_audio_train.shape[0], -1)\nX_tab_mel = X_tab_train\ny_mel = y_train\n\ndef advanced_flat_augmentation(feature_vector):\n    augmented = feature_vector.copy()\n    if np.random.random() > 0.5:\n        augmented += np.random.normal(0, 0.01, augmented.shape)\n    if np.random.random() > 0.5:\n        augmented = np.roll(augmented, np.random.randint(-5,5))\n    return augmented\n\nX_mel_augmented = []\nX_tab_augmented = []\ny_augmented = []\n\n# Original train data\nX_mel_augmented.extend(X_mel_2d)\nX_tab_augmented.extend(X_tab_mel)\ny_augmented.extend(y_mel)\n\n# Augmented data (3x)\nfor i in range(len(X_mel_2d)):\n    for _ in range(3):\n        aug_mel = advanced_flat_augmentation(X_mel_2d[i])\n        X_mel_augmented.append(aug_mel)\n        X_tab_augmented.append(X_tab_mel[i])\n        y_augmented.append(y_mel[i])\n\nX_mel_augmented = np.array(X_mel_augmented)\nX_tab_augmented = np.array(X_tab_augmented)\ny_augmented = np.array(y_augmented)\n\nprint(\"âœ… Preprocessing complete! Ready for training + testing.\")\nprint(f\"Train samples (augmented): {len(X_mel_augmented)}, Test samples: {X_audio_test_fixed.shape[0]}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T11:27:26.477191Z","iopub.execute_input":"2025-12-07T11:27:26.477783Z","iopub.status.idle":"2025-12-07T11:28:02.562371Z","shell.execute_reply.started":"2025-12-07T11:27:26.477762Z","shell.execute_reply":"2025-12-07T11:28:02.561629Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/librosa/core/pitch.py:103: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/usr/local/lib/python3.11/dist-packages/librosa/core/pitch.py:103: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n","output_type":"stream"},{"name":"stdout","text":"âœ… Preprocessing complete! Ready for training + testing.\nTrain samples (augmented): 1856, Test samples: 338\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"# -----------------------------\n# 7ï¸âƒ£ Train-validation split (85% train, 15% validation)\n# -----------------------------\nX_tab_train, X_tab_val, X_audio_train, X_audio_val, y_train, y_val = train_test_split(\n    X_tab_scaled, X_audio, y,\n    test_size=0.15,       # 15% for validation\n    stratify=y,\n    random_state=42\n)\n\n# Reshape audio for model\nX_audio_train = X_audio_train.reshape(X_audio_train.shape[0], X_audio_train.shape[1], 1)\nX_audio_val = X_audio_val.reshape(X_audio_val.shape[0], X_audio_val.shape[1], 1)\nX_audio_test_fixed = X_audio_test.reshape(X_audio_test.shape[0], X_audio_test.shape[1], 1)\n\nprint(f\"âœ… Training samples: {X_audio_train.shape[0]}, Validation samples: {X_audio_val.shape[0]}, Test samples: {X_audio_test_fixed.shape[0]}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T11:28:02.563780Z","iopub.execute_input":"2025-12-07T11:28:02.564091Z","iopub.status.idle":"2025-12-07T11:28:02.571478Z","shell.execute_reply.started":"2025-12-07T11:28:02.564067Z","shell.execute_reply":"2025-12-07T11:28:02.570742Z"}},"outputs":[{"name":"stdout","text":"âœ… Training samples: 464, Validation samples: 82, Test samples: 338\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"# ======================================\n# FIXED SUPER ADVANCED MODEL TRAINING (No PCA)\n# ======================================\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers, Model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# -----------------------------\n# 1ï¸âƒ£ Convert augmented lists to arrays\n# -----------------------------\nX_mel_augmented = np.array(X_mel_augmented)\nX_tab_augmented = np.array(X_tab_augmented)\ny_augmented = np.array(y_augmented)\n\n# -----------------------------\n# 2ï¸âƒ£ Train-validation split (85% train, 15% validation)\n# -----------------------------\nX_mel_aug_train, X_mel_aug_val, X_tab_aug_train, X_tab_aug_val, y_aug_train, y_aug_val = train_test_split(\n    X_mel_augmented, X_tab_augmented, y_augmented,\n    test_size=0.15,\n    stratify=y_augmented,\n    random_state=42\n)\n\n# -----------------------------\n# 3ï¸âƒ£ Flatten MFCC features\n# -----------------------------\nX_mel_aug_train_flat = X_mel_aug_train.reshape(X_mel_aug_train.shape[0], -1)\nX_mel_aug_val_flat   = X_mel_aug_val.reshape(X_mel_aug_val.shape[0], -1)\n\nprint(f\"âœ… Flattened MFCC features: Train {X_mel_aug_train_flat.shape}, Val {X_mel_aug_val_flat.shape}\")\n\n# -----------------------------\n# 4ï¸âƒ£ Create Fixed Super Advanced Model\n# -----------------------------\nnum_classes = 3\n\ndef create_fixed_super_advanced_model(tabular_dim, pretrained_dim, num_classes):\n    \"\"\"Fixed super advanced model\"\"\"\n    \n    pretrained_input = tf.keras.Input(shape=(pretrained_dim,), name='pretrained_features')\n    x_pre = layers.Dense(1024, activation='relu')(pretrained_input)\n    x_pre = layers.BatchNormalization()(x_pre)\n    x_pre = layers.Dropout(0.4)(x_pre)\n    x_pre = layers.Dense(512, activation='relu')(x_pre)\n    x_pre = layers.BatchNormalization()(x_pre)\n    x_pre = layers.Dropout(0.3)(x_pre)\n    x_pre = layers.Dense(256, activation='relu')(x_pre)\n    x_pre = layers.BatchNormalization()(x_pre)\n    x_pre = layers.Dropout(0.2)(x_pre)\n    \n    tabular_input = tf.keras.Input(shape=(tabular_dim,), name='tabular_input')\n    x_tab = layers.Dense(256, activation='relu')(tabular_input)\n    x_tab = layers.BatchNormalization()(x_tab)\n    x_tab = layers.Dropout(0.3)(x_tab)\n    x_tab = layers.Dense(128, activation='relu')(x_tab)\n    x_tab = layers.BatchNormalization()(x_tab)\n    x_tab = layers.Dropout(0.2)(x_tab)\n    x_tab = layers.Dense(64, activation='relu')(x_tab)\n    \n    combined = layers.concatenate([x_pre, x_tab])\n    x = layers.Dense(512, activation='relu')(combined)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.5)(x)\n    \n    residual1 = layers.Dense(512, activation='relu')(x)\n    residual1 = layers.BatchNormalization()(residual1)\n    x = layers.add([x, residual1])\n    x = layers.Dropout(0.4)(x)\n    \n    x = layers.Dense(256, activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    residual2 = layers.Dense(256, activation='relu')(x)\n    residual2 = layers.BatchNormalization()(residual2)\n    x = layers.add([x, residual2])\n    x = layers.Dropout(0.3)(x)\n    \n    x = layers.Dense(128, activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.2)(x)\n    x = layers.Dense(64, activation='relu')(x)\n    \n    output = layers.Dense(num_classes, activation='softmax')(x)\n    \n    model = Model(inputs=[tabular_input, pretrained_input], outputs=output, name='fixed_super_advanced_model')\n    return model\n\nsuper_model = create_fixed_super_advanced_model(\n    tabular_dim=X_tab_aug_train.shape[1],\n    pretrained_dim=X_mel_aug_train_flat.shape[1],\n    num_classes=num_classes\n)\n\nsuper_model.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nprint(\"âœ… Model created and compiled!\")\nsuper_model.summary()\n\n# -----------------------------\n# 5ï¸âƒ£ Compute class weights\n# -----------------------------\nclass_weights = compute_class_weight('balanced', classes=np.unique(y_aug_train), y=y_aug_train)\nclass_weight_dict = dict(enumerate(class_weights))\nprint(f\"âœ… Class weights: {class_weight_dict}\")\n\n# -----------------------------\n# 6ï¸âƒ£ Callbacks\n# -----------------------------\ncallbacks = [\n    tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=40, restore_best_weights=True, verbose=1),\n    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=20, min_lr=1e-8, verbose=1),\n    tf.keras.callbacks.ModelCheckpoint('fixed_super_advanced_model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n]\n\n# -----------------------------\n# 7ï¸âƒ£ Train the model\n# -----------------------------\nsuper_history = super_model.fit(\n    [X_tab_aug_train, X_mel_aug_train_flat],\n    y_aug_train,\n    batch_size=16,\n    epochs=300,\n    validation_data=([X_tab_aug_val, X_mel_aug_val_flat], y_aug_val),\n    class_weight=class_weight_dict,\n    callbacks=callbacks,\n    verbose=1\n)\n\n# -----------------------------\n# 8ï¸âƒ£ Evaluate on validation\n# -----------------------------\nfrom sklearn.metrics import accuracy_score\n\nsuper_model.load_weights('fixed_super_advanced_model.h5')\nsuper_pred = super_model.predict([X_tab_aug_val, X_mel_aug_val_flat])\nsuper_accuracy = accuracy_score(y_aug_val, np.argmax(super_pred, axis=1))\nprint(f\"\\nğŸ¯ Validation Accuracy: {super_accuracy:.4f}\")\n\n# -----------------------------\n# 9ï¸âƒ£ Plot train / validation metrics\n# -----------------------------\nplt.figure(figsize=(12,5))\nplt.subplot(1,2,1)\nplt.plot(super_history.history['loss'], label='Train Loss')\nplt.plot(super_history.history['val_loss'], label='Val Loss')\nplt.title('Loss')\nplt.legend()\n\nplt.subplot(1,2,2)\nplt.plot(super_history.history['accuracy'], label='Train Accuracy')\nplt.plot(super_history.history['val_accuracy'], label='Val Accuracy')\nplt.title('Accuracy')\nplt.legend()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T11:28:02.572770Z","iopub.execute_input":"2025-12-07T11:28:02.573191Z","iopub.status.idle":"2025-12-07T11:30:18.847776Z","shell.execute_reply.started":"2025-12-07T11:28:02.573168Z","shell.execute_reply":"2025-12-07T11:30:18.847097Z"}},"outputs":[{"name":"stdout","text":"âœ… Flattened MFCC features: Train (1577, 180), Val (279, 180)\nâœ… Model created and compiled!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"fixed_super_advanced_model\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"fixed_super_advanced_model\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ pretrained_features â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m180\u001b[0m)       â”‚          \u001b[38;5;34m0\u001b[0m â”‚ -                 â”‚\nâ”‚ (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_26 (\u001b[38;5;33mDense\u001b[0m)    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)      â”‚    \u001b[38;5;34m185,344\u001b[0m â”‚ pretrained_featuâ€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)      â”‚      \u001b[38;5;34m4,096\u001b[0m â”‚ dense_26[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ tabular_input       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m)         â”‚          \u001b[38;5;34m0\u001b[0m â”‚ -                 â”‚\nâ”‚ (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_18          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)      â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\nâ”‚ (\u001b[38;5;33mDropout\u001b[0m)           â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_29 (\u001b[38;5;33mDense\u001b[0m)    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       â”‚      \u001b[38;5;34m2,560\u001b[0m â”‚ tabular_input[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_27 (\u001b[38;5;33mDense\u001b[0m)    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       â”‚    \u001b[38;5;34m524,800\u001b[0m â”‚ dropout_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       â”‚      \u001b[38;5;34m1,024\u001b[0m â”‚ dense_29[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       â”‚      \u001b[38;5;34m2,048\u001b[0m â”‚ dense_27[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_21          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\nâ”‚ (\u001b[38;5;33mDropout\u001b[0m)           â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_19          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\nâ”‚ (\u001b[38;5;33mDropout\u001b[0m)           â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_30 (\u001b[38;5;33mDense\u001b[0m)    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚     \u001b[38;5;34m32,896\u001b[0m â”‚ dropout_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_28 (\u001b[38;5;33mDense\u001b[0m)    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       â”‚    \u001b[38;5;34m131,328\u001b[0m â”‚ dropout_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚        \u001b[38;5;34m512\u001b[0m â”‚ dense_30[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       â”‚      \u001b[38;5;34m1,024\u001b[0m â”‚ dense_28[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_22          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\nâ”‚ (\u001b[38;5;33mDropout\u001b[0m)           â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_20          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\nâ”‚ (\u001b[38;5;33mDropout\u001b[0m)           â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_31 (\u001b[38;5;33mDense\u001b[0m)    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        â”‚      \u001b[38;5;34m8,256\u001b[0m â”‚ dropout_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ concatenate_2       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m320\u001b[0m)       â”‚          \u001b[38;5;34m0\u001b[0m â”‚ dropout_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], â”‚\nâ”‚ (\u001b[38;5;33mConcatenate\u001b[0m)       â”‚                   â”‚            â”‚ dense_31[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_32 (\u001b[38;5;33mDense\u001b[0m)    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       â”‚    \u001b[38;5;34m164,352\u001b[0m â”‚ concatenate_2[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       â”‚      \u001b[38;5;34m2,048\u001b[0m â”‚ dense_32[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_23          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\nâ”‚ (\u001b[38;5;33mDropout\u001b[0m)           â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_33 (\u001b[38;5;33mDense\u001b[0m)    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       â”‚    \u001b[38;5;34m262,656\u001b[0m â”‚ dropout_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       â”‚      \u001b[38;5;34m2,048\u001b[0m â”‚ dense_33[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ add_4 (\u001b[38;5;33mAdd\u001b[0m)         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       â”‚          \u001b[38;5;34m0\u001b[0m â”‚ dropout_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], â”‚\nâ”‚                     â”‚                   â”‚            â”‚ batch_normalizatâ€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_24          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       â”‚          \u001b[38;5;34m0\u001b[0m â”‚ add_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       â”‚\nâ”‚ (\u001b[38;5;33mDropout\u001b[0m)           â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_34 (\u001b[38;5;33mDense\u001b[0m)    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       â”‚    \u001b[38;5;34m131,328\u001b[0m â”‚ dropout_24[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       â”‚      \u001b[38;5;34m1,024\u001b[0m â”‚ dense_34[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_35 (\u001b[38;5;33mDense\u001b[0m)    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       â”‚     \u001b[38;5;34m65,792\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       â”‚      \u001b[38;5;34m1,024\u001b[0m â”‚ dense_35[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ add_5 (\u001b[38;5;33mAdd\u001b[0m)         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\nâ”‚                     â”‚                   â”‚            â”‚ batch_normalizatâ€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_25          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       â”‚          \u001b[38;5;34m0\u001b[0m â”‚ add_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       â”‚\nâ”‚ (\u001b[38;5;33mDropout\u001b[0m)           â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_36 (\u001b[38;5;33mDense\u001b[0m)    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚     \u001b[38;5;34m32,896\u001b[0m â”‚ dropout_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚        \u001b[38;5;34m512\u001b[0m â”‚ dense_36[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_26          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\nâ”‚ (\u001b[38;5;33mDropout\u001b[0m)           â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_37 (\u001b[38;5;33mDense\u001b[0m)    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        â”‚      \u001b[38;5;34m8,256\u001b[0m â”‚ dropout_26[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_38 (\u001b[38;5;33mDense\u001b[0m)    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)         â”‚        \u001b[38;5;34m195\u001b[0m â”‚ dense_37[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ<span style=\"font-weight: bold\"> Layer (type)        </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape      </span>â”ƒ<span style=\"font-weight: bold\">    Param # </span>â”ƒ<span style=\"font-weight: bold\"> Connected to      </span>â”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ pretrained_features â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">180</span>)       â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                 â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)      â”‚    <span style=\"color: #00af00; text-decoration-color: #00af00\">185,344</span> â”‚ pretrained_featuâ€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)      â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span> â”‚ dense_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ tabular_input       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)         â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                 â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_18          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)      â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560</span> â”‚ tabular_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       â”‚    <span style=\"color: #00af00; text-decoration-color: #00af00\">524,800</span> â”‚ dropout_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> â”‚ dense_29[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> â”‚ dense_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_21          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_19          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> â”‚ dropout_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       â”‚    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> â”‚ dropout_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> â”‚ dense_30[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> â”‚ dense_28[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_22          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_20          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> â”‚ dropout_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ concatenate_2       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>)       â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ dropout_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       â”‚                   â”‚            â”‚ dense_31[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       â”‚    <span style=\"color: #00af00; text-decoration-color: #00af00\">164,352</span> â”‚ concatenate_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> â”‚ dense_32[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_23          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       â”‚    <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span> â”‚ dropout_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> â”‚ dense_33[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ add_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ dropout_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], â”‚\nâ”‚                     â”‚                   â”‚            â”‚ batch_normalizatâ€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_24          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ add_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_34 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       â”‚    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> â”‚ dropout_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> â”‚ dense_34[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_35 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> â”‚ batch_normalizatâ€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> â”‚ dense_35[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ add_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\nâ”‚                     â”‚                   â”‚            â”‚ batch_normalizatâ€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_25          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ add_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_36 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> â”‚ dropout_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> â”‚ dense_36[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_26          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_37 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> â”‚ dropout_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_38 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">195</span> â”‚ dense_37[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,566,019\u001b[0m (5.97 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,566,019</span> (5.97 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,558,339\u001b[0m (5.94 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,558,339</span> (5.94 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m7,680\u001b[0m (30.00 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,680</span> (30.00 KB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"âœ… Class weights: {0: 1.301155115511551, 1: 0.7651625424551188, 2: 1.0816186556927299}\nEpoch 1/300\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.2825 - loss: 1.6869\nEpoch 1: val_accuracy improved from -inf to 0.30466, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 76ms/step - accuracy: 0.2826 - loss: 1.6858 - val_accuracy: 0.3047 - val_loss: 1.2430 - learning_rate: 1.0000e-04\nEpoch 2/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3197 - loss: 1.3735\nEpoch 2: val_accuracy did not improve from 0.30466\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3201 - loss: 1.3702 - val_accuracy: 0.2903 - val_loss: 1.2089 - learning_rate: 1.0000e-04\nEpoch 3/300\n\u001b[1m88/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3337 - loss: 1.2603\nEpoch 3: val_accuracy improved from 0.30466 to 0.32616, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.3340 - loss: 1.2618 - val_accuracy: 0.3262 - val_loss: 1.1578 - learning_rate: 1.0000e-04\nEpoch 4/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3633 - loss: 1.1953\nEpoch 4: val_accuracy improved from 0.32616 to 0.44086, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.3656 - loss: 1.1941 - val_accuracy: 0.4409 - val_loss: 1.0869 - learning_rate: 1.0000e-04\nEpoch 5/300\n\u001b[1m89/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4201 - loss: 1.1494\nEpoch 5: val_accuracy improved from 0.44086 to 0.51254, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4194 - loss: 1.1509 - val_accuracy: 0.5125 - val_loss: 1.0108 - learning_rate: 1.0000e-04\nEpoch 6/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4327 - loss: 1.1188\nEpoch 6: val_accuracy improved from 0.51254 to 0.53405, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4341 - loss: 1.1173 - val_accuracy: 0.5341 - val_loss: 0.9349 - learning_rate: 1.0000e-04\nEpoch 7/300\n\u001b[1m89/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4505 - loss: 1.0627\nEpoch 7: val_accuracy improved from 0.53405 to 0.63799, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4508 - loss: 1.0625 - val_accuracy: 0.6380 - val_loss: 0.8676 - learning_rate: 1.0000e-04\nEpoch 8/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4943 - loss: 1.0358\nEpoch 8: val_accuracy improved from 0.63799 to 0.68100, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4946 - loss: 1.0357 - val_accuracy: 0.6810 - val_loss: 0.8362 - learning_rate: 1.0000e-04\nEpoch 9/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5143 - loss: 0.9638\nEpoch 9: val_accuracy improved from 0.68100 to 0.73118, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5157 - loss: 0.9633 - val_accuracy: 0.7312 - val_loss: 0.8085 - learning_rate: 1.0000e-04\nEpoch 10/300\n\u001b[1m89/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5313 - loss: 0.9898\nEpoch 10: val_accuracy improved from 0.73118 to 0.74194, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5306 - loss: 0.9875 - val_accuracy: 0.7419 - val_loss: 0.7903 - learning_rate: 1.0000e-04\nEpoch 11/300\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5942 - loss: 0.8860\nEpoch 11: val_accuracy improved from 0.74194 to 0.75986, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5943 - loss: 0.8858 - val_accuracy: 0.7599 - val_loss: 0.7561 - learning_rate: 1.0000e-04\nEpoch 12/300\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6018 - loss: 0.8646\nEpoch 12: val_accuracy improved from 0.75986 to 0.76344, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6017 - loss: 0.8647 - val_accuracy: 0.7634 - val_loss: 0.7389 - learning_rate: 1.0000e-04\nEpoch 13/300\n\u001b[1m97/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6534 - loss: 0.8017\nEpoch 13: val_accuracy improved from 0.76344 to 0.79570, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6530 - loss: 0.8021 - val_accuracy: 0.7957 - val_loss: 0.6948 - learning_rate: 1.0000e-04\nEpoch 14/300\n\u001b[1m97/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6551 - loss: 0.8245\nEpoch 14: val_accuracy did not improve from 0.79570\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6552 - loss: 0.8239 - val_accuracy: 0.7957 - val_loss: 0.6702 - learning_rate: 1.0000e-04\nEpoch 15/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6535 - loss: 0.7791\nEpoch 15: val_accuracy did not improve from 0.79570\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6543 - loss: 0.7780 - val_accuracy: 0.7885 - val_loss: 0.6519 - learning_rate: 1.0000e-04\nEpoch 16/300\n\u001b[1m98/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6699 - loss: 0.7851\nEpoch 16: val_accuracy improved from 0.79570 to 0.80287, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6704 - loss: 0.7842 - val_accuracy: 0.8029 - val_loss: 0.6290 - learning_rate: 1.0000e-04\nEpoch 17/300\n\u001b[1m98/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7197 - loss: 0.7102\nEpoch 17: val_accuracy did not improve from 0.80287\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7195 - loss: 0.7102 - val_accuracy: 0.7849 - val_loss: 0.6068 - learning_rate: 1.0000e-04\nEpoch 18/300\n\u001b[1m88/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7391 - loss: 0.7198\nEpoch 18: val_accuracy did not improve from 0.80287\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7378 - loss: 0.7193 - val_accuracy: 0.7957 - val_loss: 0.5886 - learning_rate: 1.0000e-04\nEpoch 19/300\n\u001b[1m98/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7549 - loss: 0.6982\nEpoch 19: val_accuracy did not improve from 0.80287\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7547 - loss: 0.6978 - val_accuracy: 0.7921 - val_loss: 0.5794 - learning_rate: 1.0000e-04\nEpoch 20/300\n\u001b[1m98/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7276 - loss: 0.6725\nEpoch 20: val_accuracy did not improve from 0.80287\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7276 - loss: 0.6728 - val_accuracy: 0.7849 - val_loss: 0.5858 - learning_rate: 1.0000e-04\nEpoch 21/300\n\u001b[1m96/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7505 - loss: 0.6580\nEpoch 21: val_accuracy did not improve from 0.80287\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7495 - loss: 0.6596 - val_accuracy: 0.7957 - val_loss: 0.5803 - learning_rate: 1.0000e-04\nEpoch 22/300\n\u001b[1m97/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7187 - loss: 0.7027\nEpoch 22: val_accuracy did not improve from 0.80287\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7190 - loss: 0.7018 - val_accuracy: 0.7885 - val_loss: 0.5685 - learning_rate: 1.0000e-04\nEpoch 23/300\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7604 - loss: 0.6099\nEpoch 23: val_accuracy did not improve from 0.80287\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7604 - loss: 0.6101 - val_accuracy: 0.7921 - val_loss: 0.5624 - learning_rate: 1.0000e-04\nEpoch 24/300\n\u001b[1m98/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7629 - loss: 0.6483\nEpoch 24: val_accuracy did not improve from 0.80287\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7628 - loss: 0.6484 - val_accuracy: 0.7957 - val_loss: 0.5546 - learning_rate: 1.0000e-04\nEpoch 25/300\n\u001b[1m98/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7548 - loss: 0.6222\nEpoch 25: val_accuracy did not improve from 0.80287\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7546 - loss: 0.6226 - val_accuracy: 0.7993 - val_loss: 0.5407 - learning_rate: 1.0000e-04\nEpoch 26/300\n\u001b[1m98/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7598 - loss: 0.6357\nEpoch 26: val_accuracy improved from 0.80287 to 0.81004, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7598 - loss: 0.6356 - val_accuracy: 0.8100 - val_loss: 0.5314 - learning_rate: 1.0000e-04\nEpoch 27/300\n\u001b[1m97/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7498 - loss: 0.6244\nEpoch 27: val_accuracy improved from 0.81004 to 0.81720, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7500 - loss: 0.6245 - val_accuracy: 0.8172 - val_loss: 0.5223 - learning_rate: 1.0000e-04\nEpoch 28/300\n\u001b[1m98/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7634 - loss: 0.6208\nEpoch 28: val_accuracy did not improve from 0.81720\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7636 - loss: 0.6206 - val_accuracy: 0.8065 - val_loss: 0.5215 - learning_rate: 1.0000e-04\nEpoch 29/300\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7635 - loss: 0.6011\nEpoch 29: val_accuracy did not improve from 0.81720\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7635 - loss: 0.6012 - val_accuracy: 0.8136 - val_loss: 0.5176 - learning_rate: 1.0000e-04\nEpoch 30/300\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7659 - loss: 0.6506\nEpoch 30: val_accuracy did not improve from 0.81720\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7660 - loss: 0.6503 - val_accuracy: 0.8100 - val_loss: 0.5109 - learning_rate: 1.0000e-04\nEpoch 31/300\n\u001b[1m88/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7747 - loss: 0.5918\nEpoch 31: val_accuracy did not improve from 0.81720\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7738 - loss: 0.5947 - val_accuracy: 0.8136 - val_loss: 0.5246 - learning_rate: 1.0000e-04\nEpoch 32/300\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8002 - loss: 0.5358\nEpoch 32: val_accuracy did not improve from 0.81720\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8001 - loss: 0.5361 - val_accuracy: 0.8065 - val_loss: 0.5128 - learning_rate: 1.0000e-04\nEpoch 33/300\n\u001b[1m95/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7425 - loss: 0.6289\nEpoch 33: val_accuracy did not improve from 0.81720\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7440 - loss: 0.6265 - val_accuracy: 0.8136 - val_loss: 0.5069 - learning_rate: 1.0000e-04\nEpoch 34/300\n\u001b[1m88/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7705 - loss: 0.5795\nEpoch 34: val_accuracy did not improve from 0.81720\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7718 - loss: 0.5792 - val_accuracy: 0.8029 - val_loss: 0.5051 - learning_rate: 1.0000e-04\nEpoch 35/300\n\u001b[1m97/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7704 - loss: 0.5828\nEpoch 35: val_accuracy did not improve from 0.81720\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7704 - loss: 0.5826 - val_accuracy: 0.8029 - val_loss: 0.5048 - learning_rate: 1.0000e-04\nEpoch 36/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7955 - loss: 0.5752\nEpoch 36: val_accuracy did not improve from 0.81720\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7945 - loss: 0.5740 - val_accuracy: 0.8065 - val_loss: 0.5126 - learning_rate: 1.0000e-04\nEpoch 37/300\n\u001b[1m97/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7803 - loss: 0.5631\nEpoch 37: val_accuracy did not improve from 0.81720\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7806 - loss: 0.5631 - val_accuracy: 0.8100 - val_loss: 0.5013 - learning_rate: 1.0000e-04\nEpoch 38/300\n\u001b[1m97/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7974 - loss: 0.5256\nEpoch 38: val_accuracy did not improve from 0.81720\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7971 - loss: 0.5270 - val_accuracy: 0.8100 - val_loss: 0.4934 - learning_rate: 1.0000e-04\nEpoch 39/300\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8006 - loss: 0.5777\nEpoch 39: val_accuracy did not improve from 0.81720\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8005 - loss: 0.5775 - val_accuracy: 0.8100 - val_loss: 0.4921 - learning_rate: 1.0000e-04\nEpoch 40/300\n\u001b[1m98/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7935 - loss: 0.5563\nEpoch 40: val_accuracy improved from 0.81720 to 0.82079, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7935 - loss: 0.5565 - val_accuracy: 0.8208 - val_loss: 0.4881 - learning_rate: 1.0000e-04\nEpoch 41/300\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8141 - loss: 0.5168\nEpoch 41: val_accuracy did not improve from 0.82079\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8140 - loss: 0.5168 - val_accuracy: 0.8172 - val_loss: 0.4810 - learning_rate: 1.0000e-04\nEpoch 42/300\n\u001b[1m96/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8052 - loss: 0.5225\nEpoch 42: val_accuracy improved from 0.82079 to 0.83154, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8047 - loss: 0.5228 - val_accuracy: 0.8315 - val_loss: 0.4774 - learning_rate: 1.0000e-04\nEpoch 43/300\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7869 - loss: 0.5485\nEpoch 43: val_accuracy did not improve from 0.83154\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7869 - loss: 0.5484 - val_accuracy: 0.8244 - val_loss: 0.4762 - learning_rate: 1.0000e-04\nEpoch 44/300\n\u001b[1m97/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8112 - loss: 0.4910\nEpoch 44: val_accuracy did not improve from 0.83154\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8110 - loss: 0.4920 - val_accuracy: 0.8280 - val_loss: 0.4709 - learning_rate: 1.0000e-04\nEpoch 45/300\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7797 - loss: 0.5380\nEpoch 45: val_accuracy did not improve from 0.83154\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7797 - loss: 0.5379 - val_accuracy: 0.8136 - val_loss: 0.4792 - learning_rate: 1.0000e-04\nEpoch 46/300\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8021 - loss: 0.4952\nEpoch 46: val_accuracy did not improve from 0.83154\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8021 - loss: 0.4954 - val_accuracy: 0.8100 - val_loss: 0.4644 - learning_rate: 1.0000e-04\nEpoch 47/300\n\u001b[1m98/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8157 - loss: 0.4906\nEpoch 47: val_accuracy did not improve from 0.83154\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8155 - loss: 0.4911 - val_accuracy: 0.8244 - val_loss: 0.4622 - learning_rate: 1.0000e-04\nEpoch 48/300\n\u001b[1m97/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8138 - loss: 0.5045\nEpoch 48: val_accuracy did not improve from 0.83154\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8131 - loss: 0.5053 - val_accuracy: 0.8315 - val_loss: 0.4550 - learning_rate: 1.0000e-04\nEpoch 49/300\n\u001b[1m98/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7929 - loss: 0.4958\nEpoch 49: val_accuracy did not improve from 0.83154\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7929 - loss: 0.4962 - val_accuracy: 0.8244 - val_loss: 0.4551 - learning_rate: 1.0000e-04\nEpoch 50/300\n\u001b[1m98/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8124 - loss: 0.4936\nEpoch 50: val_accuracy improved from 0.83154 to 0.83513, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8124 - loss: 0.4937 - val_accuracy: 0.8351 - val_loss: 0.4487 - learning_rate: 1.0000e-04\nEpoch 51/300\n\u001b[1m97/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8158 - loss: 0.4767\nEpoch 51: val_accuracy did not improve from 0.83513\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8157 - loss: 0.4776 - val_accuracy: 0.8244 - val_loss: 0.4415 - learning_rate: 1.0000e-04\nEpoch 52/300\n\u001b[1m98/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8141 - loss: 0.4652\nEpoch 52: val_accuracy did not improve from 0.83513\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8140 - loss: 0.4659 - val_accuracy: 0.8136 - val_loss: 0.4407 - learning_rate: 1.0000e-04\nEpoch 53/300\n\u001b[1m94/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8268 - loss: 0.4513\nEpoch 53: val_accuracy did not improve from 0.83513\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8264 - loss: 0.4533 - val_accuracy: 0.8280 - val_loss: 0.4410 - learning_rate: 1.0000e-04\nEpoch 54/300\n\u001b[1m87/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8046 - loss: 0.4819\nEpoch 54: val_accuracy did not improve from 0.83513\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8050 - loss: 0.4819 - val_accuracy: 0.8244 - val_loss: 0.4355 - learning_rate: 1.0000e-04\nEpoch 55/300\n\u001b[1m96/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8072 - loss: 0.4988\nEpoch 55: val_accuracy did not improve from 0.83513\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8072 - loss: 0.4988 - val_accuracy: 0.8280 - val_loss: 0.4356 - learning_rate: 1.0000e-04\nEpoch 56/300\n\u001b[1m97/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8105 - loss: 0.4846\nEpoch 56: val_accuracy did not improve from 0.83513\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8103 - loss: 0.4851 - val_accuracy: 0.8280 - val_loss: 0.4340 - learning_rate: 1.0000e-04\nEpoch 57/300\n\u001b[1m98/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8066 - loss: 0.4971\nEpoch 57: val_accuracy did not improve from 0.83513\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8067 - loss: 0.4970 - val_accuracy: 0.8280 - val_loss: 0.4325 - learning_rate: 1.0000e-04\nEpoch 58/300\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8134 - loss: 0.4721\nEpoch 58: val_accuracy improved from 0.83513 to 0.83871, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8134 - loss: 0.4721 - val_accuracy: 0.8387 - val_loss: 0.4290 - learning_rate: 1.0000e-04\nEpoch 59/300\n\u001b[1m88/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8167 - loss: 0.4897\nEpoch 59: val_accuracy did not improve from 0.83871\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8167 - loss: 0.4902 - val_accuracy: 0.8315 - val_loss: 0.4276 - learning_rate: 1.0000e-04\nEpoch 60/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7946 - loss: 0.5130\nEpoch 60: val_accuracy did not improve from 0.83871\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7961 - loss: 0.5104 - val_accuracy: 0.8280 - val_loss: 0.4281 - learning_rate: 1.0000e-04\nEpoch 61/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8303 - loss: 0.4366\nEpoch 61: val_accuracy improved from 0.83871 to 0.84588, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8292 - loss: 0.4379 - val_accuracy: 0.8459 - val_loss: 0.4252 - learning_rate: 1.0000e-04\nEpoch 62/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8165 - loss: 0.4571\nEpoch 62: val_accuracy did not improve from 0.84588\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8171 - loss: 0.4577 - val_accuracy: 0.8351 - val_loss: 0.4226 - learning_rate: 1.0000e-04\nEpoch 63/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8251 - loss: 0.4616\nEpoch 63: val_accuracy did not improve from 0.84588\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8252 - loss: 0.4611 - val_accuracy: 0.8315 - val_loss: 0.4197 - learning_rate: 1.0000e-04\nEpoch 64/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8358 - loss: 0.4140\nEpoch 64: val_accuracy did not improve from 0.84588\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8350 - loss: 0.4173 - val_accuracy: 0.8315 - val_loss: 0.4187 - learning_rate: 1.0000e-04\nEpoch 65/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8197 - loss: 0.4759\nEpoch 65: val_accuracy did not improve from 0.84588\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8197 - loss: 0.4746 - val_accuracy: 0.8459 - val_loss: 0.4094 - learning_rate: 1.0000e-04\nEpoch 66/300\n\u001b[1m89/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8498 - loss: 0.4190\nEpoch 66: val_accuracy did not improve from 0.84588\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8482 - loss: 0.4225 - val_accuracy: 0.8459 - val_loss: 0.4076 - learning_rate: 1.0000e-04\nEpoch 67/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8300 - loss: 0.4446\nEpoch 67: val_accuracy did not improve from 0.84588\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8297 - loss: 0.4448 - val_accuracy: 0.8351 - val_loss: 0.4062 - learning_rate: 1.0000e-04\nEpoch 68/300\n\u001b[1m89/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8324 - loss: 0.4241\nEpoch 68: val_accuracy did not improve from 0.84588\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8315 - loss: 0.4264 - val_accuracy: 0.8387 - val_loss: 0.3982 - learning_rate: 1.0000e-04\nEpoch 69/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8300 - loss: 0.4944\nEpoch 69: val_accuracy did not improve from 0.84588\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8301 - loss: 0.4913 - val_accuracy: 0.8387 - val_loss: 0.3992 - learning_rate: 1.0000e-04\nEpoch 70/300\n\u001b[1m88/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8393 - loss: 0.4179\nEpoch 70: val_accuracy did not improve from 0.84588\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8374 - loss: 0.4212 - val_accuracy: 0.8459 - val_loss: 0.3968 - learning_rate: 1.0000e-04\nEpoch 71/300\n\u001b[1m97/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8142 - loss: 0.4406\nEpoch 71: val_accuracy improved from 0.84588 to 0.85305, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8144 - loss: 0.4408 - val_accuracy: 0.8530 - val_loss: 0.4004 - learning_rate: 1.0000e-04\nEpoch 72/300\n\u001b[1m89/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8097 - loss: 0.4324\nEpoch 72: val_accuracy improved from 0.85305 to 0.85663, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8111 - loss: 0.4321 - val_accuracy: 0.8566 - val_loss: 0.3928 - learning_rate: 1.0000e-04\nEpoch 73/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8258 - loss: 0.4254\nEpoch 73: val_accuracy did not improve from 0.85663\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8260 - loss: 0.4276 - val_accuracy: 0.8495 - val_loss: 0.3935 - learning_rate: 1.0000e-04\nEpoch 74/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8222 - loss: 0.4488\nEpoch 74: val_accuracy did not improve from 0.85663\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8228 - loss: 0.4483 - val_accuracy: 0.8530 - val_loss: 0.3892 - learning_rate: 1.0000e-04\nEpoch 75/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8519 - loss: 0.3747\nEpoch 75: val_accuracy did not improve from 0.85663\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8507 - loss: 0.3783 - val_accuracy: 0.8530 - val_loss: 0.3878 - learning_rate: 1.0000e-04\nEpoch 76/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8367 - loss: 0.4293\nEpoch 76: val_accuracy did not improve from 0.85663\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8368 - loss: 0.4287 - val_accuracy: 0.8495 - val_loss: 0.3776 - learning_rate: 1.0000e-04\nEpoch 77/300\n\u001b[1m88/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8399 - loss: 0.3949\nEpoch 77: val_accuracy did not improve from 0.85663\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8399 - loss: 0.3960 - val_accuracy: 0.8566 - val_loss: 0.3733 - learning_rate: 1.0000e-04\nEpoch 78/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8496 - loss: 0.4117\nEpoch 78: val_accuracy did not improve from 0.85663\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8482 - loss: 0.4123 - val_accuracy: 0.8530 - val_loss: 0.3761 - learning_rate: 1.0000e-04\nEpoch 79/300\n\u001b[1m89/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8491 - loss: 0.3868\nEpoch 79: val_accuracy did not improve from 0.85663\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8487 - loss: 0.3891 - val_accuracy: 0.8495 - val_loss: 0.3791 - learning_rate: 1.0000e-04\nEpoch 80/300\n\u001b[1m88/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8266 - loss: 0.4354\nEpoch 80: val_accuracy did not improve from 0.85663\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8273 - loss: 0.4361 - val_accuracy: 0.8566 - val_loss: 0.3839 - learning_rate: 1.0000e-04\nEpoch 81/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8518 - loss: 0.4108\nEpoch 81: val_accuracy did not improve from 0.85663\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8519 - loss: 0.4096 - val_accuracy: 0.8530 - val_loss: 0.3812 - learning_rate: 1.0000e-04\nEpoch 82/300\n\u001b[1m89/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8107 - loss: 0.4166\nEpoch 82: val_accuracy did not improve from 0.85663\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8128 - loss: 0.4146 - val_accuracy: 0.8566 - val_loss: 0.3727 - learning_rate: 1.0000e-04\nEpoch 83/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8550 - loss: 0.3453\nEpoch 83: val_accuracy improved from 0.85663 to 0.86738, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8543 - loss: 0.3490 - val_accuracy: 0.8674 - val_loss: 0.3674 - learning_rate: 1.0000e-04\nEpoch 84/300\n\u001b[1m88/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8441 - loss: 0.4197\nEpoch 84: val_accuracy improved from 0.86738 to 0.87097, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8442 - loss: 0.4188 - val_accuracy: 0.8710 - val_loss: 0.3709 - learning_rate: 1.0000e-04\nEpoch 85/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8534 - loss: 0.3734\nEpoch 85: val_accuracy did not improve from 0.87097\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8544 - loss: 0.3725 - val_accuracy: 0.8674 - val_loss: 0.3609 - learning_rate: 1.0000e-04\nEpoch 86/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8381 - loss: 0.4113\nEpoch 86: val_accuracy did not improve from 0.87097\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8373 - loss: 0.4113 - val_accuracy: 0.8602 - val_loss: 0.3614 - learning_rate: 1.0000e-04\nEpoch 87/300\n\u001b[1m89/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8484 - loss: 0.3943\nEpoch 87: val_accuracy did not improve from 0.87097\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8483 - loss: 0.3936 - val_accuracy: 0.8602 - val_loss: 0.3627 - learning_rate: 1.0000e-04\nEpoch 88/300\n\u001b[1m98/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8435 - loss: 0.3906\nEpoch 88: val_accuracy did not improve from 0.87097\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8437 - loss: 0.3901 - val_accuracy: 0.8710 - val_loss: 0.3659 - learning_rate: 1.0000e-04\nEpoch 89/300\n\u001b[1m89/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8626 - loss: 0.3706\nEpoch 89: val_accuracy improved from 0.87097 to 0.87455, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8609 - loss: 0.3737 - val_accuracy: 0.8746 - val_loss: 0.3599 - learning_rate: 1.0000e-04\nEpoch 90/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8326 - loss: 0.3962\nEpoch 90: val_accuracy did not improve from 0.87455\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8337 - loss: 0.3948 - val_accuracy: 0.8674 - val_loss: 0.3601 - learning_rate: 1.0000e-04\nEpoch 91/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8454 - loss: 0.4021\nEpoch 91: val_accuracy did not improve from 0.87455\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8453 - loss: 0.4011 - val_accuracy: 0.8710 - val_loss: 0.3578 - learning_rate: 1.0000e-04\nEpoch 92/300\n\u001b[1m92/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8498 - loss: 0.3945\nEpoch 92: val_accuracy did not improve from 0.87455\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8503 - loss: 0.3947 - val_accuracy: 0.8674 - val_loss: 0.3628 - learning_rate: 1.0000e-04\nEpoch 93/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8437 - loss: 0.3754\nEpoch 93: val_accuracy did not improve from 0.87455\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8447 - loss: 0.3737 - val_accuracy: 0.8710 - val_loss: 0.3594 - learning_rate: 1.0000e-04\nEpoch 94/300\n\u001b[1m88/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8647 - loss: 0.3517\nEpoch 94: val_accuracy improved from 0.87455 to 0.87814, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8634 - loss: 0.3543 - val_accuracy: 0.8781 - val_loss: 0.3535 - learning_rate: 1.0000e-04\nEpoch 95/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8517 - loss: 0.3693\nEpoch 95: val_accuracy did not improve from 0.87814\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8520 - loss: 0.3702 - val_accuracy: 0.8638 - val_loss: 0.3607 - learning_rate: 1.0000e-04\nEpoch 96/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8463 - loss: 0.3941\nEpoch 96: val_accuracy did not improve from 0.87814\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8477 - loss: 0.3905 - val_accuracy: 0.8674 - val_loss: 0.3479 - learning_rate: 1.0000e-04\nEpoch 97/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8668 - loss: 0.3450\nEpoch 97: val_accuracy did not improve from 0.87814\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8662 - loss: 0.3459 - val_accuracy: 0.8710 - val_loss: 0.3518 - learning_rate: 1.0000e-04\nEpoch 98/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8626 - loss: 0.3566\nEpoch 98: val_accuracy did not improve from 0.87814\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8618 - loss: 0.3579 - val_accuracy: 0.8602 - val_loss: 0.3637 - learning_rate: 1.0000e-04\nEpoch 99/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8539 - loss: 0.3514\nEpoch 99: val_accuracy did not improve from 0.87814\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8536 - loss: 0.3516 - val_accuracy: 0.8710 - val_loss: 0.3542 - learning_rate: 1.0000e-04\nEpoch 100/300\n\u001b[1m89/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8487 - loss: 0.3728\nEpoch 100: val_accuracy did not improve from 0.87814\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8509 - loss: 0.3690 - val_accuracy: 0.8746 - val_loss: 0.3522 - learning_rate: 1.0000e-04\nEpoch 101/300\n\u001b[1m88/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8441 - loss: 0.3479\nEpoch 101: val_accuracy did not improve from 0.87814\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8452 - loss: 0.3482 - val_accuracy: 0.8746 - val_loss: 0.3403 - learning_rate: 1.0000e-04\nEpoch 102/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8780 - loss: 0.2956\nEpoch 102: val_accuracy improved from 0.87814 to 0.88172, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8763 - loss: 0.2996 - val_accuracy: 0.8817 - val_loss: 0.3396 - learning_rate: 1.0000e-04\nEpoch 103/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8632 - loss: 0.3360\nEpoch 103: val_accuracy did not improve from 0.88172\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8622 - loss: 0.3379 - val_accuracy: 0.8710 - val_loss: 0.3434 - learning_rate: 1.0000e-04\nEpoch 104/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8732 - loss: 0.3463\nEpoch 104: val_accuracy did not improve from 0.88172\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8736 - loss: 0.3456 - val_accuracy: 0.8674 - val_loss: 0.3522 - learning_rate: 1.0000e-04\nEpoch 105/300\n\u001b[1m89/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8668 - loss: 0.3567\nEpoch 105: val_accuracy did not improve from 0.88172\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8669 - loss: 0.3551 - val_accuracy: 0.8710 - val_loss: 0.3483 - learning_rate: 1.0000e-04\nEpoch 106/300\n\u001b[1m93/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8793 - loss: 0.2870\nEpoch 106: val_accuracy did not improve from 0.88172\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8789 - loss: 0.2895 - val_accuracy: 0.8781 - val_loss: 0.3415 - learning_rate: 1.0000e-04\nEpoch 107/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8670 - loss: 0.3074\nEpoch 107: val_accuracy did not improve from 0.88172\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8682 - loss: 0.3080 - val_accuracy: 0.8781 - val_loss: 0.3319 - learning_rate: 1.0000e-04\nEpoch 108/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8814 - loss: 0.3320\nEpoch 108: val_accuracy did not improve from 0.88172\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8794 - loss: 0.3337 - val_accuracy: 0.8781 - val_loss: 0.3319 - learning_rate: 1.0000e-04\nEpoch 109/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8817 - loss: 0.3361\nEpoch 109: val_accuracy did not improve from 0.88172\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8809 - loss: 0.3357 - val_accuracy: 0.8781 - val_loss: 0.3384 - learning_rate: 1.0000e-04\nEpoch 110/300\n\u001b[1m89/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8711 - loss: 0.3366\nEpoch 110: val_accuracy did not improve from 0.88172\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8701 - loss: 0.3384 - val_accuracy: 0.8638 - val_loss: 0.3504 - learning_rate: 1.0000e-04\nEpoch 111/300\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8800 - loss: 0.3332\nEpoch 111: val_accuracy did not improve from 0.88172\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8800 - loss: 0.3332 - val_accuracy: 0.8746 - val_loss: 0.3395 - learning_rate: 1.0000e-04\nEpoch 112/300\n\u001b[1m88/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8674 - loss: 0.3269\nEpoch 112: val_accuracy did not improve from 0.88172\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8673 - loss: 0.3268 - val_accuracy: 0.8746 - val_loss: 0.3392 - learning_rate: 1.0000e-04\nEpoch 113/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8833 - loss: 0.2919\nEpoch 113: val_accuracy did not improve from 0.88172\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8817 - loss: 0.2938 - val_accuracy: 0.8781 - val_loss: 0.3412 - learning_rate: 1.0000e-04\nEpoch 114/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8832 - loss: 0.3076\nEpoch 114: val_accuracy did not improve from 0.88172\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8824 - loss: 0.3087 - val_accuracy: 0.8746 - val_loss: 0.3353 - learning_rate: 1.0000e-04\nEpoch 115/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8900 - loss: 0.3135\nEpoch 115: val_accuracy did not improve from 0.88172\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8889 - loss: 0.3152 - val_accuracy: 0.8710 - val_loss: 0.3399 - learning_rate: 1.0000e-04\nEpoch 116/300\n\u001b[1m89/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8601 - loss: 0.3350\nEpoch 116: val_accuracy did not improve from 0.88172\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8607 - loss: 0.3351 - val_accuracy: 0.8781 - val_loss: 0.3303 - learning_rate: 1.0000e-04\nEpoch 117/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8809 - loss: 0.3032\nEpoch 117: val_accuracy did not improve from 0.88172\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8795 - loss: 0.3058 - val_accuracy: 0.8817 - val_loss: 0.3266 - learning_rate: 1.0000e-04\nEpoch 118/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8691 - loss: 0.3285\nEpoch 118: val_accuracy improved from 0.88172 to 0.89247, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8693 - loss: 0.3275 - val_accuracy: 0.8925 - val_loss: 0.3216 - learning_rate: 1.0000e-04\nEpoch 119/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8956 - loss: 0.2699\nEpoch 119: val_accuracy did not improve from 0.89247\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8952 - loss: 0.2711 - val_accuracy: 0.8889 - val_loss: 0.3188 - learning_rate: 1.0000e-04\nEpoch 120/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8876 - loss: 0.3245\nEpoch 120: val_accuracy did not improve from 0.89247\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8866 - loss: 0.3247 - val_accuracy: 0.8889 - val_loss: 0.3145 - learning_rate: 1.0000e-04\nEpoch 121/300\n\u001b[1m88/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8713 - loss: 0.2902\nEpoch 121: val_accuracy did not improve from 0.89247\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8718 - loss: 0.2923 - val_accuracy: 0.8817 - val_loss: 0.3220 - learning_rate: 1.0000e-04\nEpoch 122/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8870 - loss: 0.2872\nEpoch 122: val_accuracy did not improve from 0.89247\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8869 - loss: 0.2878 - val_accuracy: 0.8889 - val_loss: 0.3236 - learning_rate: 1.0000e-04\nEpoch 123/300\n\u001b[1m88/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8835 - loss: 0.2883\nEpoch 123: val_accuracy did not improve from 0.89247\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8837 - loss: 0.2874 - val_accuracy: 0.8889 - val_loss: 0.3272 - learning_rate: 1.0000e-04\nEpoch 124/300\n\u001b[1m87/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8701 - loss: 0.3193\nEpoch 124: val_accuracy improved from 0.89247 to 0.89606, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8707 - loss: 0.3181 - val_accuracy: 0.8961 - val_loss: 0.3150 - learning_rate: 1.0000e-04\nEpoch 125/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8929 - loss: 0.2921\nEpoch 125: val_accuracy did not improve from 0.89606\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8936 - loss: 0.2915 - val_accuracy: 0.8853 - val_loss: 0.3127 - learning_rate: 1.0000e-04\nEpoch 126/300\n\u001b[1m88/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8895 - loss: 0.2689\nEpoch 126: val_accuracy improved from 0.89606 to 0.89964, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8904 - loss: 0.2678 - val_accuracy: 0.8996 - val_loss: 0.3139 - learning_rate: 1.0000e-04\nEpoch 127/300\n\u001b[1m89/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9012 - loss: 0.2678\nEpoch 127: val_accuracy did not improve from 0.89964\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8998 - loss: 0.2706 - val_accuracy: 0.8817 - val_loss: 0.3254 - learning_rate: 1.0000e-04\nEpoch 128/300\n\u001b[1m87/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8886 - loss: 0.2711\nEpoch 128: val_accuracy did not improve from 0.89964\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8885 - loss: 0.2712 - val_accuracy: 0.8925 - val_loss: 0.3224 - learning_rate: 1.0000e-04\nEpoch 129/300\n\u001b[1m88/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8734 - loss: 0.3080\nEpoch 129: val_accuracy did not improve from 0.89964\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8742 - loss: 0.3052 - val_accuracy: 0.8889 - val_loss: 0.3178 - learning_rate: 1.0000e-04\nEpoch 130/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9035 - loss: 0.2655\nEpoch 130: val_accuracy did not improve from 0.89964\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9032 - loss: 0.2652 - val_accuracy: 0.8889 - val_loss: 0.3156 - learning_rate: 1.0000e-04\nEpoch 131/300\n\u001b[1m89/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8686 - loss: 0.3152\nEpoch 131: val_accuracy did not improve from 0.89964\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8701 - loss: 0.3127 - val_accuracy: 0.8889 - val_loss: 0.3103 - learning_rate: 1.0000e-04\nEpoch 132/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8900 - loss: 0.2907\nEpoch 132: val_accuracy did not improve from 0.89964\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8901 - loss: 0.2905 - val_accuracy: 0.8925 - val_loss: 0.3197 - learning_rate: 1.0000e-04\nEpoch 133/300\n\u001b[1m98/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9004 - loss: 0.2736\nEpoch 133: val_accuracy did not improve from 0.89964\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9002 - loss: 0.2737 - val_accuracy: 0.8961 - val_loss: 0.3124 - learning_rate: 1.0000e-04\nEpoch 134/300\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8844 - loss: 0.2945\nEpoch 134: val_accuracy improved from 0.89964 to 0.90323, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8844 - loss: 0.2943 - val_accuracy: 0.9032 - val_loss: 0.3070 - learning_rate: 1.0000e-04\nEpoch 135/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8700 - loss: 0.3355\nEpoch 135: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8700 - loss: 0.3343 - val_accuracy: 0.8961 - val_loss: 0.3084 - learning_rate: 1.0000e-04\nEpoch 136/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8916 - loss: 0.2803\nEpoch 136: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8926 - loss: 0.2768 - val_accuracy: 0.8889 - val_loss: 0.3124 - learning_rate: 1.0000e-04\nEpoch 137/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8782 - loss: 0.2997\nEpoch 137: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8792 - loss: 0.2965 - val_accuracy: 0.8996 - val_loss: 0.3197 - learning_rate: 1.0000e-04\nEpoch 138/300\n\u001b[1m92/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8880 - loss: 0.2617\nEpoch 138: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8885 - loss: 0.2613 - val_accuracy: 0.8817 - val_loss: 0.3119 - learning_rate: 1.0000e-04\nEpoch 139/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9041 - loss: 0.2572\nEpoch 139: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9028 - loss: 0.2592 - val_accuracy: 0.8925 - val_loss: 0.3042 - learning_rate: 1.0000e-04\nEpoch 140/300\n\u001b[1m88/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8842 - loss: 0.2780\nEpoch 140: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8852 - loss: 0.2776 - val_accuracy: 0.8889 - val_loss: 0.3118 - learning_rate: 1.0000e-04\nEpoch 141/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8841 - loss: 0.2892\nEpoch 141: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8849 - loss: 0.2882 - val_accuracy: 0.8889 - val_loss: 0.3000 - learning_rate: 1.0000e-04\nEpoch 142/300\n\u001b[1m95/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9016 - loss: 0.2323\nEpoch 142: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9011 - loss: 0.2334 - val_accuracy: 0.8889 - val_loss: 0.3001 - learning_rate: 1.0000e-04\nEpoch 143/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9034 - loss: 0.2530\nEpoch 143: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9030 - loss: 0.2539 - val_accuracy: 0.8961 - val_loss: 0.2889 - learning_rate: 1.0000e-04\nEpoch 144/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9009 - loss: 0.2528\nEpoch 144: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9017 - loss: 0.2512 - val_accuracy: 0.8996 - val_loss: 0.2917 - learning_rate: 1.0000e-04\nEpoch 145/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9024 - loss: 0.2476\nEpoch 145: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9023 - loss: 0.2487 - val_accuracy: 0.8925 - val_loss: 0.2949 - learning_rate: 1.0000e-04\nEpoch 146/300\n\u001b[1m92/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9015 - loss: 0.2375\nEpoch 146: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9008 - loss: 0.2403 - val_accuracy: 0.9032 - val_loss: 0.2851 - learning_rate: 1.0000e-04\nEpoch 147/300\n\u001b[1m92/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8969 - loss: 0.2615\nEpoch 147: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8979 - loss: 0.2595 - val_accuracy: 0.8996 - val_loss: 0.2888 - learning_rate: 1.0000e-04\nEpoch 148/300\n\u001b[1m92/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9090 - loss: 0.2342\nEpoch 148: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9092 - loss: 0.2339 - val_accuracy: 0.8961 - val_loss: 0.2901 - learning_rate: 1.0000e-04\nEpoch 149/300\n\u001b[1m92/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8914 - loss: 0.2809\nEpoch 149: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8916 - loss: 0.2804 - val_accuracy: 0.8961 - val_loss: 0.2969 - learning_rate: 1.0000e-04\nEpoch 150/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8686 - loss: 0.3154\nEpoch 150: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8699 - loss: 0.3139 - val_accuracy: 0.8925 - val_loss: 0.2812 - learning_rate: 1.0000e-04\nEpoch 151/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9052 - loss: 0.2262\nEpoch 151: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9054 - loss: 0.2270 - val_accuracy: 0.8889 - val_loss: 0.2928 - learning_rate: 1.0000e-04\nEpoch 152/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9184 - loss: 0.2166\nEpoch 152: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9173 - loss: 0.2191 - val_accuracy: 0.8889 - val_loss: 0.3011 - learning_rate: 1.0000e-04\nEpoch 153/300\n\u001b[1m87/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9235 - loss: 0.2236\nEpoch 153: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9225 - loss: 0.2231 - val_accuracy: 0.8996 - val_loss: 0.2980 - learning_rate: 1.0000e-04\nEpoch 154/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9173 - loss: 0.2284\nEpoch 154: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n\nEpoch 154: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9177 - loss: 0.2279 - val_accuracy: 0.8996 - val_loss: 0.2892 - learning_rate: 1.0000e-04\nEpoch 155/300\n\u001b[1m89/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8954 - loss: 0.2525\nEpoch 155: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8955 - loss: 0.2547 - val_accuracy: 0.8996 - val_loss: 0.2837 - learning_rate: 5.0000e-05\nEpoch 156/300\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8987 - loss: 0.2448\nEpoch 156: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8988 - loss: 0.2447 - val_accuracy: 0.8996 - val_loss: 0.2823 - learning_rate: 5.0000e-05\nEpoch 157/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9096 - loss: 0.2177\nEpoch 157: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9085 - loss: 0.2192 - val_accuracy: 0.9032 - val_loss: 0.2922 - learning_rate: 5.0000e-05\nEpoch 158/300\n\u001b[1m89/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8908 - loss: 0.2635\nEpoch 158: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8917 - loss: 0.2623 - val_accuracy: 0.9032 - val_loss: 0.2869 - learning_rate: 5.0000e-05\nEpoch 159/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9085 - loss: 0.2184\nEpoch 159: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9091 - loss: 0.2188 - val_accuracy: 0.9032 - val_loss: 0.2925 - learning_rate: 5.0000e-05\nEpoch 160/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9252 - loss: 0.1909\nEpoch 160: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9242 - loss: 0.1944 - val_accuracy: 0.9032 - val_loss: 0.2961 - learning_rate: 5.0000e-05\nEpoch 161/300\n\u001b[1m98/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9124 - loss: 0.2149\nEpoch 161: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9123 - loss: 0.2153 - val_accuracy: 0.8961 - val_loss: 0.2887 - learning_rate: 5.0000e-05\nEpoch 162/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9061 - loss: 0.2497\nEpoch 162: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9055 - loss: 0.2491 - val_accuracy: 0.8961 - val_loss: 0.2847 - learning_rate: 5.0000e-05\nEpoch 163/300\n\u001b[1m89/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9341 - loss: 0.1781\nEpoch 163: val_accuracy did not improve from 0.90323\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9335 - loss: 0.1789 - val_accuracy: 0.8961 - val_loss: 0.2803 - learning_rate: 5.0000e-05\nEpoch 164/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9036 - loss: 0.2523\nEpoch 164: val_accuracy improved from 0.90323 to 0.90681, saving model to fixed_super_advanced_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9039 - loss: 0.2515 - val_accuracy: 0.9068 - val_loss: 0.2823 - learning_rate: 5.0000e-05\nEpoch 165/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8957 - loss: 0.2529\nEpoch 165: val_accuracy did not improve from 0.90681\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8968 - loss: 0.2515 - val_accuracy: 0.8996 - val_loss: 0.2833 - learning_rate: 5.0000e-05\nEpoch 166/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9055 - loss: 0.2277\nEpoch 166: val_accuracy did not improve from 0.90681\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9063 - loss: 0.2259 - val_accuracy: 0.8925 - val_loss: 0.2867 - learning_rate: 5.0000e-05\nEpoch 167/300\n\u001b[1m88/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9183 - loss: 0.1995\nEpoch 167: val_accuracy did not improve from 0.90681\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9173 - loss: 0.2032 - val_accuracy: 0.8996 - val_loss: 0.2851 - learning_rate: 5.0000e-05\nEpoch 168/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8967 - loss: 0.2290\nEpoch 168: val_accuracy did not improve from 0.90681\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8976 - loss: 0.2283 - val_accuracy: 0.8961 - val_loss: 0.2910 - learning_rate: 5.0000e-05\nEpoch 169/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9142 - loss: 0.2033\nEpoch 169: val_accuracy did not improve from 0.90681\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9139 - loss: 0.2041 - val_accuracy: 0.8961 - val_loss: 0.2908 - learning_rate: 5.0000e-05\nEpoch 170/300\n\u001b[1m89/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9270 - loss: 0.1910\nEpoch 170: val_accuracy did not improve from 0.90681\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9266 - loss: 0.1925 - val_accuracy: 0.8961 - val_loss: 0.2840 - learning_rate: 5.0000e-05\nEpoch 171/300\n\u001b[1m89/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9307 - loss: 0.2057\nEpoch 171: val_accuracy did not improve from 0.90681\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9294 - loss: 0.2078 - val_accuracy: 0.8961 - val_loss: 0.2800 - learning_rate: 5.0000e-05\nEpoch 172/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9108 - loss: 0.2139\nEpoch 172: val_accuracy did not improve from 0.90681\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9104 - loss: 0.2149 - val_accuracy: 0.9068 - val_loss: 0.2802 - learning_rate: 5.0000e-05\nEpoch 173/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9266 - loss: 0.2069\nEpoch 173: val_accuracy did not improve from 0.90681\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9262 - loss: 0.2073 - val_accuracy: 0.8961 - val_loss: 0.2871 - learning_rate: 5.0000e-05\nEpoch 174/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9248 - loss: 0.1815\nEpoch 174: val_accuracy did not improve from 0.90681\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9242 - loss: 0.1835 - val_accuracy: 0.8961 - val_loss: 0.2843 - learning_rate: 5.0000e-05\nEpoch 175/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9018 - loss: 0.2513\nEpoch 175: val_accuracy did not improve from 0.90681\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9025 - loss: 0.2487 - val_accuracy: 0.8925 - val_loss: 0.2831 - learning_rate: 5.0000e-05\nEpoch 176/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9231 - loss: 0.2010\nEpoch 176: val_accuracy did not improve from 0.90681\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9229 - loss: 0.2013 - val_accuracy: 0.8996 - val_loss: 0.2856 - learning_rate: 5.0000e-05\nEpoch 177/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9261 - loss: 0.2098\nEpoch 177: val_accuracy did not improve from 0.90681\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9251 - loss: 0.2109 - val_accuracy: 0.8996 - val_loss: 0.2843 - learning_rate: 5.0000e-05\nEpoch 178/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8890 - loss: 0.2671\nEpoch 178: val_accuracy did not improve from 0.90681\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8900 - loss: 0.2652 - val_accuracy: 0.8961 - val_loss: 0.2788 - learning_rate: 5.0000e-05\nEpoch 179/300\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9216 - loss: 0.2125\nEpoch 179: val_accuracy did not improve from 0.90681\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9216 - loss: 0.2126 - val_accuracy: 0.8889 - val_loss: 0.2797 - learning_rate: 5.0000e-05\nEpoch 180/300\n\u001b[1m97/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9299 - loss: 0.1814\nEpoch 180: val_accuracy did not improve from 0.90681\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9300 - loss: 0.1817 - val_accuracy: 0.8925 - val_loss: 0.2849 - learning_rate: 5.0000e-05\nEpoch 181/300\n\u001b[1m88/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9143 - loss: 0.2017\nEpoch 181: val_accuracy did not improve from 0.90681\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9150 - loss: 0.2011 - val_accuracy: 0.8925 - val_loss: 0.2864 - learning_rate: 5.0000e-05\nEpoch 182/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9361 - loss: 0.1682\nEpoch 182: val_accuracy did not improve from 0.90681\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9347 - loss: 0.1725 - val_accuracy: 0.8925 - val_loss: 0.2819 - learning_rate: 5.0000e-05\nEpoch 183/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9307 - loss: 0.1553\nEpoch 183: val_accuracy did not improve from 0.90681\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9299 - loss: 0.1586 - val_accuracy: 0.8925 - val_loss: 0.2854 - learning_rate: 5.0000e-05\nEpoch 184/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9098 - loss: 0.1917\nEpoch 184: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n\nEpoch 184: val_accuracy did not improve from 0.90681\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9106 - loss: 0.1919 - val_accuracy: 0.8925 - val_loss: 0.2794 - learning_rate: 5.0000e-05\nEpoch 185/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9391 - loss: 0.1791\nEpoch 185: val_accuracy did not improve from 0.90681\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9380 - loss: 0.1805 - val_accuracy: 0.8925 - val_loss: 0.2790 - learning_rate: 2.5000e-05\nEpoch 186/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9336 - loss: 0.1802\nEpoch 186: val_accuracy did not improve from 0.90681\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9331 - loss: 0.1818 - val_accuracy: 0.8961 - val_loss: 0.2758 - learning_rate: 2.5000e-05\nEpoch 187/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9271 - loss: 0.1763\nEpoch 187: val_accuracy did not improve from 0.90681\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9269 - loss: 0.1773 - val_accuracy: 0.8925 - val_loss: 0.2782 - learning_rate: 2.5000e-05\nEpoch 188/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9159 - loss: 0.2299\nEpoch 188: val_accuracy did not improve from 0.90681\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9161 - loss: 0.2284 - val_accuracy: 0.9032 - val_loss: 0.2797 - learning_rate: 2.5000e-05\nEpoch 189/300\n\u001b[1m88/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9318 - loss: 0.1779\nEpoch 189: val_accuracy did not improve from 0.90681\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9310 - loss: 0.1797 - val_accuracy: 0.8925 - val_loss: 0.2822 - learning_rate: 2.5000e-05\nEpoch 190/300\n\u001b[1m89/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9287 - loss: 0.1758\nEpoch 190: val_accuracy did not improve from 0.90681\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9284 - loss: 0.1768 - val_accuracy: 0.8961 - val_loss: 0.2825 - learning_rate: 2.5000e-05\nEpoch 191/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9199 - loss: 0.2039\nEpoch 191: val_accuracy did not improve from 0.90681\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9199 - loss: 0.2033 - val_accuracy: 0.8996 - val_loss: 0.2796 - learning_rate: 2.5000e-05\nEpoch 192/300\n\u001b[1m89/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9171 - loss: 0.2192\nEpoch 192: val_accuracy did not improve from 0.90681\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9177 - loss: 0.2170 - val_accuracy: 0.8996 - val_loss: 0.2740 - learning_rate: 2.5000e-05\nEpoch 193/300\n\u001b[1m88/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9084 - loss: 0.2303\nEpoch 193: val_accuracy did not improve from 0.90681\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9101 - loss: 0.2251 - val_accuracy: 0.8925 - val_loss: 0.2759 - learning_rate: 2.5000e-05\nEpoch 194/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9345 - loss: 0.1648\nEpoch 194: val_accuracy did not improve from 0.90681\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9344 - loss: 0.1651 - val_accuracy: 0.8925 - val_loss: 0.2745 - learning_rate: 2.5000e-05\nEpoch 195/300\n\u001b[1m92/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9256 - loss: 0.1805\nEpoch 195: val_accuracy did not improve from 0.90681\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9263 - loss: 0.1804 - val_accuracy: 0.8925 - val_loss: 0.2753 - learning_rate: 2.5000e-05\nEpoch 196/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9293 - loss: 0.1832\nEpoch 196: val_accuracy did not improve from 0.90681\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9289 - loss: 0.1851 - val_accuracy: 0.8925 - val_loss: 0.2746 - learning_rate: 2.5000e-05\nEpoch 197/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9257 - loss: 0.1808\nEpoch 197: val_accuracy did not improve from 0.90681\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9250 - loss: 0.1820 - val_accuracy: 0.8925 - val_loss: 0.2770 - learning_rate: 2.5000e-05\nEpoch 198/300\n\u001b[1m88/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9116 - loss: 0.2436\nEpoch 198: val_accuracy did not improve from 0.90681\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9134 - loss: 0.2386 - val_accuracy: 0.8925 - val_loss: 0.2737 - learning_rate: 2.5000e-05\nEpoch 199/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9175 - loss: 0.1983\nEpoch 199: val_accuracy did not improve from 0.90681\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9182 - loss: 0.1972 - val_accuracy: 0.8925 - val_loss: 0.2723 - learning_rate: 2.5000e-05\nEpoch 200/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9452 - loss: 0.1554\nEpoch 200: val_accuracy did not improve from 0.90681\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9435 - loss: 0.1581 - val_accuracy: 0.8925 - val_loss: 0.2785 - learning_rate: 2.5000e-05\nEpoch 201/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9148 - loss: 0.2092\nEpoch 201: val_accuracy did not improve from 0.90681\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9149 - loss: 0.2099 - val_accuracy: 0.8853 - val_loss: 0.2774 - learning_rate: 2.5000e-05\nEpoch 202/300\n\u001b[1m91/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9130 - loss: 0.2394\nEpoch 202: val_accuracy did not improve from 0.90681\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9131 - loss: 0.2387 - val_accuracy: 0.8925 - val_loss: 0.2739 - learning_rate: 2.5000e-05\nEpoch 203/300\n\u001b[1m92/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9140 - loss: 0.2093\nEpoch 203: val_accuracy did not improve from 0.90681\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9146 - loss: 0.2082 - val_accuracy: 0.8925 - val_loss: 0.2742 - learning_rate: 2.5000e-05\nEpoch 204/300\n\u001b[1m90/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9109 - loss: 0.2093\nEpoch 204: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n\nEpoch 204: val_accuracy did not improve from 0.90681\n\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9118 - loss: 0.2084 - val_accuracy: 0.8925 - val_loss: 0.2784 - learning_rate: 2.5000e-05\nEpoch 204: early stopping\nRestoring model weights from the end of the best epoch: 164.\n\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step\n\nğŸ¯ Validation Accuracy: 0.9068\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1200x500 with 2 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"# ======================================\n# Prepare test tabular features\n# ======================================\n\n# 1ï¸âƒ£ Select tabular features from test_df\ntab_features = ['age', 'gender', 'tbContactHistory', 'wheezingHistory', \n                'phlegmCough', 'familyAsthmaHistory', 'feverHistory', \n                'coldPresent', 'packYears']\n\nX_tab_test = test_df[tab_features].values\n\n# 2ï¸âƒ£ Apply the same Iterative Imputer as training\nX_tab_test_imputed = iterative_imputer.transform(X_tab_test)\n\n# 3ï¸âƒ£ Apply the same StandardScaler as training\nX_tab_test_scaled = scaler_tab.transform(X_tab_test_imputed)\n\n# 4ï¸âƒ£ Reshape test audio features\nX_audio_test_fixed = np.array(X_audio_test)  # Ensure it's np.array\nX_audio_test_flat = X_audio_test_fixed.reshape(X_audio_test_fixed.shape[0], -1)\n\n# 5ï¸âƒ£ Predict\nsuper_pred_test = super_model.predict([X_tab_test_scaled, X_audio_test_flat], verbose=1)\ny_pred_test = np.argmax(super_pred_test, axis=1)\n\n# 6ï¸âƒ£ Display results\ntest_results_df = pd.DataFrame({\n    'candidateID': test_df['candidateID'],\n    'predicted_disease': y_pred_test\n})\n\nprint(\"ğŸ¯ Sample predictions on test data:\")\nprint(test_results_df.head(10))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T11:30:18.848548Z","iopub.execute_input":"2025-12-07T11:30:18.848810Z","iopub.status.idle":"2025-12-07T11:30:19.223711Z","shell.execute_reply.started":"2025-12-07T11:30:18.848791Z","shell.execute_reply":"2025-12-07T11:30:19.222864Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\nğŸ¯ Sample predictions on test data:\n     candidateID  predicted_disease\n0  136bac9a3e081                  0\n1  b121e45942a46                  1\n2  6b6853c07e4fb                  2\n3  71de185eac888                  2\n4  25deed742f133                  1\n5  1de4591779d31                  0\n6  102efeabb10a5                  1\n7  522d1f8600a13                  2\n8  e41530046a74e                  1\n9  6337b96a160eb                  2\n","output_type":"stream"}],"execution_count":28}]}