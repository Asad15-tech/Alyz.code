{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3780d29d",
   "metadata": {},
   "source": [
    "# Regression Algorithms\n",
    "\n",
    "## 1. Linear Regression\n",
    "\n",
    "### Kam\n",
    "Linear Regression ek simple model hai jo dependent aur independent variables ke darmiyan linear relationship establish karta hai.\n",
    "\n",
    "### Parameters\n",
    "- **fit_intercept**: Yeh batata hai ki kya intercept ko fit karna hai ya nahi.  \n",
    "  **Example**: Agar fit_intercept = True hai, to model intercept ko shamil karega. ğŸ“\n",
    "  \n",
    "- **normalize**: Input data ko normalize karne ka option.  \n",
    "  **Example**: Agar normalize = True hai, to data ko scale kiya jayega. âš–ï¸\n",
    "  \n",
    "- **n_jobs**: Parallel jobs ki ginti jo computation ke liye istemal hoti hai.  \n",
    "  **Example**: Agar n_jobs = -1 hai, to saare cores ka istemal hota hai. ğŸ–¥ï¸\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Ridge Regression\n",
    "\n",
    "### Kam\n",
    "Ridge Regression ek regularization technique hai jo linear regression ke saath overfitting ko reduce karta hai.\n",
    "\n",
    "### Parameters\n",
    "- **alpha**: Regularization strength.  \n",
    "  **Example**: Agar alpha = 1 hai, to model ko regularization apply hota hai. ğŸ›¡ï¸\n",
    "  \n",
    "- **fit_intercept**: Yeh batata hai ki kya intercept ko fit karna hai ya nahi.  \n",
    "  **Example**: fit_intercept = True se model intercept shamil karega. ğŸ“\n",
    "  \n",
    "- **solver**: Optimization algorithm ka naam.  \n",
    "  **Example**: 'auto' se best solver ka selection hota hai. âš™ï¸\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Lasso Regression\n",
    "\n",
    "### Kam\n",
    "Lasso Regression bhi ek regularization technique hai jo variables ko select karne mein madad karta hai.\n",
    "\n",
    "### Parameters\n",
    "- **alpha**: Regularization strength.  \n",
    "  **Example**: Agar alpha = 0.1 hai, to model ko thoda regularization milega. ğŸ”\n",
    "  \n",
    "- **fit_intercept**: Yeh batata hai ki kya intercept ko fit karna hai ya nahi.  \n",
    "  **Example**: fit_intercept = True se model intercept ko shamil karega. ğŸ“\n",
    "  \n",
    "- **max_iter**: Iterations ki maximum ginti.  \n",
    "  **Example**: max_iter = 1000 se model 1000 iterations tak chalega. ğŸ”„\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Polynomial Regression\n",
    "\n",
    "### Kam\n",
    "Polynomial Regression linear regression ka extension hai jo nonlinear relationships ko model karta hai.\n",
    "\n",
    "### Parameters\n",
    "- **degree**: Polynomial ka degree.  \n",
    "  **Example**: Agar degree = 2 hai, to model quadratic relationship ko capture karega. ğŸ“ˆ\n",
    "  \n",
    "- **fit_intercept**: Yeh batata hai ki kya intercept ko fit karna hai ya nahi.  \n",
    "  **Example**: fit_intercept = True se model intercept shamil karega. ğŸ“\n",
    "  \n",
    "- **include_bias**: Bias term ko include karne ka option.  \n",
    "  **Example**: include_bias = True se bias term shamil hota hai. ğŸ”¢\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Support Vector Regression (SVR)\n",
    "\n",
    "### Kam\n",
    "SVR ek powerful technique hai jo support vector machines ka istemal karke regression problems ko solve karta hai.\n",
    "\n",
    "### Parameters\n",
    "- **C**: Regularization parameter.  \n",
    "  **Example**: C = 1 se model ki flexibility zyada hoti hai. âš–ï¸\n",
    "  \n",
    "- **epsilon**: Epsilon-insensitive loss function ki range.  \n",
    "  **Example**: epsilon = 0.1 se model kuch errors ko ignore karega. ğŸ“‰\n",
    "  \n",
    "- **kernel**: Kernel function ka type (e.g., 'linear', 'rbf').  \n",
    "  **Example**: RBF kernel complex relationships ko capture karta hai. ğŸŒŒ\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Decision Tree Regressor\n",
    "\n",
    "### Kam\n",
    "Decision Tree Regressor simple rules ka istemal karke predictions karta hai, jo easily interpretable hota hai.\n",
    "\n",
    "### Parameters\n",
    "- **criterion**: Splitting criteria (e.g., 'mse' for mean squared error).  \n",
    "  **Example**: Agar criterion = 'mse' hai, to tree mean squared error ke basis par split hoga. ğŸ“Š\n",
    "  \n",
    "- **max_depth**: Tree ki maximum gehraai.  \n",
    "  **Example**: Agar max_depth = 5 hai, to tree sirf 5 levels tak ja sakta hai. ğŸŒ³\n",
    "  \n",
    "- **min_samples_split**: Ek node ko split karne ke liye minimum samples ki ginti.  \n",
    "  **Example**: Agar min_samples_split = 10 hai, to node tab tak nahi split hoga jab tak 10 samples nahi hain. ğŸ”\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Random Forest Regressor\n",
    "\n",
    "### Kam\n",
    "Random Forest multiple decision trees ka ensemble hai jo accuracy badhata hai aur overfitting ko kam karta hai.\n",
    "\n",
    "### Parameters\n",
    "- **n_estimators**: Total trees ki ginti.  \n",
    "  **Example**: 200 trees se model zyada robust hota hai. ğŸŒ²\n",
    "  \n",
    "- **max_features**: Har tree ke liye features ka selection.  \n",
    "  **Example**: Agar max_features = 'sqrt', to har tree sqrt(features) ka istemal karega. ğŸ§©\n",
    "  \n",
    "- **min_samples_split**: Ek node ko split karne ke liye minimum samples ki ginti.  \n",
    "  **Example**: Agar min_samples_split = 10 hai, to node tab tak nahi split hoga jab tak 10 samples nahi hain. ğŸ”\n",
    "\n",
    "---\n",
    "\n",
    "## 8. K-Neighbors Regressor\n",
    "\n",
    "### Kam\n",
    "K-Neighbors Regressor distance-based approach istemal karta hai, jahan neighbors ki averaging ke zariye prediction kiya jata hai.\n",
    "\n",
    "### Parameters\n",
    "- **n_neighbors**: Neighbors ki ginti jo prediction mein shamil hoti hai.  \n",
    "  **Example**: Agar n_neighbors = 5 hai, to model 5 sabse kareeb neighbors ki averaging karta hai. ğŸ‘¥\n",
    "  \n",
    "- **weights**: Weighting method (uniform ya distance).  \n",
    "  **Example**: Distance weights se kareeb neighbors zyada impact rakhte hain. âš–ï¸\n",
    "  \n",
    "- **metric**: Distance ko measure karne ka tareeqa (e.g., 'euclidean').  \n",
    "  **Example**: Euclidean distance use karne se points ka distance seedha measure hota hai. ğŸ“\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Gradient Boosting Regressor\n",
    "\n",
    "### Kam\n",
    "Gradient Boosting sequentially trees ko build karta hai jo pehle se existing trees ke errors ko correct karte hain.\n",
    "\n",
    "### Parameters\n",
    "- **n_estimators**: Number of boosting stages.  \n",
    "  **Example**: Agar n_estimators = 100 hai, to 100 boosting stages honge. ğŸ”¢\n",
    "  \n",
    "- **learning_rate**: Model ki learning speed.  \n",
    "  **Example**: Learning rate 0.05 se model dheere seekhta hai. ğŸ¢\n",
    "  \n",
    "- **max_depth**: Har tree ki gehraai.  \n",
    "  **Example**: Agar max_depth = 3 hai, to tree ki gehraai 3 levels tak hoti hai. ğŸ“\n",
    "\n",
    "---\n",
    "\n",
    "## 10. XGBoost Regressor\n",
    "\n",
    "### Kam\n",
    "XGBoost ek powerful gradient boosting framework hai jo regression problems ko efficiently solve karta hai.\n",
    "\n",
    "### Parameters\n",
    "- **n_estimators**: Yeh number of trees batata hai jo model mein shamil honge.  \n",
    "  **Example**: Agar aap 100 trees ka istemal karte hain, to model zyada accurate hota hai. ğŸŒ³\n",
    "  \n",
    "- **learning_rate**: Yeh model ki learning speed ko control karta hai.  \n",
    "  **Example**: 0.1 learning rate se model dheere seekhta hai, lekin zyada samay leta hai. â³\n",
    "  \n",
    "- **max_depth**: Yeh har tree ki gehraai ko set karta hai.  \n",
    "  **Example**: Agar max_depth 3 hai, to tree sirf 3 levels tak ja sakta hai. ğŸ“\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1027dd0c",
   "metadata": {},
   "source": [
    "# Regression Metrics\n",
    "\n",
    "## 1. Mean Absolute Error (MAE)\n",
    "- **Definition**: Actual aur predicted values ke darmiyan absolute differences ka average.\n",
    "- **Formula**: \n",
    "  MAE = (1/n) * Î£ |y_i - Å·_i|\n",
    "\n",
    "## 2. Mean Squared Error (MSE)\n",
    "- **Definition**: Actual aur predicted values ke darmiyan squared differences ka average.\n",
    "- **Formula**: \n",
    "  MSE = (1/n) * Î£ (y_i - Å·_i)Â²\n",
    "\n",
    "## 3. Root Mean Squared Error (RMSE)\n",
    "- **Definition**: MSE ka square root, jo error ko same units mein dikhata hai.\n",
    "- **Formula**: \n",
    "  RMSE = âˆš(MSE)\n",
    "\n",
    "## 4. R-squared (RÂ²)\n",
    "- **Definition**: Model ki explanatory power ko dikhata hai; yeh batata hai ki model ne variance ka kitna hissa explain kiya.\n",
    "- **Formula**: \n",
    "  RÂ² = 1 - (SS_res / SS_tot)\n",
    "  - **SS_res**: Residual sum of squares\n",
    "  - **SS_tot**: Total sum of squares\n",
    "\n",
    "## 5. Adjusted R-squared\n",
    "- **Definition**: RÂ² ka modified version jo number of predictors ko consider karta hai; yeh model ki complexity ko penalize karta hai.\n",
    "- **Formula**: \n",
    "  Adjusted RÂ² = 1 - (1 - RÂ²) * (n - 1) / (n - p - 1)\n",
    "  - **n**: Number of observations\n",
    "  - **p**: Number of predictors\n",
    "\n",
    "## 6. Mean Absolute Percentage Error (MAPE)\n",
    "- **Definition**: Actual aur predicted values ke darmiyan percentage errors ka average.\n",
    "- **Formula**: \n",
    "  MAPE = (100/n) * Î£ |(y_i - Å·_i) / y_i|\n",
    "\n",
    "# **All the Best**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pandas_pratice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
