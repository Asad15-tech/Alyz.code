{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3488f8bf",
   "metadata": {},
   "source": [
    "# â¤ï¸ Heart Disease Prediction System using Machine Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b973f05",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "# ðŸ“„ Project Metadata\n",
    "### **Title:** **Heart Disease Prediction using Machine Learning** â¤ï¸â€ðŸ©¹\n",
    "### **Author:** **Asad Ali** âœï¸\n",
    "### **Institute:** **University of Okara** ðŸŽ“\n",
    "### **Email:** ðŸ“§ [asadalyy834@gmail.com](mailto:asadalyy834@gmail.com)\n",
    "### **Course:** **Data Science / Machine Learning** ðŸ“ŠðŸ§ \n",
    "### **Date:** **July 2025** ðŸ“…\n",
    "### **Version:** **1.0** ðŸ”¢\n",
    "### **Language:** **Python** ðŸ\n",
    "### **Libraries Overview:**\n",
    "- **Pandas** ðŸ“š: Data manipulation and analysis\n",
    "- **NumPy** ðŸ”¢: Numerical computing\n",
    "- **Scikit-learn** âš™ï¸: Machine learning tools and algorithms\n",
    "- **Matplotlib** ðŸ“ˆ: Data visualization\n",
    "- **Seaborn** ðŸŒŠ: Statistical data visualization\n",
    "- **Plotly** ðŸ“Š: Interactive data visualization\n",
    "### **Dataset:** **UCI Heart Disease Dataset from Kaggle** ðŸ“‹\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e606f443",
   "metadata": {},
   "source": [
    "# ðŸ“Š About the Dataset\n",
    "\n",
    "### ðŸ§¬ Context\n",
    "\n",
    "This is a **multivariate dataset** â€” it contains multiple statistical variables and supports numerical data analysis. Although the full dataset includes **76 attributes**, most published research focuses on **14 key features**.\n",
    "\n",
    "- ðŸ“ **Primary Source Used:** *Cleveland database* â€” the most commonly used by machine learning researchers.\n",
    "- ðŸ§  **Main Objective:** Predict whether a patient has heart disease or not based on medical parameters.\n",
    "- ðŸ”Ž **Secondary Objective:** Gain diagnostic insights through statistical and machine learning exploration.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Selected Attribute Descriptions (14 Core Features)\n",
    "\n",
    "| ðŸ”¢ No. | ðŸ§¬ Column Name | ðŸ“– Description                                                                 |\n",
    "|-------:|---------------|--------------------------------------------------------------------------------|\n",
    "| 1ï¸âƒ£    | `age`         | Age of the patient (in years)                                                  |\n",
    "| 2ï¸âƒ£    | `sex`         | Gender of patient (`0` = Female, `1` = Male)                                   |\n",
    "| 3ï¸âƒ£    | `cp`          | Chest pain type: `typical angina`, `atypical angina`, `non-anginal`, `asymptomatic` |\n",
    "| 4ï¸âƒ£    | `trestbps`    | Resting blood pressure (in mm Hg at admission)                                 |\n",
    "| 5ï¸âƒ£    | `chol`        | Serum cholesterol level (in mg/dl)                                             |\n",
    "| 6ï¸âƒ£    | `fbs`         | Fasting blood sugar > 120 mg/dl (`1` = True; `0` = False)                       |\n",
    "| 7ï¸âƒ£    | `restecg`     | ECG results: `normal`, `ST-T abnormality`, `left ventricular hypertrophy`      |\n",
    "| 8ï¸âƒ£    | `thalach`     | Maximum heart rate achieved                                                    |\n",
    "| 9ï¸âƒ£    | `exang`       | Exercise-induced angina (`1` = Yes; `0` = No)                                   |\n",
    "| ðŸ”Ÿ     | `oldpeak`     | ST depression induced by exercise relative to rest                             |\n",
    "| 1ï¸âƒ£1ï¸âƒ£ | `slope`       | Slope of the peak exercise ST segment                                          |\n",
    "| 1ï¸âƒ£2ï¸âƒ£ | `ca`          | Number of major vessels (0â€“3) colored by fluoroscopy                           |\n",
    "| 1ï¸âƒ£3ï¸âƒ£ | `thal`        | Thalassemia condition: `normal`, `fixed defect`, `reversible defect`           |\n",
    "| 1ï¸âƒ£4ï¸âƒ£ | `target/num`  | Predicted attribute (0 = No Disease, 1 = Heart Disease)                         |\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§¾ Additional Columns (May Appear in Extended Datasets)\n",
    "\n",
    "| ðŸ”¹ Column        | ðŸ” Description                              |\n",
    "|------------------|--------------------------------------------|\n",
    "| `id`             | Unique ID for each patient                 |\n",
    "| `origin`         | Source location of data (e.g., Hungary)   |\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ‘¨â€âš•ï¸ Acknowledgements\n",
    "\n",
    "**Contributors & Medical Institutions:**\n",
    "\n",
    "- ðŸ¥ *Hungarian Institute of Cardiology, Budapest*: **Dr. Andras Janosi**  \n",
    "- ðŸ¥ *University Hospital, Zurich, Switzerland*: **Dr. William Steinbrunn**  \n",
    "- ðŸ¥ *University Hospital, Basel, Switzerland*: **Dr. Matthias Pfisterer**  \n",
    "- ðŸ¥ *V.A. Medical Center, Long Beach & Cleveland Clinic*: **Dr. Robert Detrano**\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“š Relevant Research Papers\n",
    "\n",
    "- ðŸ“„ *International application of a new probability algorithm for the diagnosis of coronary artery disease*  \n",
    "  âž¤ *Detrano, R. et al., American Journal of Cardiology, 1989*\n",
    "\n",
    "- ðŸ“„ *Instance-based prediction of heart-disease presence with the Cleveland database*  \n",
    "  âž¤ *David W. Aha & Dennis Kibler*\n",
    "\n",
    "- ðŸ“„ *Models of incremental concept formation*  \n",
    "  âž¤ *Gennari, J.H., Langley, P., & Fisher, D., Artificial Intelligence, 1989*\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ™ Citation Request\n",
    "\n",
    "> The authors request that any publication using this dataset must credit the principal investigators:\n",
    "> \n",
    "> - **Dr. Andras Janosi** â€“ Hungarian Institute of Cardiology  \n",
    "> - **Dr. William Steinbrunn** â€“ University Hospital, Zurich  \n",
    "> - **Dr. Matthias Pfisterer** â€“ University Hospital, Basel  \n",
    "> - **Dr. Robert Detrano** â€“ Cleveland Clinic Foundation & Long Beach VA Medical Center\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139a4529",
   "metadata": {},
   "source": [
    "# 1. ðŸ“š Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c18e3270",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "72f78558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting to Display max rows and columns\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "75ead46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  age     sex    dataset               cp  trestbps   chol    fbs         restecg  thalch  exang  oldpeak        slope   ca               thal  num\n",
      "0   1   63    Male  Cleveland   typical angina     145.0  233.0   True  lv hypertrophy   150.0  False      2.3  downsloping  0.0       fixed defect    0\n",
      "1   2   67    Male  Cleveland     asymptomatic     160.0  286.0  False  lv hypertrophy   108.0   True      1.5         flat  3.0             normal    2\n",
      "2   3   67    Male  Cleveland     asymptomatic     120.0  229.0  False  lv hypertrophy   129.0   True      2.6         flat  2.0  reversable defect    1\n",
      "3   4   37    Male  Cleveland      non-anginal     130.0  250.0  False          normal   187.0  False      3.5  downsloping  0.0             normal    0\n",
      "4   5   41  Female  Cleveland  atypical angina     130.0  204.0  False  lv hypertrophy   172.0  False      1.4    upsloping  0.0             normal    0\n"
     ]
    }
   ],
   "source": [
    "# Let's Load the Dataset that is in our local directory\n",
    "df = pd.read_csv(\"heart_disease_uci.csv\")\n",
    "# Let's have a look at the first few rows of the dataset\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c4ce08e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information about the Dataset\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 920 entries, 0 to 919\n",
      "Data columns (total 16 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   id        920 non-null    int64  \n",
      " 1   age       920 non-null    int64  \n",
      " 2   sex       920 non-null    object \n",
      " 3   dataset   920 non-null    object \n",
      " 4   cp        920 non-null    object \n",
      " 5   trestbps  861 non-null    float64\n",
      " 6   chol      890 non-null    float64\n",
      " 7   fbs       830 non-null    object \n",
      " 8   restecg   918 non-null    object \n",
      " 9   thalch    865 non-null    float64\n",
      " 10  exang     865 non-null    object \n",
      " 11  oldpeak   858 non-null    float64\n",
      " 12  slope     611 non-null    object \n",
      " 13  ca        309 non-null    float64\n",
      " 14  thal      434 non-null    object \n",
      " 15  num       920 non-null    int64  \n",
      "dtypes: float64(5), int64(3), object(8)\n",
      "memory usage: 115.1+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Getting the info of our Dataset\n",
    "print(\"Information about the Dataset\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6d00e86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the Dataset\n",
      "The Dataset has 920 rows and 16 columns.\n"
     ]
    }
   ],
   "source": [
    "# Check the Shape of the Dataset\n",
    "print(\"Shape of the Dataset\")\n",
    "print(\"The Dataset has\", df.shape[0], \"rows and\", df.shape[1], \"columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffed8e7a",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d0d919",
   "metadata": {},
   "source": [
    "# 2. Data Preprocessing ðŸ”\n",
    "## 1. **Handling Missing Values:** Fill or drop missing data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "57f39fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values in the Dataset:\n",
      "ca          611\n",
      "thal        486\n",
      "slope       309\n",
      "fbs          90\n",
      "oldpeak      62\n",
      "trestbps     59\n",
      "exang        55\n",
      "thalch       55\n",
      "chol         30\n",
      "restecg       2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Let's First check for missing values in our dataset\n",
    "missing_values = df.isnull().sum().sort_values(ascending=False)\n",
    "print(\"Missing Values in the Dataset:\")\n",
    "print(missing_values[missing_values > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7aeffc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ca          66.413043\n",
      "thal        52.826087\n",
      "slope       33.586957\n",
      "fbs          9.782609\n",
      "oldpeak      6.739130\n",
      "trestbps     6.413043\n",
      "exang        5.978261\n",
      "thalch       5.978261\n",
      "chol         3.260870\n",
      "restecg      0.217391\n",
      "cp           0.000000\n",
      "dataset      0.000000\n",
      "id           0.000000\n",
      "age          0.000000\n",
      "sex          0.000000\n",
      "num          0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Check the percentage of missing values in our dataset\n",
    "print((df.isnull().sum() / len(df) * 100).sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "82cf0a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Types of Columns with Missing Values:\n",
      "ca          float64\n",
      "thal         object\n",
      "slope        object\n",
      "fbs          object\n",
      "oldpeak     float64\n",
      "trestbps    float64\n",
      "exang        object\n",
      "thalch      float64\n",
      "chol        float64\n",
      "restecg      object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Let's check the type of each column that has missing values in our dataset.\n",
    "print(\"Data Types of Columns with Missing Values:\")\n",
    "print(df.dtypes[missing_values[missing_values > 0].index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6c527eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values in the Dataset after Imputation:\n",
      "thal       486\n",
      "slope      309\n",
      "fbs         90\n",
      "exang       55\n",
      "restecg      2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Let's impute missing values that have int or float data types using KNN Imputer\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df.select_dtypes(include=[np.number])), columns=df.select_dtypes(include=[np.number]).columns)\n",
    "# Replacing the original numeric columns with the imputed ones\n",
    "df[df.select_dtypes(include=[np.number]).columns] = df_imputed\n",
    "# Let's again have a look at the missing values after imputation of numeric columns\n",
    "missing_values_after_imputation = df.isnull().sum().sort_values(ascending=False)\n",
    "print(\"Missing Values in the Dataset after Imputation:\")\n",
    "print(missing_values_after_imputation[missing_values_after_imputation > 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "bb99cab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Types of Columns with Missing Values after Imputation:\n",
      "thal       object\n",
      "slope      object\n",
      "fbs        object\n",
      "exang      object\n",
      "restecg    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Let's again check the datatype of each column that contain missing values after imputation of numeric columns\n",
    "print(\"Data Types of Columns with Missing Values after Imputation:\")\n",
    "print(df.dtypes[missing_values_after_imputation[missing_values_after_imputation > 0].index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654868c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values in the Dataset after Encoding and RF Imputation:\n",
      "id          0\n",
      "age         0\n",
      "sex         0\n",
      "dataset     0\n",
      "cp          0\n",
      "trestbps    0\n",
      "chol        0\n",
      "fbs         0\n",
      "restecg     0\n",
      "thalch      0\n",
      "exang       0\n",
      "oldpeak     0\n",
      "slope       0\n",
      "ca          0\n",
      "thal        0\n",
      "num         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Encode , Impute and Decode using ML Model\n",
    "def encode_and_rf_impute(df):\n",
    "    df_copy = df.copy()\n",
    "    # Encode object columns\n",
    "    label_encoders = {}\n",
    "    object_cols = df_copy.select_dtypes(include=['object']).columns\n",
    "    for col in object_cols:\n",
    "        le = LabelEncoder()\n",
    "        df_copy[col] = le.fit_transform(df_copy[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "\n",
    "    # Impute object columns using RandomForestClassifier\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    for col in object_cols:\n",
    "        missing = df[col].isnull()\n",
    "        if missing.any():\n",
    "            not_missing = ~missing\n",
    "            X_train = df_copy.loc[not_missing].drop(columns=[col])\n",
    "            y_train = df_copy.loc[not_missing, col]\n",
    "            X_pred = df_copy.loc[missing].drop(columns=[col])\n",
    "            clf = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "            clf.fit(X_train, y_train)\n",
    "            df_copy.loc[missing, col] = clf.predict(X_pred)\n",
    "\n",
    "    # Decode object columns back to original values\n",
    "    for col, le in label_encoders.items():\n",
    "        df_copy[col] = le.inverse_transform(df_copy[col].round().astype(int))\n",
    "\n",
    "    return df_copy\n",
    "\n",
    "# Apply the function using RandomForestClassifier for categorical columns\n",
    "df  = encode_and_rf_impute(df)\n",
    "print(\"Missing Values in the Dataset after Encoding and RF Imputation:\")\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4e0a6319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Rows of the Dataset after Encoding and Imputation:\n",
      "        id   age     sex      dataset               cp  trestbps   chol    fbs           restecg  thalch  exang  oldpeak        slope   ca               thal  num\n",
      "705  706.0  65.0    Male  Switzerland     asymptomatic     145.0    0.0  False  st-t abnormality    67.0  False      1.3         flat  1.4       fixed defect  3.0\n",
      "75    76.0  65.0  Female    Cleveland      non-anginal     160.0  360.0  False    lv hypertrophy   151.0  False      0.8    upsloping  0.0             normal  0.0\n",
      "9     10.0  53.0    Male    Cleveland     asymptomatic     140.0  203.0   True    lv hypertrophy   155.0   True      3.1  downsloping  0.0  reversable defect  1.0\n",
      "373  374.0  44.0    Male      Hungary     asymptomatic     150.0  412.0  False            normal   170.0  False      0.0    upsloping  1.2             normal  0.0\n",
      "208  209.0  55.0    Male    Cleveland  atypical angina     130.0  262.0  False            normal   155.0  False      0.0    upsloping  0.0             normal  0.0\n"
     ]
    }
   ],
   "source": [
    "# Let's again check the few random rows of the dataset after encoding and imputation\n",
    "print(\"Random Rows of the Dataset after Encoding and Imputation:\")\n",
    "print(df.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "60cae588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 920 entries, 0 to 919\n",
      "Data columns (total 16 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   id        920 non-null    float64\n",
      " 1   age       920 non-null    float64\n",
      " 2   sex       920 non-null    object \n",
      " 3   dataset   920 non-null    object \n",
      " 4   cp        920 non-null    object \n",
      " 5   trestbps  920 non-null    float64\n",
      " 6   chol      920 non-null    float64\n",
      " 7   fbs       920 non-null    object \n",
      " 8   restecg   920 non-null    object \n",
      " 9   thalch    920 non-null    float64\n",
      " 10  exang     920 non-null    object \n",
      " 11  oldpeak   920 non-null    float64\n",
      " 12  slope     920 non-null    object \n",
      " 13  ca        920 non-null    float64\n",
      " 14  thal      920 non-null    object \n",
      " 15  num       920 non-null    float64\n",
      "dtypes: float64(8), object(8)\n",
      "memory usage: 115.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# Getting the info of our dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad90c93c",
   "metadata": {},
   "source": [
    "- ###  Missing values ka rola howa khtm ðŸ˜Ž "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b9ae0e",
   "metadata": {},
   "source": [
    "## 2. **Cleaning:** Remove duplicates and handle outliers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pandas_pratice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
