{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2231927,"sourceType":"datasetVersion","datasetId":1340873}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"```markdown\n# üìå RNN & LSTM Explanation\n\n## üî∑ **RNN (Recurrent Neural Network) ‚Äî Simple Definition**\nRNN is a type of neural network that is specially designed for **sequence data** such as text, audio, or time-series.  \nIt processes data **one step at a time** and uses its previous output as memory, allowing it to learn **short-term patterns** in sequential information.\n\n---\n\n## üî∑ **LSTM (Long Short-Term Memory) ‚Äî Complete Explanation**\n\nLSTM is an advanced and improved version of RNN created to overcome the problem of **forgetting long-term information**.\n\n### ‚úÖ **Why LSTM?**\nBasic RNNs cannot remember important information for long sequences (vanishing gradient problem).  \nLSTM solves this by using a **memory cell** that can keep information for many time steps.\n\n---\n\n## üî∑ **How LSTM Works**\nLSTM has a special structure with **three gates** that control the flow of information:\n\n### 1Ô∏è‚É£ **Forget Gate**\nDecides which part of the past information should be removed from the memory.\n\n### 2Ô∏è‚É£ **Input Gate**\nDecides what new information should be added to the memory.\n\n### 3Ô∏è‚É£ **Output Gate**\nControls what part of the memory should be sent to the next layer.\n\nBecause of these gates, LSTM is able to:\n- Remember long-term dependencies  \n- Forget unnecessary information  \n- Learn complex patterns in sentences  \n\n---\n\n## üî∑ **Advantages of LSTM**\n- Remembers long sequences  \n- Avoids vanishing gradient problem  \n- Works very well with NLP and time-series  \n- Learns contextual meaning between words  \n\n---\n\n## üî∑ **Where LSTM Is Used**\n- Sentiment analysis  \n- Text classification  \n- Next-word prediction  \n- Chatbots  \n- Speech recognition  \n- Time-series forecasting  \n\n---\n\n## üî∑ **Summary**\nLSTM is powerful because it understands **order**, **context**, and **long-term relationships** in text and other sequence data, making it ideal for modern NLP tasks.\n```\n","metadata":{"execution":{"iopub.status.busy":"2025-12-04T08:39:30.172972Z","iopub.execute_input":"2025-12-04T08:39:30.173165Z","iopub.status.idle":"2025-12-04T08:39:30.183539Z","shell.execute_reply.started":"2025-12-04T08:39:30.173147Z","shell.execute_reply":"2025-12-04T08:39:30.182445Z"}}},{"cell_type":"code","source":"# ================================\n# üìå IMPORTING ALL REQUIRED LIBRARIES\n# ================================\n\n# --- 1. Warnings ---\nimport warnings                         # To suppress unwanted warning messages\nwarnings.filterwarnings(\"ignore\")       # Disables all warnings (optional)\n\n# --- 2. Basic Data Handling & Processing ---\nimport numpy as np                      # For numerical operations (arrays, math functions)\nimport pandas as pd                     # For loading, cleaning & analyzing datasets\n\n# --- 3. Data Visualization ---\nimport matplotlib.pyplot as plt         # For creating plots, charts, and visualizations\nimport seaborn as sns                   # For advanced and beautiful statistical plots\nfrom wordcloud import WordCloud         # For generating word cloud images\n\n# --- 4. Text Preprocessing (Cleaning Raw Text) ---\nimport re                                # For regex operations (remove URLs, mentions, etc.)\nimport string                            # For removing punctuation marks\nfrom bs4 import BeautifulSoup            # For removing HTML tags from text\n\n# --- 5. NLP Utilities (Tokenization, Lemmatization, Stopwords etc.) ---\nimport nltk                               # Natural Language Processing library\nfrom nltk.corpus import stopwords         # Contains a list of common English stopwords\nfrom nltk.stem import WordNetLemmatizer   # Converts words to their root form (lemmatization)\nfrom nltk.tokenize import word_tokenize   # Splits sentences into individual words\n\n# NLTK Resource Downloads (Uncomment if running first time)\n# nltk.download(\"punkt\")                  \n# nltk.download(\"stopwords\")\n# nltk.download(\"wordnet\")\n\n# --- 6. Machine Learning (Sklearn) ---\nfrom sklearn.model_selection import train_test_split   # To split dataset into train/test parts\nfrom sklearn.preprocessing import LabelEncoder         # Converts text labels into numeric labels\nfrom sklearn.metrics import classification_report      # For detailed model performance report\n\n# --- 7. Deep Learning (TensorFlow / Keras) ---\nimport tensorflow as tf                                # TensorFlow backend framework\n\n# Text processing for neural networks\nfrom tensorflow.keras.preprocessing.text import Tokenizer        # Converts text to sequences\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences # Pads sequences to equal length\n\n# Model building\nfrom tensorflow.keras.models import Sequential                    # Sequential neural network model\n\n# Neural network layers\nfrom tensorflow.keras.layers import (Embedding, LSTM, Dense,\n                                     Dropout, Bidirectional,\n                                     BatchNormalization)          # LSTM & additional layers\n\n# Regularization, optimizer & training improvement\nfrom tensorflow.keras.regularizers import l2                      # L2 regularization to reduce overfitting\nfrom tensorflow.keras.optimizers import Adam                      # Optimizer for training\nfrom tensorflow.keras.callbacks import EarlyStopping              # Stops training early to prevent overfitting\n\n\nprint(\"Libraries Imported Successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T08:42:38.551561Z","iopub.execute_input":"2025-12-04T08:42:38.551864Z","iopub.status.idle":"2025-12-04T08:42:38.559958Z","shell.execute_reply.started":"2025-12-04T08:42:38.551846Z","shell.execute_reply":"2025-12-04T08:42:38.559035Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import NLTK tools\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\n\n# Download necessary NLTK resources\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T08:42:38.560985Z","iopub.execute_input":"2025-12-04T08:42:38.561313Z","iopub.status.idle":"2025-12-04T08:42:38.583998Z","shell.execute_reply.started":"2025-12-04T08:42:38.561295Z","shell.execute_reply":"2025-12-04T08:42:38.583241Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================\n# üìå STEP 1 ‚Äî LOAD DATASET + BASIC EDA\n# ============================================\n\n# Load dataset (replace filename if needed)\ndf = pd.read_csv(\"/kaggle/input/twitter-sentiment-dataset/Twitter_Data.csv\")\n\n# Show the first 5 rows\nprint(\"üìå Displaying first 5 rows of dataset:\\n\")\ndisplay(df.head())\n\n# Check dataset shape (rows, columns)\nprint(\"\\nüìå Dataset Shape (rows, columns):\")\nprint(df.shape)\n\n# Check column names\nprint(\"\\nüìå Dataset Columns:\")\nprint(df.columns)\n\n# Check for missing values\nprint(\"\\nüìå Missing values in each column:\")\nprint(df.isnull().sum())\n\n# View class distribution (Sentiment counts)\nprint(\"\\nüìå Sentiment label distribution:\")\nprint(df['category'].value_counts())\n\n# Plot sentiment distribution\nplt.figure(figsize=(6,4))\nsns.countplot(data=df, x=\"category\")\nplt.title(\"Sentiment Distribution\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T08:42:38.584859Z","iopub.execute_input":"2025-12-04T08:42:38.585073Z","iopub.status.idle":"2025-12-04T08:42:39.297012Z","shell.execute_reply.started":"2025-12-04T08:42:38.585050Z","shell.execute_reply":"2025-12-04T08:42:39.296384Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Removing Missin values \n# =============================================\n# üìå STEP: Check missing values percentage\n# =============================================\n\n# Calculate missing values percentage for each column\nmissing_percent = df.isnull().sum() / len(df) * 100\nprint(\"üìå Missing Values Percentage in Each Column:\")\nprint(missing_percent)\n\n# Drop rows with any missing values\ndf = df.dropna().reset_index(drop=True)\nprint(\"\\nüìå Dataset shape after removing missing values:\", df.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T08:42:39.298356Z","iopub.execute_input":"2025-12-04T08:42:39.298749Z","iopub.status.idle":"2025-12-04T08:42:39.342818Z","shell.execute_reply.started":"2025-12-04T08:42:39.298730Z","shell.execute_reply":"2025-12-04T08:42:39.342189Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['text_length'] = df['clean_text'].apply(len)\nplt.figure(figsize=(8,4))\nsns.histplot(df['text_length'], bins=50)\nplt.title(\"Tweet Length Distribution\")\nplt.xlabel(\"Number of characters\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T08:42:39.343565Z","iopub.execute_input":"2025-12-04T08:42:39.343856Z","iopub.status.idle":"2025-12-04T08:42:39.658968Z","shell.execute_reply.started":"2025-12-04T08:42:39.343838Z","shell.execute_reply":"2025-12-04T08:42:39.658355Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['word_count'] = df['clean_text'].apply(lambda x: len(str(x).split()))\nplt.figure(figsize=(8,4))\nsns.histplot(df['word_count'], bins=50)\nplt.title(\"Words per Tweet Distribution\")\nplt.xlabel(\"Number of words\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T08:42:39.659644Z","iopub.execute_input":"2025-12-04T08:42:39.659938Z","iopub.status.idle":"2025-12-04T08:42:40.148171Z","shell.execute_reply.started":"2025-12-04T08:42:39.659920Z","shell.execute_reply":"2025-12-04T08:42:40.147524Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['category'].value_counts().plot(kind='pie', autopct='%1.1f%%', figsize=(5,5))\nplt.title(\"Sentiment Class Proportion\")\nplt.ylabel(\"\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T08:42:40.148900Z","iopub.execute_input":"2025-12-04T08:42:40.149127Z","iopub.status.idle":"2025-12-04T08:42:40.280710Z","shell.execute_reply.started":"2025-12-04T08:42:40.149105Z","shell.execute_reply":"2025-12-04T08:42:40.279956Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Display the Word Cloud\nall_text = \" \".join(df['clean_text'].astype(str))\nwordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_text)\n\nplt.figure(figsize=(10,5))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T08:42:40.281502Z","iopub.execute_input":"2025-12-04T08:42:40.281762Z","iopub.status.idle":"2025-12-04T08:42:51.432819Z","shell.execute_reply.started":"2025-12-04T08:42:40.281740Z","shell.execute_reply":"2025-12-04T08:42:51.432020Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize NLTK tools\nlemmatizer = WordNetLemmatizer()\nstop_words = set(stopwords.words('english'))\n\n# Function to clean each tweet\ndef preprocess_text(text):\n    # 1Ô∏è‚É£ Convert to string and lowercase\n    text = str(text).lower()\n    \n    # 2Ô∏è‚É£ Remove HTML tags\n    text = BeautifulSoup(text, \"html.parser\").get_text()\n    \n    # 3Ô∏è‚É£ Remove URLs\n    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)\n    \n    # 4Ô∏è‚É£ Remove mentions and hashtags\n    text = re.sub(r'\\@\\w+|\\#','', text)\n    \n    # 5Ô∏è‚É£ Remove numbers\n    text = re.sub(r'\\d+', '', text)\n    \n    # 6Ô∏è‚É£ Remove punctuation (including single & double quotes explicitly)\n    text = text.translate(str.maketrans('', '', string.punctuation))  # removes !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n    text = text.replace(\"'\", \"\").replace('\"', \"\")  # extra explicit safety for quotes\n    \n    # 7Ô∏è‚É£ Remove extra whitespaces\n    text = text.strip()\n    \n    # 8Ô∏è‚É£ Tokenize\n    words = word_tokenize(text)\n    \n    # 9Ô∏è‚É£ Remove stopwords and lemmatize\n    words = [lemmatizer.lemmatize(w) for w in words if w not in stop_words]\n    \n    # Join back to string\n    return ' '.join(words)\n\n# Apply preprocessing to the 'clean_text' column\ndf['clean_text_processed'] = df['clean_text'].apply(preprocess_text)\n\n# Show sample processed text\nprint(\"üìå Sample processed tweets:\\n\")\ndisplay(df[['clean_text', 'clean_text_processed']].head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T08:42:51.433669Z","iopub.execute_input":"2025-12-04T08:42:51.434084Z","iopub.status.idle":"2025-12-04T08:43:27.821571Z","shell.execute_reply.started":"2025-12-04T08:42:51.434058Z","shell.execute_reply":"2025-12-04T08:43:27.820810Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Again check the head of the data.\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T08:43:27.824388Z","iopub.execute_input":"2025-12-04T08:43:27.824742Z","iopub.status.idle":"2025-12-04T08:43:27.834376Z","shell.execute_reply.started":"2025-12-04T08:43:27.824722Z","shell.execute_reply":"2025-12-04T08:43:27.833373Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# 1Ô∏è‚É£ Maximum number of words to keep (vocabulary size)\nMAX_NB_WORDS = 10000  \n\n# 2Ô∏è‚É£ Maximum sequence length (from text length histogram)\nMAX_SEQUENCE_LENGTH = 50  \n\n# 3Ô∏è‚É£ Initialize tokenizer\ntokenizer = Tokenizer(num_words=MAX_NB_WORDS, oov_token=\"<OOV>\")\n\n# 4Ô∏è‚É£ Fit tokenizer on preprocessed text\ntokenizer.fit_on_texts(df['clean_text_processed'])\n\n# 5Ô∏è‚É£ Convert text to sequences (word indices)\nsequences = tokenizer.texts_to_sequences(df['clean_text_processed'])\n\n# 6Ô∏è‚É£ Pad sequences to same length\nX = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n\n# 7Ô∏è‚É£ Print example\nprint(\"üìå Example original text:\")\nprint(df['clean_text_processed'].iloc[0])\nprint(\"\\nüìå Example tokenized sequence:\")\nprint(X[0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T08:43:27.835365Z","iopub.execute_input":"2025-12-04T08:43:27.835654Z","iopub.status.idle":"2025-12-04T08:43:32.364183Z","shell.execute_reply.started":"2025-12-04T08:43:27.835634Z","shell.execute_reply":"2025-12-04T08:43:32.363397Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\ny = df[\"category\"].values\ny = y + 1\n# First, split off 10% for test\nX_temp, X_test, y_temp, y_test = train_test_split(\n    X, y, test_size=0.10, random_state=42, stratify=y\n)\n\n# Now, split remaining 90% into 75% train and 15% validation\n# 15% of total dataset = 0.15, but relative to temp (90%) ‚Üí 0.15 / 0.90 ‚âà 0.1667\nX_train, X_val, y_train, y_val = train_test_split(\n    X_temp, y_temp, test_size=0.1667, random_state=42, stratify=y_temp\n)\n\n# Print shapes\nprint(\"üìå X_train shape:\", X_train.shape)\nprint(\"üìå X_val shape:\", X_val.shape)\nprint(\"üìå X_test shape:\", X_test.shape)\nprint(\"üìå y_train shape:\", y_train.shape)\nprint(\"üìå y_val shape:\", y_val.shape)\nprint(\"üìå y_test shape:\", y_test.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T08:43:32.365005Z","iopub.execute_input":"2025-12-04T08:43:32.365352Z","iopub.status.idle":"2025-12-04T08:43:32.500355Z","shell.execute_reply.started":"2025-12-04T08:43:32.365325Z","shell.execute_reply":"2025-12-04T08:43:32.499702Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vocab_size = 10000        # Total number of unique words your tokenizer knows\nembedding_dim = 100       # Size of dense vector for each word (word representation)\nmax_length = X_train.shape[1]  # Length of each padded sequence\n\nmodel = Sequential()      # Sequential model ‚Üí layers stacked one after another\n\n\n# ============================\n# üîπ 1. Embedding Layer\n# ============================\nmodel.add(Embedding(\n    input_dim=vocab_size,       # How many words to embed (vocabulary size)\n    output_dim=embedding_dim,   # Size of word vector (100-dimensional)\n    input_length=max_length,    # Input sequence length\n    mask_zero=True              # Ignores padding (0s) so model doesn't learn noise\n))\n# üìå **Purpose:**\n# Converts word indices ‚Üí dense vectors.\n# Helps model understand word meaning, position, similarity etc.\n\n\n# ============================\n# üîπ 2. Bidirectional LSTM Layer\n# ============================\nmodel.add(Bidirectional(LSTM(\n    128,                        # Number of LSTM units/neurons\n    return_sequences=False,     # False ‚Üí only last output used (classification task)\n    dropout=0.3,                # Drop neurons during training to avoid overfitting\n    recurrent_dropout=0.2,      # Dropout inside LSTM on recurrent connections\n    kernel_regularizer=l2(0.001) # Apply L2 regularization ‚Üí reduces overfitting\n)))\n# üìå **Purpose:**\n# LSTM reads sequence **forward + backward** to capture full context.\n# Good for sentiment/text classification.\n# - 128 units store sequence memory\n# - Dropout prevents overfitting\n# - L2 regularization makes weights smaller ‚Üí stable learning\n\n\n# ============================\n# üîπ 3. Batch Normalization\n# ============================\nmodel.add(BatchNormalization())\n# üìå **Purpose:**\n# Normalizes activations ‚Üí faster training + more stability\n\n\n# ============================\n# üîπ 4. Dropout Layer\n# ============================\nmodel.add(Dropout(0.5))\n# üìå **Purpose:**\n# Randomly drops 50% neurons ‚Üí prevents overfitting\n\n\n# ============================\n# üîπ 5. Dense Layer (Hidden Layer)\n# ============================\nmodel.add(Dense(\n    64,                        # Number of neurons in Dense layer\n    activation='relu',         # Activation to learn non-linear features\n    kernel_regularizer=l2(0.001) # L2 regularizer to avoid overfitting\n))\n# üìå **Purpose:**\n# Learns deep high-level patterns extracted by LSTM.\n\n\n# BatchNorm again for stable training\nmodel.add(BatchNormalization())\n\n# Dropout again to prevent overfitting\nmodel.add(Dropout(0.3))\n\n\n# ============================\n# üîπ 6. Output Layer\n# ============================\nmodel.add(Dense(\n    3,                        # Number of classes (change if different)\n    activation='softmax'      # Softmax ‚Üí multi-class probability output\n))\n# üìå **Purpose:**\n# Final predictions ‚Üí returns probability for each class.\n\n\n# ============================\n# üîπ Compile Model\n# ============================\nmodel.build(input_shape=(None, max_length))\nmodel.compile(\n    optimizer=Adam(learning_rate=0.001),  # Adam optimizer for fast training\n    loss='categorical_crossentropy',       # Loss for multi-class classification\n    metrics=['accuracy']                   # Track accuracy\n)\n\nmodel.summary()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T08:43:32.501112Z","iopub.execute_input":"2025-12-04T08:43:32.501344Z","iopub.status.idle":"2025-12-04T08:43:34.752305Z","shell.execute_reply.started":"2025-12-04T08:43:32.501327Z","shell.execute_reply":"2025-12-04T08:43:34.751764Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# Early stopping to prevent overfitting\nearly_stop = EarlyStopping(\n    monitor='val_accuracy',\n    patience=5,\n    restore_best_weights=True,\n    verbose=1\n)\n\n# Recompile model with sparse_categorical_crossentropy\nmodel.compile(\n    optimizer=Adam(learning_rate=0.001),\n    loss='sparse_categorical_crossentropy',  # Changed to sparse\n    metrics=['accuracy']\n)\n\n# Train LSTM\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_val, y_val),\n    epochs=20,\n    batch_size=128,\n    callbacks=[early_stop]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T08:43:34.752981Z","iopub.execute_input":"2025-12-04T08:43:34.753237Z","iopub.status.idle":"2025-12-04T09:17:14.647872Z","shell.execute_reply.started":"2025-12-04T08:43:34.753210Z","shell.execute_reply":"2025-12-04T09:17:14.646938Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(8,5))\nplt.plot(history.history['accuracy'], label='Train Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.title('Training vs Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T09:17:14.649143Z","iopub.execute_input":"2025-12-04T09:17:14.649447Z","iopub.status.idle":"2025-12-04T09:17:14.853562Z","shell.execute_reply.started":"2025-12-04T09:17:14.649414Z","shell.execute_reply":"2025-12-04T09:17:14.852910Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nplt.figure(figsize=(8,5))\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('Training vs Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T09:17:14.854272Z","iopub.execute_input":"2025-12-04T09:17:14.854559Z","iopub.status.idle":"2025-12-04T09:17:15.019819Z","shell.execute_reply.started":"2025-12-04T09:17:14.854541Z","shell.execute_reply":"2025-12-04T09:17:15.019241Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# Predictions on test set\ny_pred = model.predict(X_test)\ny_pred_classes = np.argmax(y_pred, axis=1)\n\n# Classification report\nprint(classification_report(y_test, y_pred_classes, target_names=['Negative','Neutral','Positive']))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T09:17:15.020592Z","iopub.execute_input":"2025-12-04T09:17:15.020845Z","iopub.status.idle":"2025-12-04T09:17:36.730419Z","shell.execute_reply.started":"2025-12-04T09:17:15.020823Z","shell.execute_reply":"2025-12-04T09:17:36.729800Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Predict probabilities\ny_pred_probs = model.predict(X_test)\n\n# Convert probabilities to class labels\ny_pred_classes = np.argmax(y_pred_probs, axis=1)\n\n# Map 0,1,2 back to -1,0,1 for interpretability\ny_pred_labels = y_pred_classes - 1\ny_actual_labels = y_test - 1\n\n# Create a dataframe to compare\ncomparison_df = pd.DataFrame({\n    'Actual': y_actual_labels,\n    'Predicted': y_pred_labels\n})\n\n# Display first 10 predictions\nprint(\"üìå Actual vs Predicted (first 10 rows):\")\ndisplay(comparison_df.sample(10))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T09:17:36.731165Z","iopub.execute_input":"2025-12-04T09:17:36.731414Z","iopub.status.idle":"2025-12-04T09:17:56.989745Z","shell.execute_reply.started":"2025-12-04T09:17:36.731386Z","shell.execute_reply":"2025-12-04T09:17:56.988974Z"}},"outputs":[],"execution_count":null}]}